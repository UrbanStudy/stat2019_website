---
title: 'STAT565 Notes'
author: ""
date: "Winter 2019"
output: html_document
---

#  {.tabset .tabset-fade .tabset-pills}

## 1. Introduction

### 1.1 Strategy of Experimentation

### 1.2 Some Typical Applications of Experimental Design

 - Characterizing a Process
 - Optimizing a Process
 - Designing a Product
 - Formulating a Product
 - Designing a Web Page

### 1.3 Basic Principles

 - randomization: both the allocation of the experimental material and the order in which the individual runs of the experiment are to be performed are randomly determined.
 - replication: an independent repeat run of each factor combination. replication $\neq$ repeated measurements.
 - blocking: a design technique used to improve the precision with which comparisons among the factors of
interest are made.
 - factorial

### 1.4 Guidelines for Designing Experiments

 1. Recognition of and statement of the problem.
 2. Selection of the response variable.
 3. Choice of factors, levels, and range.
  - potential design factors
   + design factors
   + held-constant factors
   + allowed-to-vary factors
  - nuisance factors
   + controllable factors
   + uncontrollable factors
   + noise factors
 4. Choice of experimental design
 5. Performing the experiment
 6. Statistical analysis of the data
 7. Conclusions and recommendations
 
### 1.5 A Brief History of Statistical Design

### 1.6 Summary: Using Statistical Techniques in Experimentation

 1. Use your nonstatistical knowledge of the problem
 2. Keep the design and analysis as simple as possible
 3. Recognize the difference between practical and statistical significance
 4. Experiments are usually iterative


## 2. Simple Comparative Experiments

### 2.1 Introduction

**Theorem 2-1** The Central Limit Theorem (p.31)

If $y_1, y_2,.., y_n$ is a sequence of n independent and identically distributed random variables with $E(y_i)=\mu$ and $V(y_i)=\sigma^2$ (both finite) and $x=y_1 + y_2 +..+y_n$, then the limiting form of the distribution of
$$Z_n=\frac {x ‚àí n\mu}{\sqrt{n\sigma^2}}$$
as $n\to\infty$ is the standard normal distribution.

### 2.2 Basic Statistical Concepts

### 2.3 Sampling and Sampling Distributions

### 2.4 Inferences About the Differences in Means, Randomized Designs

#### 2.4.1 Hypothesis Testing

#### 2.4.2 Confidence Intervals

#### 2.4.3 Choice of Sample Size

#### 2.4.4 The Case Where $\sigma_1^2\neq\sigma_2^2$

#### 2.4.5 The Case Where $\sigma_1$ and $\sigma_2$ are Known

#### 2.4.6 Comparing a Single Mean to a Specified Value

#### 2.4.7 Summary

### 2.5 Inferences About the Differences in Means, Paired Comparison Designs

#### 2.5.1 The Paired Comparison Problem

#### 2.5.2 Advantages of the Paired Comparison Design

### 2.6 Inferences About the Variances of Normal Distributions


## 3. ANOVA

### 3.1 An Example

a single-factor experiment with _a_ levels of the factor and _n_ replicates

### 3.2 The Analysis of Variance

*a* treatments or different *levels* of a *single factor*.

$$\text{mean model}\quad y_{ij}=\mu_{i}+\varepsilon_{ij} \begin{cases}i=1,2,‚Ä¶,a\\j=1,2,‚Ä¶,n\end{cases}$$

where $y_{ij}$ is the ijth observation, $\mu_i$ is the mean of the ith factor level or treatment, and $\varepsilon_{ij}$ is a random error component. $E[y_{ij}]=\mu_i$

$$\text{effects model}\quad y_{ij}=\mu+\tau_{i}+\varepsilon_{ij} \begin{cases}i=1,2,‚Ä¶,a\\j=1,2,‚Ä¶,n\end{cases}$$

$\mu$ is a parameter common to all treatments called the overall mean, and $\tau_{i}$ is a parameter unique to the ith treatment called the ith treatment effect.

yij ‚àº N(ùúá + ùúèi, ùúé
2)
$$y_{ij}\sim N(\mu+\tau_{i},\sigma^2) $$

$$\begin{array}\ \text{linear statistical models} \\ \text{(one-way or single-factor analysis of variance (ANOVA) model)}\end{array} \begin{cases}
\text{fixed effects model}\\ \\ \text{random effects model} \\ \text{(components of variancemodel)}\end{cases}$$

### 3.3 Analysis of the Fixed Effects Model

$y_{i.}$ represents the total of the observations under the ith treatment. Let $\bar y_{i.}$ represent the average of the observations under the ith treatment. Similarly, let $y_{..}$ represent the grand total of all the observations and $y_{..}$ represent the grand average of all the observations.
$$y_{i.} =\sum^n_{j=1}y_{ij}\quad \bar y_{i.} = \frac{y_{i.}}n\quad i=1,2,..,a$$
$$y_{..}=\sum^a_{i=1}\sum^n_{j=1}y_{ij}\quad \bar y_{..}=\frac{y_{..}}N$$

where $N=an$ is the total number of observations.

$$H_0: \mu_1=\mu_2=..=\mu_a\qquad H_A: \mu_i\neq\mu_j\quad \text{for at least one pair (i, j)}$$

$$\frac{\sum_{i=1}^a\mu_i}{a}=\mu\quad \sum_{i=1}^a\tau_i=0$$

$$H_0: \tau_1=\tau_2=..=\tau_a=0\qquad H_A: \mu_i\neq\mu_j\quad \text{for at least one pair (i, j)}$$

#### 3.3.1 Decomposition of the Total Sum of Squares

$$SST = \sum^a_{i=1}\sum^n_{j=1}(y_{ij}‚àí\bar y_{..})^2=\sum^a_{i=1}\sum^n_{j=1}[(\bar y_{i.}‚àí\bar y_{..})+(y_{ij}‚àí\bar y_{i.})]^2$$

$$=n\sum^a_{i=1}(\bar y_{i.}‚àí\bar y_{..})^2+\sum^a_{i=1}\sum^n_{j=1}(y_{ij}‚àí\bar y_{i.})^2+2\sum^a_{i=1}\sum^n_{j=1}(\bar y_{i.}‚àí\bar y_{..})(y_{ij}‚àí\bar y_{i.})$$

$$\because \sum^n_{j=1}(y_{ij}‚àí\bar y_{i.})=y_{i.}-n\bar y_{i.}=y_{i.}-n\frac{y_{i.}}n=0$$
$$\therefore \sum^a_{i=1}\sum^n_{j=1}(y_{ij}‚àí\bar y_{..})^2=n\sum^a_{i=1}(\bar y_{i.}‚àí\bar y_{..})^2+\sum^a_{i=1}\sum^n_{j=1}(y_{ij}‚àí\bar y_{i.})^2$$

$$SS_T = SS_{treatments} + SS_E$$

$$SS_E=\sum^a_{i=1}\sum^n_{j=1}(y_{ij}‚àí\bar y_{i.})^2=\sum^a_{i=1}\left[\sum^n_{j=1}(y_{ij}‚àí\bar y_{i.})^2\right]$$

$$S_i^2=\frac{\sum^n_{j=1}(y_{ij}‚àí\bar y_{i.})^2}{n-1}\quad i=1,2,..,a$$

|      |  SS | df  | MS  | E(SS) | E(MS) |
| ---  | --- | --- | --- |  ---  |  ---  |
|$SS_T$    |$\sum^a_{i=1}\sum^n_{j=1}(y_{ij}‚àí\bar y_{..})^2$     |$N-1$|
|$SS_{Trt}$|$n\sum^a_{i=1}(\bar y_{i.}‚àí\bar y_{..})^2$           |$a-1$|$\frac{SS_{Trt}}{N-a}$|$(a-1)\sigma^2+n\sum^a_{i=1}\tau_i^2$|$\sigma^2+\frac{n}{a-1}\sum^a_{i=1}\tau_i^2$
|$SS_E$    |$\sum^a_{i=1}\sum^n_{j=1}(y_{ij}‚àí\bar y_{i.})^2$     |$N-a$| $\frac{SS_E}{N-a}$   | $(N-a)\sigma^2$ | $\sigma^2$

#### 3.3.2 Statistical Analysis

**Theorem 3-1** Cochran's Theorem (p.72)

Let $Z_i$ be NID(0,1) for $i = 1, 2,..,\nu$ and

$$\sum_{i=1}^\nu Z_i^2 = Q_1 + Q_2 + ¬∑ ¬∑ ¬∑ + Q_s$$


 
where $s\le\nu$, and $Q_i$ has $\nu_i$ degrees of freedom $(i= 1,2,..,s)$. Then $Q_1,Q_2,..,Q_s$ are independent chi-square random variables with $\nu_1,\nu_2,.., \nu_s$ degrees of freedom, respectively, if and only if
$$\nu=\nu_1 +\nu_2+..+\nu_s$$

#### 3.3.3 Estimation of the Model Parameters

#### 3.3.4 Unbalanced Data

### 3.4 Model Adequacy Checking (verification)

#### 3.4.1 The Normality Assumption

#### 3.4.2 Plot of Residuals in Time Sequence

#### 3.4.3 Plot of Residuals Versus Fitted Values

#### 3.4.4 Plots of Residuals Versus Other Variables

### 3.5 Practical Interpretation of Results

#### 3.5.1 A Regression Model

#### 3.5.2 Comparisons Among Treatment Means

#### 3.5.3 Graphical Comparisons of Means

#### 3.5.4 Contrasts

#### 3.5.5 Orthogonal Contrasts

#### 3.5.6 Scheff√©‚Äôs Method for Comparing All Contrasts

 - Fisher's Least Significant Difference Method
 - Boferroni Method
 - Tukey's method
 - Dunnett's method

#### 3.5.7 Comparing Pairs of Treatment Means

#### 3.5.8 Comparing Treatment Means with a Control

### 3.6 Sample Computer Output

### 3.7 Determining Sample Size

#### 3.7.1 Operating Characteristic and Power Curves

#### 3.7.2 Confidence Interval Estimation Method

### 3.8 Other Examples of Single-Factor Experiments

#### 3.8.1 Chocolate and Cardiovascular Health

#### 3.8.2 A Real Economy Application of a Designed Experiment

#### 3.8.3 Discovering Dispersion Effects

### 3.9 The Random Effects Model

#### 3.9.1 A Single Random Factor

#### 3.9.2 Analysis of Variance for the Random Model

#### 3.9.3 Estimating the Model Parameters

### 3.10 The Regression Approach to the Analysis of Variance

#### 3.10.1 Least Squares Estimation of the Model Parameters

#### 3.10.2 The General Regression Significance Test

### 3.11 Nonparametric Methods in the Analysis of Variance

#### 3.11.1 The Kruskal‚ÄìWallis Test

#### 3.11.2 General Comments on the Rank Transformation



## 4. RCBD, LS, GLS, BIBD

### 4.1 The Randomized Complete Block Design

#### 4.1.1 Statistical Analysis of the RCBD

#### 4.1.2 Model Adequacy Checking

#### 4.1.3 Some Other Aspects of the Randomized Complete Block Design

#### 4.1.4 Estimating Model Parameters and the General Regression Significance Test

### 4.2 Latin Square Design

### 4.3 The Graeco-Latin Square Design

### 4.4 Balanced Imcomplete Block Design

#### 4.4.1 Statistical Analysis of the BIBD

#### 4.4.2 Least Squares Estimation of the Parameters

#### 4.4.3 Recovery of Interblock Information in the BIBD

## 5. Factional Designs

### 5.1 Basic Definitions and Principles

### 5.2 The Advantage of Factorials

### 5.3 The Two-Factor Factorial Design

#### 5.3.1 An Example

#### 5.3.2 Statistical Analysis of the Fixed Effects Model

#### 5.3.3 Model Adequacy Checking

#### 5.3.4 Estimating the Model Parameters

#### 5.3.5 Choice of Sample Size

#### 5.3.6 The Assumption of No Interaction in a Two-Factor Model

#### 5.3.7 One Observation per Cell

### 5.4 The General Factorial Design

### 5.5 Fitting Response Curves and Surfaces


### 5.6 Blocking in a Factorial Design

## 6. The $2^k$ Factional Design

6.1 Introduction

### 6.2 The $2^2$ Design

### 6.3 The $2^3$ Design

### 6.4 The General $2^k$ Design

### 6.5 A Single Replicate of the 2k Design

### 6.6 Additional Examples of Unreplicated $2^k$ Designs

### 6.7 $2^k$ Designs are Optimal Designs

### 6.8 The Addition of Center Points to the $2^k$ Design

### 6.9 Why We Work with Coded Design Variables