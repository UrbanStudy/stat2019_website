<!doctype html>
<html lang="en">
<head>
	<meta charset="utf-8" />
	<link rel="icon" sizes="76x76" href="../img/favicon.png">
	<link rel="icon" type="image/png" href="../img/favicon.png">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

	<title>STAT</title>

	<meta content='width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0' name='viewport' />

	
	<link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons" />
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700" />
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" />
  <link rel="stylesheet" href="https://cdn.rawgit.com/gaborcsardi/r-font/master/rlogo.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/wesbos/Font-Awesome-Docker-Icon/master/fontcustom/fontcustom.css">

	
    <link href="../css/bootstrap.min.css" rel="stylesheet" />
    <link href="../css/material-kit.css" rel="stylesheet"/>
</head>

  
<body class="profile-page">
  	<nav class="navbar navbar-transparent navbar-fixed-top navbar-color-on-scroll">
	    	<div class="container">
        	
        	<div class="navbar-header">
        		<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navigation-example">
            		<span class="sr-only">Toggle navigation</span>
		            <span class="icon-bar"></span>
		            <span class="icon-bar"></span>
		            <span class="icon-bar"></span>
        		</button>
        		<a class="navbar-brand" href="../">STAT</a>
        	</div>

        	<div class="collapse navbar-collapse" id="navigation-example">
        		<ul class="nav navbar-nav navbar-right">
      
        

        <li class="nav-item">
          <a href="../">
            <i class="sidebar-button-icon fa fa-lg fa-home"></i>
            <span>Home</span>
          </a>
        </li>
        
      
        

        <li class="nav-item">
          <a href="../syllabus">
            <i class="material-icons">date_range</i>
            <span>Syllabus</span>
          </a>
        </li>
        
      
        

        <li class="nav-item">
          <a href="../tools">
            <i class="material-icons">build</i>
            <span>Tools and Resources</span>
          </a>
        </li>
        
      
        

        <li class="nav-item">
          <a href="../notes">
            <i class="material-icons">gavel</i>
            <span>Notes</span>
          </a>
        </li>
        
      
        		</ul>
        	</div>
    	</div>
    </nav>
  <div class="wrapper">
    <div class="header header-filter" style="background-image: url('http://www.greggirard.com/content/gallery/GIrard_KWC_temple01.jpg');">
    </div>
		<div class="main main-raised">
			<div class="container">
		  	<div class="section text-left">
		  	  <h2 class="title">Notes</h2>
	            <h3 id="table-of-common-distributions">Table of Common Distributions</h3>

<table \sum_{i="1" i="0">
<thead>
<tr>
<th>Distribution</th>
<th>CDF</th>
<th>P(X=x),f(x)</th>
<th>Î¼</th>
<th>$\mathbf{EX^2}$</th>
<th>Var</th>
<th>MGF,$M_X(t),E[e^{tx}]$</th>
<th>M'(t)</th>
<th>M''(t)</th>
</tr>
</thead>

<tbody>
<tr>
<td>$Bern(p)$</td>
<td></td>
<td>$p<sup>xq</sup>{1-x},x\in{1,0}$</td>
<td>$p$</td>
<td>$p$</td>
<td>$pq$</td>
<td>$pe^t+q$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Binom(n,p)$</td>
<td>$I_{1-p}(n-x,x+1)$</td>
<td>$\binom{n}{x}p^x q^{n-x}; x \in {0,1..n}$</td>
<td>$np$</td>
<td>$\mu(\mu+q)$</td>
<td>$\mu{q}$</td>
<td>$(pe<sup>t+q)</sup>n$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Geom(p)$</td>
<td>$1-q^x$</td>
<td>$pq^{x-1},x\in 1,2,..$</td>
<td>$\frac1p$</td>
<td>$\frac{p+2q}{p^2}$</td>
<td>$\frac{q}{p^2}$</td>
<td>$\frac{pe<sup>t}{1-qe</sup>t},t&lt;-\ln{q}$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Geom2(p)$</td>
<td>$1-q^{x+1}$</td>
<td>$pq^x,x\in 0,1,..$</td>
<td>$\frac{q}p$</td>
<td>$\frac{q<sup>2+q}{p</sup>2}$</td>
<td>$\frac{q}{p^2}$</td>
<td>$\frac{p}{1-qe^t}, qe^t&lt;1$</td>
<td>$\frac{pqe<sup>t}{(1-qe</sup>t)^2}$</td>
<td>$\frac{2pqe<sup>t}{(1-qe</sup>t)^3}-M'(t)$</td>
</tr>

<tr>
<td>$NBin(r,p)$</td>
<td>$x\in r,r+1..$</td>
<td>$\binom{x-1}{r-1}p<sup>rq</sup>{x-r}$</td>
<td>$\frac{r}p$</td>
<td>$ $</td>
<td>$\frac{rq}{p^2}$</td>
<td>$(\frac{pe<sup>t}{1-qe</sup>t})^r$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$NBin2(r,p)$</td>
<td>$ $</td>
<td>$\binom{x+r-1}{r-1}p<sup>rq</sup>x, x \in 0,1..$</td>
<td>$\frac{rq}p$</td>
<td>$ $</td>
<td>$\frac{rq}{p^2}$</td>
<td>$(\frac{p}{1-qe<sup>t})</sup>r, qe^t&lt;1$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Hgeom(N,m,k)$</td>
<td>$ $</td>
<td>$\frac{\binom{m}{x}\binom{N-m}{k-x}}{\binom{N}{k}}$</td>
<td>$\frac{km}{N}$</td>
<td>$ $</td>
<td>$\mu\frac{(N-m)(N-k)}{N(N-1)}$</td>
<td></td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Hgeom(w,b,k)$</td>
<td>$ $</td>
<td>$\frac{\binom{w}{x}\binom{b}{k-x}}{\binom{w+b}{k}}$</td>
<td>$\frac{kw}{w+b}$</td>
<td>$ $</td>
<td>$\mu\frac{b(w+b-k)}{(w+b)(w+b-1)}$</td>
<td></td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Pois(\mu)$</td>
<td>$e<sup>{-\mu}\sum_</sup>x\frac{\mu^i}{i!}$</td>
<td>$\frac{\mu<sup>x}{x!}e</sup>{-\mu},x \in 0,1..$</td>
<td>$\mu$</td>
<td>$\mu^2+\mu$</td>
<td>$\mu$</td>
<td>$e<sup>{\mu(e</sup>t-1)}$</td>
<td>$\mu e^tM(t)$</td>
<td>$\mu e^t(1+\mu e^t)M(t)$</td>
</tr>

<tr>
<td>$Unif(n)$</td>
<td></td>
<td>$\frac{1}n,x \in 1,2..n$</td>
<td>$\frac{n+1}2$</td>
<td>$\frac{(n+1)(2n+1)}{6}$</td>
<td>$\frac{(n^2-1)}{12}$</td>
<td>$\frac<sup>n{e</sup>{ti}}}n$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Unif(a,b)$</td>
<td>$\frac{x-a}{b-a}$</td>
<td>$\frac{1}{b-a},x \in(a,b)$</td>
<td>$\frac{a+b}{2}$</td>
<td>$\frac{(b<sup>2+ab+a</sup>2)}{3}$</td>
<td>$\frac{(b-a)^2}{12}$</td>
<td>$\frac{e<sup>{tb}-e</sup>{ta}}{t(b-a)}$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$N(\mu,\sigma^2)$</td>
<td></td>
<td>$\frac{1}{\sigma\sqrt{2\pi}} e<sup>{-\frac{(x-\mu)</sup>2}{2\sigma^2}}$</td>
<td>$\mu$</td>
<td>$\mu<sup>2+\sigma</sup>2$</td>
<td>$\sigma^2$</td>
<td>$e^{\mu t +\frac{\sigma<sup>2t</sup>2}2}$</td>
<td>$(\mu+\sigma^2t)M(t)$</td>
<td>$[(\mu+\sigma<sup>2t)</sup>2+\sigma^2]M(t)$</td>
</tr>

<tr>
<td>$N(0, 1)$</td>
<td></td>
<td>$\frac{1}{\sqrt{2\pi}}e<sup>{-\frac{x</sup>2}2}$</td>
<td>$0$</td>
<td>$1$</td>
<td>$1$</td>
<td>$e<sup>{\frac{t</sup>2}2}$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$\mathcal{LN}(\mu,\sigma^2)$</td>
<td>$x&gt;0$</td>
<td>$\frac{1}{x\sigma \sqrt{2\pi}}e^{\frac{-(\ln x-\mu)<sup>2}{2\sigma</sup>2}}$</td>
<td>$e<sup>{\mu+\frac{\sigma</sup>2}2}$</td>
<td>$e<sup>{2\mu+2\sigma</sup>2}$</td>
<td>$\theta<sup>2(e</sup>{\sigma^2}-1)$</td>
<td>$\times$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Cauchy(\theta,\sigma^2)$</td>
<td></td>
<td>$\frac{1}{\pi\sigma}\frac1{1+(\frac{x-\theta}{\sigma})^2}$</td>
<td>$\theta{median}$</td>
<td>$\theta\pm\sigma{quartiles}$</td>
<td>$\times$</td>
<td>$ $</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$DExpo(\mu,\sigma^2)$</td>
<td>$-\infty&lt;x&lt;\infty$</td>
<td>$\frac{1}{2\sigma} e^{-</td>
<td>\frac{x-\mu}{\sigma}</td>
<td>}$</td>
<td>$\mu$</td>
<td>$\mu<sup>2+2\sigma</sup>2$</td>
<td>$2\sigma^2$</td>
<td>$\frac{e^{\mu t}}{1-\sigma<sup>2t</sup>2}$</td>
</tr>

<tr>
<td>$Expo(\lambda)$</td>
<td>$1-e^{-\lambda x}$</td>
<td>$\lambda e^{-\lambda x}$</td>
<td>$\frac{1}{\lambda}$</td>
<td>$ $</td>
<td>$\frac{1}{\lambda^2}$</td>
<td>$\frac{\lambda}{\lambda - t}, t &lt; \lambda$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Expo2(\beta)$</td>
<td>$x \in (0,\infty)$</td>
<td>$\frac1{\beta} e^{-\frac{x}\beta}$</td>
<td>$\beta$</td>
<td>$ $</td>
<td>$\beta^2$</td>
<td>$(1-\beta t)^{-1}$</td>
<td>$\beta(1-\beta t)^{-2}$</td>
<td>$2\beta^2(1-\beta t)^{-3}$</td>
</tr>

<tr>
<td>$Gam(a, \lambda)$</td>
<td>$x \in (0,\infty)$</td>
<td>$\frac{\lambda<sup>a}{\Gamma(a)}x</sup>{a-1}e^{-\lambda x}$</td>
<td>$\frac{a}{\lambda}$</td>
<td></td>
<td>$\frac{a}{\lambda^2}$</td>
<td>$(\frac{\lambda}{\lambda - t})^a, t &lt; \lambda$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Gam2(\alpha,\beta)$</td>
<td>$ $</td>
<td>$\frac{1}{\Gamma(a)\beta<sup>{\alpha}}x</sup>{a-1}e^{-x/\beta}$</td>
<td>$\alpha\beta$</td>
<td>$\alpha(\alpha+1)\beta^2$</td>
<td>$\alpha\beta^2$</td>
<td>$(1-\beta t)^{-a}, t &lt;\frac1\beta$</td>
<td>$\mu{M(t)}(1-\beta t)^{-1}$</td>
<td>$E[X^2]{M(t)}(1-\beta t)^{-2}$</td>
</tr>

<tr>
<td>$\Gamma(\alpha)=$</td>
<td>$(\alpha-1)!$</td>
<td>$\int_0^\infty t<sup>{\alpha-1}e</sup>{-t}dt$</td>
<td>$\Gamma(\frac12)=\sqrt{\pi}$</td>
<td>$\Gamma(1)=1$</td>
<td>$\Gamma(\alpha+1)=\alpha\Gamma(\alpha)$</td>
<td>$\Gamma(-\frac12)=-2\Gamma(\frac12)$</td>
<td>$\Gamma(0)=\Gamma(-1)=\infty$</td>
<td></td>
</tr>

<tr>
<td>$B(\alpha,\beta)=$</td>
<td>$\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$</td>
<td>$\int_0^1 t<sup>{\alpha-1}(1-t)</sup>{\beta-1}dt$</td>
<td>$ $</td>
<td>$ $</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>

<tr>
<td>$Beta(a, b)$</td>
<td>$x\in(0,1)$</td>
<td>$\frac{1}{B(a,b)}x<sup>{a-1}(1-x)</sup>{b-1}$</td>
<td>$\frac{a}{a+b}$</td>
<td>$\frac{a(a+1)}{(a+b)(a+b+1)}$</td>
<td>$\frac{ab}{(a+b)^2(a+b+1)}$</td>
<td>$V=\frac{\mu(1-\mu)}{(a+b+1)}$</td>
<td>$ $</td>
<td>$M^n(t)=\frac{\Gamma(a+n)\Gamma(a+b)}{\Gamma(a+b+n)\Gamma(a)}$</td>
</tr>

<tr>
<td>$\chi_p^2$</td>
<td>$x&gt;0$</td>
<td>$\frac{1}{2<sup>{\frac{p}2}\Gamma(\frac{p}2)}x</sup>{\frac{p}2-1}e^{-\frac{x}2}$</td>
<td>$p$</td>
<td>$2p+p^2$</td>
<td>$2p$</td>
<td>$(1-2t)^{-\frac{p}2}, t&lt;\frac12$</td>
<td></td>
<td></td>
</tr>

<tr>
<td>$t_n$</td>
<td>$ $</td>
<td>$\frac{\Gamma(\frac{n+1}2)}{\sqrt{n\pi} \Gamma(\frac{n}2)} (1+\frac{x<sup>2}n)</sup>{-\frac{n+1}2}$</td>
<td>$0,n&gt;1$</td>
<td>$ $</td>
<td>$\frac{n}{n-2},n&gt;2$</td>
<td>$\times$</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>code</th>
<th>distribution</th>
</tr>
</thead>

<tbody>
<tr>
<td>dbinom(k,n,p)</td>
<td>PMF $P(X=k)$ for $X \sim \Bin(n,p)$</td>
</tr>

<tr>
<td>pbinom(x,n,p)</td>
<td>CDF $P(X \leq x)$ for $X \sim \Bin(n,p)$</td>
</tr>

<tr>
<td>qbinom(a,n,p)</td>
<td>$a$th quantile for $X \sim \Bin(n,p)$</td>
</tr>

<tr>
<td>rbinom(r,n,p)</td>
<td>vector of $r$ i.i.d.~$\Bin(n,p)$ r.v.s</td>
</tr>

<tr>
<td>dgeom(k,p)</td>
<td>PMF $P(X=k)$ for $X \sim \Geom(p)$</td>
</tr>

<tr>
<td>dhyper(k,w,b,n)</td>
<td>PMF $P(X=k)$ for $X \sim \HGeom(w,b,n)$</td>
</tr>

<tr>
<td>dnbinom(k,r,p)</td>
<td>PMF $P(X=k)$ for $X \sim \NBin(r,p)$</td>
</tr>

<tr>
<td>dpois(k,r)</td>
<td>PMF $P(X=k)$ for $X \sim \Pois(r)$</td>
</tr>

<tr>
<td>dbeta(x,a,b)</td>
<td>PDF $f(x)$ for $X \sim \Beta(a,b)$</td>
</tr>

<tr>
<td>dchisq(x,n)</td>
<td>PDF $f(x)$ for $X \sim \chi^2_n$</td>
</tr>

<tr>
<td>dexp(x,b)</td>
<td>PDF $f(x)$ for $X \sim \Expo(b)$</td>
</tr>

<tr>
<td>dgamma(x,a,r)</td>
<td>PDF $f(x)$ for $X \sim \Gam(a,r)$</td>
</tr>

<tr>
<td>dlnorm(x,m,s)</td>
<td>PDF $f(x)$ for $X \sim \mathcal{LN}(m,s^2)$</td>
</tr>

<tr>
<td>dnorm(x,m,s)</td>
<td>PDF $f(x)$ for $X \sim \N(m,s^2)$</td>
</tr>

<tr>
<td>dt(x,n)</td>
<td>PDF $f(x)$ for $X \sim t_n$</td>
</tr>

<tr>
<td>dunif(x,a,b)</td>
<td>PDF $f(x)$ for $X \sim \Unif(a,b)$</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>Normal_PDF</th>
<th>Normal_CDF</th>
<th>Chi^2_PDF</th>
<th>Chi^2_CDF</th>
</tr>
</thead>

<tbody>
<tr>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/320px-Normal_distribution_pdf.png" alt="Normal_Distribution_PDF"></figure></td>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Normal_distribution_cdf.png/320px-Normal_distribution_cdf.png" alt="Normal_Distribution_CDF"></figure></td>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Chi-square_distributionPDF.png/320px-Chi-square_distributionPDF.png" alt="Chi-square_distributionPDF"></figure></td>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Chi-square_distributionCDF.png/320px-Chi-square_distributionCDF.png" alt="Chi-square_distributionCDF"></figure></td>
</tr>

<tr>
<td>F_PDF</td>
<td>F_CDF</td>
<td>Cauchy_PDF</td>
<td>Cauchy_CDF</td>
</tr>

<tr>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/F-distribution_pdf.svg/320px-F-distribution_pdf.svg.png" alt="F-distribution_pdf"></figure></td>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/F_distributionCDF.png/320px-F_distributionCDF.png" alt="F-distribution_cdf"></figure></td>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Cauchy_distribution_pdf.png/320px-Cauchy_distribution_pdf.png" alt="Cauchy_pdf"></figure></td>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Cauchy_distribution_cdf.png/320px-Cauchy_distribution_cdf.png" alt="Cauchy_cdf"></figure></td>
</tr>

<tr>
<td>Exponential_pmf</td>
<td>Exponential_cdf</td>
<td>Binomial_pmf</td>
<td>Binomial_cdf</td>
</tr>

<tr>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Exponential_distribution_pdf.png/320px-Exponential_distribution_pdf.png" alt="Exponential_distribution_pdf"></figure></td>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Exponential_distribution_cdf.png/320px-Exponential_distribution_cdf.png" alt="Exponential_distribution_cdf"></figure></td>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/75/Binomial_distribution_pmf.svg/320px-Binomial_distribution_pmf.svg.png" alt="Binomial_distribution_pmf"></figure></td>
<td><figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Binomial_distribution_cdf.svg/320px-Binomial_distribution_cdf.svg.png" alt="Binomial_distribution_cdf"></figure></td>
</tr>
</tbody>
</table>

<h3 id="univariable-distribution">Univariable Distribution</h3>

<ol>
<li><p><a href="https://www.wolfram.com/mathematica/new-in-8/parametric-probability-distributions/index.html">Chart of Univariable Distribution</a></p></li>

<li><p><a href="https://ieeexplore-ieee-org.proxy.lib.pdx.edu/abstract/document/5755180">Eighty Univariate Distributions and Their Relationships Displayed in a Matrix Format</a></p></li>
</ol>

<p><a href="https://onlinecourses.science.psu.edu/stat414/node/109/">Probability Theory and Mathematical Statistics</a></p>

<ol>
<li>www.probabilitycourse.com</li>
</ol>

<h3 id="glossary-of-statistical-terms">Glossary of Statistical Terms</h3>

<ul>
<li>ANOVA:</li>
</ul>

<p>Analysis of variance usually refers to an analysis of a continuous dependent variable where all the predictor variables are categorical. One-way ANOVA, where there is only one predictor variable (factor; grouping variable), is a generalization of the 2-sample t-test. ANOVA with 2 groups is identical to the t-test. Two-way ANOVA refers to two predictors, and if the two are allowed to interact in the model, two-way ANOVA involves cross-classification of observations simultaneously by both factors. It is not appropriate to refer to repeated measures within subjects as two-way ANOVA (e.g., treatmentÃ time). An ANOVA table sometimes refers to statistics for more complex models, where explained variation from partial and total effects are displayed and continuous variables may be included.</p>

<ul>
<li>Bayesâ rule or theorem:</li>
</ul>

<p>$Pr(A|B) = \frac{Pr(B|A) Pr(A)}{Pr(B)}$, read as the probability that event A happens given that event B has happened equals the probability that B happens given that A has happened multiplied by the (unconditional) probability that A happens and divided by the (unconditional) probability that B happens. Bayesâ rule follows immediately from the law of conditional probability which states that $P(A|B)=P(A\cap B)P(B)$.</p>

<ul>
<li>Bayesian inference:</li>
</ul>

<p>A branch of statistics based on Bayesâ theorem. Bayesian inference doesnât use P -values and generally does not test hypotheses. It requires one to formally specify a probability distribution encapsulating the prior knowledge about, say, a treatment effect. The state of prior knowledge can be specified as âno knowledgeâ by using a flat distribution. Once the prior distribution is specified, the data are used to modify the prior state of knowledge to obtain the post-experiment state of knowledge. Final probabilities computed in the Bayesian framework are probabilities of various treatment effects.</p>

<ul>
<li>bias:</li>
</ul>

<p>A systematic error. Examples: a miscalibrated machine that reports cholesterol too high by 20mg% on the average; a satisfaction questionnaire that leads patients to never report that they are dissatisfied with their medical care; using each patientâs lowest blood pressure over 24 hours to describe a drugâs antihyptertensive properties.</p>

<ul>
<li>binary variable:</li>
</ul>

<p>A variable whose only two possible values, usually zero and one.</p>

<ul>
<li>bootstrap:</li>
</ul>

<p>A simulation technique for studying properties of statistics without the need to have the infinite population available. The most common use of the bootstrap involves taking random samples (with replacement) from the original dataset and studying how some quantity of interest varies. Each random sample has the same number of observations as the original dataset. Some of the original subjects may be omitted from the random sample and some may be sampled more than once. The bootstrap can be used to compute standard deviations and confidence limits without assuming a model. For example, if one took 200 samples with replacement from the original dataset, computed the sample median from each sample, and then computed the sample standard deviation of the 200 medians, the result would be a good estimate of the true standard deviation of the original sample median. The bootstrap can also be used to internally validate a predictive model without holding back patient data during model development.</p>

<ul>
<li>calibration:</li>
</ul>

<p>Reliability of predicted values, i.e., extent to which predicted values agree with observed values. For a predictive model a calibration curve is constructed by relating predicted to observed values in some smooth manner. The calibration curve is judged against a 45â¦ line. Miscalibration could be called bias. Calibration error is frequently assessed for predicted event probabilities. If for example 0.4 of the time it rained when the predicted probability of rain was 0.4, the rain forecast is perfectly calibrated.</p>

<ul>
<li>case-control study:</li>
</ul>

<p>A study in which subjects are selected on the basis of their outcomes, and then exposures (treatments) are ascertained. For example, to assess the association between race and operative mortality one might select all patients who died after open heart surgery in a given year and then select an equal number of patients who survived, matching on several variables other than race so as to equalize (control for) their distributions between the cases and non-cases.</p>

<ul>
<li>categorical variable:</li>
</ul>

<p>A variable having only certain possible values for which there is no logical ordering of the values. Also called a nominal, polytomous, discrete categorical variable or factor.</p>

<ul>
<li>censoring:</li>
</ul>

<p>When the response variable is the time until an event, subjects not followed long enough for the event to have occurred have their event times censored at the time of last follow-up. This kind of censoring is right censoring. For example, in a follow-up study, patients entering the study during its last year will be followed a maximum of 1 year, so they will have their time until event censored at 1 year or less. Left censoring means that the time to the event is known to be less than some value. In interval censoring the time is known to be in a specified interval. Most statistical analyses assume that what causes a subject to be censored is independent of what would cause her to have an event. If this is not the case, informative censoring is said to be present. For example, if a subject is pulled off of a drug because of a treatment failure, the censoring time is indirectly reflecting a bad clinical outcome and the resulting analysis will be biased.</p>

<ul>
<li>conditional probability:</li>
</ul>

<p>The probability of the veracity of a statement or of an event A given that a specific condition B holds or that an event B has already occurred, denoted by P (A|B). This is a probability in the presence of knowledge captured by B. For example, if the condition B is that a person is male, the conditional probability is the probability of A for males. It could be argued that there is no such thing as a completely unconditional probability. In this example one is implicitly conditioning on humans even if not considering the personâs sex.</p>

<ul>
<li>confidence limits:</li>
</ul>

<p>To say that the 0.95 confidence limits for an unknown quantity are [a, b] means that 0.95 of similarly constructed confidence limits in repeated samples from the same population would contain the unknown quantity. Very loosely speaking one could say that she is 0.95 âconfidentâ that the unknown value is in the interval [a, b], although in the frequentist school unknown parameters are constants, so they are either inside or outside intervals and there are no probabilities associated with these events. The interpretation of a single confidence interval in frequentist statistics is highly problematic. Note that a confidence interval should be symmetric about a point estimate only when the distribution of the point estimate is symmetric. Many confidence intervals are asymmetric, e.g., intervals for probabilities, odds ratios, and other ratios.</p>

<ul>
<li>confounder:</li>
</ul>

<p>A variable which is correlated with the response variable and with the treatment assignment (or exposure variable). A confounder, when properly controlled for, can explain away an apparent association between the treatment and the response.</p>

<ul>
<li>continuous variable:</li>
</ul>

<p>A variable that can take on any number of possible values. Practically speaking, when a variable can take on at least, say, 10 values, it can be treated as a continuous variable. For example, it can be plotted on a scatterplot and certain meaningful calculations can be made using the variable.</p>

<ul>
<li>critical value:</li>
</ul>

<p>The value of a test statistic (e.g., t, F , Ï2, z) that if exceeded by the observed test statistic would result in statistical significance at a chosen Î± level or better. For a z-test (normal deviate test) the critical level of z is 1.96 when Î± = 0.05 for a two-sided test. For t and F tests, critical values decrease as the sample size increases, as one requires less penalty for having to estimate the population variance as n gets large.</p>

<ul>
<li>cross-validation:</li>
</ul>

<p>This technique involves leaving out m patients at a time, fitting a model on the remaining n â m patients, and obtaining an unbiased evaluation of predictive accuracy on the m patients. The estimates are averaged over â¥ n/m repetitions. Cross-validation provides estimates that have more variation than those from bootstrapping. It may require &gt; 200 model fits to yield precise estimates of predictive accuracy.</p>

<ul>
<li>detectable difference:</li>
</ul>

<p>The value of a true population treatment effect (difference between two treatments) that if held would result in a statistical test having exactly the desired power.</p>

<ul>
<li>discrimination:</li>
</ul>

<p>A variable or modelâs discrimination ability is its ability to separate subjects having a low responses from subjects having high responses. One way to quantify discrimination is the ROC curve area.</p>

<ul>
<li>dummy variable:</li>
</ul>

<p>A device used in a multivariable regression model to describe a categorical predictor without assuming a numeric scoring. Indicator variable might be a better term. For example, treat- ments A, B, C might be described by the two dummy predictor variables X1 and X2, where X1 is a binary variable taking on the value of 1 if the treatment for the subject is B and 0 otherwise, and X2 takes on the value 1 if the subject is under treatment C and 0 otherwise. The two dummy variables completely define 3 categories, because when X1 = X2 = 0 the treatment is A.</p>

<ul>
<li>estimate:</li>
</ul>

<p>A statistical estimate of a parameter based on the data. See parameter. Examples include the sample mean, sample median, and estimated regression coefficients.</p>

<ul>
<li>frequentist statistical inference:</li>
</ul>

<p>Currently the most commonly used statistical philosophy. It uses hy- pothesis testing, type I and II errors, power, P -values, confidence limits, and adjustments of P -values for testing multiple hypotheses from the same study. Final probabilities computed using frequentist methods, P -values, are probabilities of obtaining values of statistics. The frequentist approach is also called the sampling approach as it considers the distribution of statistics over hypothetical repeated samples from the same population. The frequentist approach is concerned with long-run operating char- acteristics of statistics and estimates. Because of this and because of the backwards time/information ordering of P -values, frequentist testing requires complex multiplicity adjustments but provides no guiding principles for exactly how those adjustments should be derived. Frequentist statistics involves massive confusion of two ideas: (1) the apriori probability that an experiment will generate misleading information (e.g., the chance of a false positive result or type I error) and (2) the evidence for an asser- tion after the experiment is run. The latter should not involve a multiplicity adjustment, but because the former does, frequentists do not know how to interpret the latter when multiple hypotheses are tested or when a single hypothesis is tested sequentially.</p>

<ul>
<li>goodness of fit:</li>
</ul>

<p>Assessment of the agreement of the data with either a hypothesized pattern (e.g., independence of row and column factors in a contingency table or the form of a regression relationship) or a hypothesized distribution (e.g., comparing a histogram with expected frequencies from the normal distribution).</p>

<ul>
<li>Hawthorne effect:</li>
</ul>

<p>A change in a subject response that results from the subject knowing she is being observed.</p>

<ul>
<li>inter-quartile range:</li>
</ul>

<p>The range between the outer quartiles (25th and 75th percentiles).</p>

<ul>
<li>least squares estimate:</li>
</ul>

<p>The value of a regression coefficient that results in the minimum sum of squared errors, where an error is defined as the difference between an observed and a predicted dependent variable value.</p>

<ul>
<li>linear regression model:</li>
</ul>

<p>This is also called OLS or ordinary least squares and refers to regression for a continuous dependent variable, and usually to the case where the residuals are assumed to be Gaussian. The linear model is sometimes called general linear model, not to be confused with generalized linear model where the distribution can take on many non-Gaussian forms.</p>

<ul>
<li>logistic regression model:</li>
</ul>

<p>A multivariable regression model relating one or more predictor variables to the probabilities of various outcomes. The most commonly used logistic model is the binary logistic model7,6 which predicts the probability of an event as a function of several variables. There are several types of ordinal logistic models for predicting an ordinal outcome variable, and there is a polytomous logistic model for categorical responses. The binary and polytomous models generalize the Ï2 test for testing for association between categorical variables. One commonly used ordinal model, the proportional odds model1, generalizes the Wilcoxon 2-sample rank test. Binary logistic models are useful for predicting events in which time is not very important. They can be used to predict events by a specified time, but this can result in a loss of information. Logistic models are used to estimate adjusted odds ratios as well as probabilities of events.</p>

<ul>
<li>maximum likelihood estimate:</li>
</ul>

<p>An estimate of a statistical parameter (such as a regression coefficient, mean, variance, or standard deviation) that is the value of that parameter making the data most likely to have been observed. MLEs have excellent statistical properties in general, such as converging to population values as the sample size increases, and having the best precision from among all such competing estimators. When the data are normally distributed, maximum likelihood estimates of regression coefficients and means are equivalent to least squares estimates. When the data are not normally distributed (e.g. binary outcomes, or survival times), maximum likelihood is the standard method to estimate the regression coefficients (e.g. logistic regression, Cox regression).</p>

<ul>
<li>mean:</li>
</ul>

<p>Arithmetic average, i.e., the sum of all the values divided by the number of observations. The mean of a binary variable is equal to the proportion of ones because the sum of all the zero and one values equals the number of ones. The mean can be heavily influenced by outliers.</p>

<ul>
<li>median:</li>
</ul>

<p>Value such that half of the observationsâ values are less than and half are greater than that value. The median is also called the 50th percentile or the 0.5 quantile. The median is not heavily influenced by outliers so it can be more representative of âtypicalâ subjects. When the data happen to be normally (Gaussian) distributed, the median is not as precise as the mean in describing the central tendency.</p>

<ul>
<li>multiple comparisons:</li>
</ul>

<p>It is common for one study to involve the calculation of more than one P -value.
For example, the investigator may wish to test for treatment effects in 3 groups defined by disease etiology, she may test the effects on 4 different patient response variables, or she may look for a significant difference in blood pressure at each of 24 hourly measurements. When multiple statistical tests are done, the chances of at least one of them resulting in a false positive finding increases as the number of tests increase. This is called âinflation of type I error.â When one wishes to control the overall type I error, individual tests can be done using a more stringent Î± level, or individual P -values can be adjusted upward. Such adjustments are usually dictated when using frequentist statistics, as P -values mean the probability of getting a result this impressive if there is really no effect, and âthis impressiveâ can be taken to mean âthis impressive given the large number of statistics examined.â</p>

<ul>
<li><p>multivariable model:
A model relating multiple predictor variables (risk factors, treatments, etc.) to a single response or dependent variable. The predictor variables may be continuous, binary, or cat- egorical. When a continuous variable is used, a linearity assumption is made unless the variable is expanded to include nonlinear terms. Categorical variables are modeled using dummy variables so as to not assume numeric assignments to categories.</p></li>

<li><p>multivariate model:</p></li>
</ul>

<p>A model that simultaneously predicts more than one dependent variable, e.g. a model to predict systolic and diastolic blood pressure or a model to predict systolic blood pressure 5 min. and 60 min. after drug administration.</p>

<ul>
<li>nominal significance level:</li>
</ul>

<p>In the context of multiple comparisons involving multiple statistical tests, the apparent significance level Î± of each test is called the nominal significance level. The overall type I error for the study, the probability of at least one false positive result, will be greater than Î±.</p>

<ul>
<li>nonparametric estimator:</li>
</ul>

<p>A method for estimating a parameter without assuming an underlying dis- tribution for the data. Examples include sample quantiles, the empirical cumulative distribution, and the Kaplan-Meier survival curve estimator.</p>

<ul>
<li>nonparametric tests:</li>
</ul>

<p>A test that makes minimal assumptions about the distribution of the data or about certain parameters of a statistical model. Nonparametric tests for ordinal or continuous variables are typically based on the ranks of the data values. Such tests are unaffected by any one-one transformation of the data, e.g., by taking logs. Even if the data come from a normal distribution, rank tests lose very little efficiency (they have a relative efficiency of 3/Ï = 0.955 if the distribution is normal) compared with parametric tests such as the t-test and the linear correlation test. If the data are not normal, a rank test can be much more efficient than the corresponding parametric test. For these reasons, it is not very fruitful to test data for normality and then to decide between the parametric and nonparametric approaches. In addition, tests of normality are not always very powerful. Examples of nonparametric tests are the 2-sample Wilcoxon-Mann-Whitney test, the 1-sample Wilcoxon signed-rank test (usually used for paired data), and the Spearman, Kendall, or Somers rank correlation tests. Even though nonparametric tests do not assume a specific distribution for a group, they assume a connection between the distributions of any two groups. For example, the logrank test assumes proportional hazards, i.e., that the survival curve for group A is a power of the survival curve for group B. The Wilcoxon test, for optimal power, assumes that the cumulative distributions are in proportional odds.</p>

<ul>
<li>normal distribution:</li>
</ul>

<p>A symmetric, bell-shaped distribution that is most useful for approximating the distribution of statistical estimators. Also called the Gaussian distribution. The normal distribution cannot be relied upon to approximate the distribution of raw data. The normal distributionâs bell shape follows a rigid mathematical equation of the form eâx^2 . For a normal distribution the probability that a measurement will fall within Â±1.96 standard deviations of the mean is 0.95.</p>

<ul>
<li>null hypothesis:</li>
</ul>

<p>Customarily but not necessarily a hypothesis of no effect, e.g., no reduction in mean blood pressure or no correlation between age and blood pressure. The null hypothesis, labeled H0, is often used in the frequentist branch of statistical inference as a âstraw personâ; classical statistics often assumes what one hopes doesnât happen (no effect of a treatment) and attempts to gather evidence against that assumption (i.e., tries to reject H0). H0 usually specifies a single point such as 0mmHg reduction in blood pressure, but it can specify an interval, e.g., H0: blood pressure reduction is between -1 and +1 mmHg. âNull hypothesesâ can also be e.g. H0: correlation between X and Y is 0.5.</p>

<ul>
<li>observational study:</li>
</ul>

<p>Study in which no experimental condition (e.g., treatment) is manipulated by the investigator, i.e., randomization is not used.</p>

<ul>
<li>odds:</li>
</ul>

<p>The probability an event occurs divided by the probability that it doesnât occur. An event that
occurs 0.90 of the time has 9:1 odds of occurring since 0.9(1â0.9)= 9.</p>

<ul>
<li>odds ratio:</li>
</ul>

<p>The odds ratio for comparing two groups (A, B) on their probabilities of an outcome occurring is the odds of the event occurring for group A divided by the odds that it occurs for group B. If P(A) and P(B) represent the probability of the outcome for the two groups of subjects, the A : B odds ratio is (PA/1âPA)/(PB/1âPB). Odds ratios are in the interval [0, â). An odds ratio for a treatment is a measure of relative effect of that treatment on a binary outcome. As summary measures, odds ratios have advantages over risk ratios: they donât depend on which of two possible outcomes is labeled the âeventâ, and any odds ratio can apply to any probability of outcome in the reference group. Because of this, one frequently finds that odds ratios for comparing treatments are relatively constant across different types of patients. The same is not true of risk ratios or risk differences; these depend on the level of risk in the reference group.</p>

<ul>
<li>one-sided test:</li>
</ul>

<p>A test designed to test a directional hypothesis, yielding a one-sided P -value. For example, one might test the null hypothesis H0 that there is no difference in mortality between two treatments, with the alternative hypothesis being that the new drug lowers mortality. See also two-sided test.</p>

<ul>
<li>ordinal variable:</li>
</ul>

<p>A categorical variable for which there is a definite ordering of the categories. For example, severity of lower back pain could be ordered as none, mild, moderate, severe, and coded using these names or using numeric codes such as 0,1,2,10. Spacings between codes are not important.</p>

<ul>
<li>P -value:</li>
</ul>

<p>The probability of getting a result (e.g., t or Ï2 statistics) as or more extreme than the observed statistic had H0 been true. An Î±-level test would reject H0 if P â¤ Î±. However, the P -value can be reported instead of choosing an arbitrary value of Î±. Examples:<br>
(1) An investigator compared two randomized groups for differences in systolic blood pressure, with the two mean pressures being 134.4 mmHg and 138.2 mmHg. She obtained a two-tailed P = 0.03. This means that if there is truly no difference in the population means, one would expect to find a difference in means exceeding 3.8 mmHg in absolute value 0.03 of the time. The investigator might conclude there is evidence for a treatment effect on mean systolic blood pressure if the statistical testâs assumptions are true.<br>
(2) An investigator obtained P = 0.23 for testing a correlation being zero, with the sample correlation being 0.08. The probability of getting a correlation this large or larger in absolute value if the population correlation is zero is 0.08. No conclusion is possible other than (a) more data are needed and (b) there is no convincing evidence for or against a zero correlation. For both of these examples confidence intervals would be helpful.</p>

<ul>
<li>paired data:</li>
</ul>

<p>When each subject has two response measurements, there is a natural pairing to the data and the two responses are correlated. The correlation results from the fact that generally there is more variation between subjects than there is within subjects. Sometimes one can take the difference or log ratio of the two responses for each subject, and then analyze these âeffect measuresâ using an unpaired one-sample approach such as the Wilcoxon signed-rank test or the paired t-test. One must be careful that the effect measure is properly chosen so that it is independent of the baseline value.</p>

<ul>
<li>parameter:</li>
</ul>

<p>An unknown quantity such as the population mean, population variance, difference in two means, or regression coefficient.</p>

<ul>
<li>parametric model:</li>
</ul>

<p>A model based on a mathematical function having a few unknown parameters.</p>

<ul>
<li>parametric test:</li>
</ul>

<p>A test which makes specific assumptions about the distribution of the data or specific assumptions about model parameters. Examples include the t-test and the Pearson product-moment linear correlation test.</p>

<ul>
<li>percentile:</li>
</ul>

<p>The p-th percentile is the value such that np/100 of the observationsâ values are less than that value. The p-th quantile is the value such that np of the observationsâ values are less.</p>

<ul>
<li>posterior probability:</li>
</ul>

<p>In a Bayesian context, this is the probability of an event after making use of the information in the data. In other words, it is the prior probability of an event after updating it with the data. Posterior probability can also be called post-test probability if one equates a diagnostic test with âdataâ (see also ROC curve).</p>

<ul>
<li>power:</li>
</ul>

<p>Probability of rejecting the null hypothesis for a set value of the unknown effect. Power could also be called the sensitivity of the statistical test in detecting that effect. Power increases when the sample size and true unknown effect increase and when the inter-subject variability decreases. In a two-group comparison, power generally increases as the allocation ratio gets closer to 1:1. For a given experiment it is desirable to use a statistical test expected to have maximum power (sensitivity). A less powerful statistical test will have the same power as a better test that was applied after discarding some of the observations. For example, testing for differences in the proportion of patients with hypertension in a 500-patient study may yield the same power as a 350-patient study which used blood pressure as a continuous variable.</p>

<ul>
<li>precision:</li>
</ul>

<p>Degree of absence of random error. The precision of a statistical estimator is related to the expected error that occurs when approximating the infinite-data value. In other words, when you try to estimate some measure in a population, the precision is related to the error in the estimate. So precision can be thought of as a âmargin of errorâ in estimating some unknown value. Precision can be quantified by the width of a confidence interval and sometimes by a standard deviation of the estimator (standard error). For the confidence intervals, a âmargin for errorâ is computed so that the quoted interval has a certain probability of containing the true value (e.g., population mean difference). Some authors define precision as the reciprocal of the variance of an estimate. By that definition, precision increases linearly as the sample size increases. If instead one defines precision on the original scale of measurement instead of its square (i.e., if one uses the standard error or width of a confidence interval), precision increases as the square root of the sample size.</p>

<ul>
<li>predictor, explanatory variable, risk factor, covariate, covariable, independent variable:</li>
</ul>

<p>quan- tities which may be associated with better or worse outcome.</p>

<ul>
<li>prior probability:</li>
</ul>

<p>The probability of an event as it could best be assessed before the experiment. In diagnostic testing this is called the pre-test probability. The prior probability can come from an objective model based on previously available information, or it can be based on expert opinion. In some Bayesian analyses, prior probabilities are expressed as probability distributions which are flat lines, to reflect a complete absence of knowledge about an event. Such distributions are called non- informative, flat, or reference distributions, and analyses based on them fully let the data âspeak for themselves.â</p>

<ul>
<li>probability:</li>
</ul>

<p>In the frequentist school, the probability of an event denotes the limit of the long-term fraction of occurrences of the event. This notion of probability implies that the same experiment which generated the outcome of interest can be repeated infinitely often. Even a coin will change after 100,000 flips. Likewise, some may argue that a patient is âone of a kindâ and that repetitions of the same experiment are not possible. One could reasonably argue that a ârepetitionâ does not denote the same patient at the same stage of the disease, but rather any patient with the same severity of disease (measured with current technology). There are other schools of probability that do not require the notion of replication at all. For example, the school of subjective probability (associated with the Bayesian school) âconsiders probability as a measure of the degree of belief of a given subject in the occurrence of an event or, more generally, in the veracity of a given assertionâ (see P. 55 of5). de Finetti defined subjective probability in terms of wagers and odds in betting. A risk-neutral individual would be willing to wager $P that an event will occur when the payoff is $1 and her subjective probability is P for the event. The domain of application of probability is all-important. We assume that the true event status (e.g., dead/alive) is unknown, and we also assume that the information the probability is conditional upon (e.g. Pr{death | male, age=70}) is what we would check the probability against. In other words, we do not ask whether Pr(death | male, age=70) is accurate when compared against Pr(death | male, age=70, meanbp=45, patient on downhill course).</p>

<ul>
<li>prospective study:</li>
</ul>

<p>One in which the study is first designed, then the subjects are enrolled. Prospective studies are usually characterized by intentional data collection.</p>

<ul>
<li>quartiles:</li>
</ul>

<p>The 25th and 75th percentiles and the median. The three values divide a variables distributions into four intervals containing equal numbers of observations.</p>

<ul>
<li>random error:</li>
</ul>

<p>An error caused by sampling from a group rather than knowing the true value of a quantity such as the mean blood pressure for the entire group, e.g., healthy men over age 80. One can also speak of random errors in single measurements for individual subjects, e.g., the error in using a single blood pressure measurement to represent a subjectâs long-term blood pressure.</p>

<ul>
<li>random sample:</li>
</ul>

<p>A sample selected by a random device that ensures that the sample (if large enough) is representative of the infinite group. A probability sample is a kind of random sample in which each possible subject has a known probability of being sampled, but the probabilities can vary. For example, one may wish to over-sample African-Americans in a study to ensure good representation. In that case one could sample African-Americans with probability of 1.0 and others with a probability of 0.5.</p>

<ul>
<li>randomness:</li>
</ul>

<p>Absence of a systematic pattern. One might wish to examine whether some hormone level varies systematically over the day as opposed to having a random pattern, or whether events such as epileptic seizures tend to cluster or occur randomly in time. Sometimes the residuals in an ordinary regression model are plotted against the order in which subjects were accrued to make sure that the pattern is random (e.g., there was no learning trend for the investigators).</p>

<ul>
<li>rate:</li>
</ul>

<p>A ratio such as a change per unit time. Rates are often limits, and shouldnât be confused with probabilities. The latter are constrained to be between 0 and 1 whereas there are no constraints on possible values for rates.</p>

<ul>
<li>regression to the mean:</li>
</ul>

<p>Tendency for a variable that has an extreme value on its first measurement to have a more typical value on its second measurement. For example, suppose that subjects must have LDL cholesterol &gt; 190mg% to qualify for a study, and the median LDL cholesterol for qualifying subjects at the screening visit was 230 mg%. The median LDL cholesterol value at their second visit might be 200mg%, with several of the subjects having values below 190. This is the âsophomore slumpâ in baseball; second-year players are watched when they have phenomenal rookie years. Regression to the mean also takes many other forms, all arising because variables or subgroups are not examined at random but rather because they appear âimpressiveâ: (1) One might compare 5 treatments with a control and choose the treatment having the maximum difference. On a repeated study that treatmentâs average response will be found to be much closer to that of the control. (2) In a randomized controlled trial the investigators may wish to estimate the effect of treatment in multiple subgroups. They find that in 40 left-handed diabetics the treatment multiplies mortality by 0.4. If the study is replicated, they would find that the mortality reduction in left-handed diabetics is much closer to the mortality reduction in the overall sample of patients. (3) Researchers study the association between 40 possible risk factors and some outcome, and find that the factor with the strongest association had a correlation of 0.5 with the response. On replication, the correlation will be much lower. This result is very related to what happens in stepwise variable selection, where the most statistically significant variables selected will have their importance (regression coefficients) greatly overstated.</p>

<ul>
<li>residual:</li>
</ul>

<p>A statistical quantity that should be unrelated to certain other variables because their effects should have already been subtracted out. In ordinary multiple regression, the most commonly used residual is the difference between predicted and observed values.</p>

<ul>
<li>retrospective study:</li>
</ul>

<p>A study in which subjects were already enrolled before the study was designed, or the outcome of interest has occurred before the start of the study (an in a case control study). Such studies often have difficulties such as absence of needed adjustment (confounder) variables and missing data.</p>

<ul>
<li>risk:</li>
</ul>

<p>Often used as another name for probability but a more accurate definition is the probability of an adverse event Ã the severity of the loss that experiencing that event would entail.</p>

<ul>
<li>semi-parametric:</li>
</ul>

<p>âParametricâ assumptions may be made about some aspects of a model, while other components may be estimated ânon-parametricallyâ. In the Cox regression procedure, a parametric model for the relative hazard is overlaid on a non-parametric estimate of baseline hazard2.</p>

<ul>
<li>significance level:</li>
</ul>

<p>A preset value of Î± against which P -values are judged in order to reject H0 (see Type I error). Sometimes a P -value itself is called the significance level.</p>

<ul>
<li>standard deviation:</li>
</ul>

<p>A measure of the variability (spread) of measurements across subjects. The standard deviation has a simple interpretation only if the data distribution is Gaussian (normal), and in that restrictive case the mean Â±1.96 standard deviations is expected to cover 0.95 of the distribution of the measurement. Standard deviation is the square root of the variance.</p>

<ul>
<li>standard error:</li>
</ul>

<p>The standard deviation of a statistical estimator. For example, the standard deviation of a mean is called the standard error of the mean, and it equals the standard deviation of individual measurements divided by the square root of the sample size. Standard errors describe the precision of a statistical summary, not the variability across subjects. Standard errors go to zero as the sample size â â.</p>

<ul>
<li>survival analysis:</li>
</ul>

<p>A branch of statistics dealing with the analysis of the time until an event such as death. Survival analysis is distinguished by its emphasis on estimating the time course of events and in dealing with censoring. See Cox model.</p>

<ul>
<li>survival function:</li>
</ul>

<p>The probability of being free of the event at a specified time.</p>

<ul>
<li>survival time:</li>
</ul>

<p>Interval between the time origin and the occurrence of the event or censoring2.</p>

<ul>
<li>symmetric distribution:</li>
</ul>

<p>One in which values to the left of the mean by a certain amount are just as likely to be observed as values to the right of the mean by the same amount. For symmetric distributions, the population mean and median are identical and the distance between the 25th and 50th percentiles equals the distance between the 50th and 75th percentiles.</p>

<ul>
<li>two-sided test:</li>
</ul>

<p>A test that is non-directional and that leads to a two-sided P -value. If the null hypothesis H0 is that two treatments have the same mortality outcome, a two-sided alternative is that the mortality difference is nonzero. Two-sided P -values are larger than one-sided P -values (they are double if the distribution of the test statistic is symmetric). They can be thought of as a multiplicity adjustment that would allow a claim to be made that a treatment lowers or raises mortality. See also one-sided test.</p>

<ul>
<li>type I error:</li>
</ul>

<p>False positive rate â the probability of rejecting H0 (i.e., declaring âstatistical significanceâ) when the null hypothesis is in fact true. The type I error is often called Î±.</p>

<ul>
<li>type II error:</li>
</ul>

<p>Failing to detect an effect that is real, i.e., the false negative rate. The type II error is referred to as Î², which is one minus the power of the test. In other words, the power of the test is 1 â Î².</p>

<ul>
<li>variance:</li>
</ul>

<p>A measure of the spread or variability of a distribution, equaling the average value of the squared difference between measurements and the population mean measurement. From a sample of measurements, the variance is estimated by the sample variance, which is the sum of squared differences from the sample mean, divided by the number of measurements minus 1. The minus 1 is a kind of âpenaltyâ that corrects for estimating the population mean with the sample mean. Variances are typically only useful when the measurements follow a normal or at least a symmetric distribution.</p>

	      </div>
      </div>
		</div>
    <footer class="footer">
  <div class="container">
	  <nav class="pull-left">
	    <ul>
	    
        

        <li>
          <a href="http://d2l.pdx.edu/"><i class="button-icon fa fa-2x fa-map-marker"></i> D2L@pdx</a>
        </li>
        
      
        

        <li>
          <a href="https://www.datacamp.com/groups/usp654"><i class="button-icon fa fa-2x fa-pencil"></i> DataCamp</a>
        </li>
        
      
        

        <li>
          <a href="https://usp634-s2018.slack.com/messages"><i class="button-icon fa fa-2x fa-slack"></i> Slack</a>
        </li>
        
      
        

        <li>
          <a href="https://github.com/UrbanStudy/"><i class="button-icon fa fa-2x fa-github"></i> GitHub</a>
        </li>
        
      
      </ul>
    </nav>
	  <div class="copyright pull-right">
	    <img src="../img/cc-by.svg"/>
	  </div>
  </div>
</footer>
	</div>
</body>

			

	
	<script src="../js/jquery.min.js" type="text/javascript"></script>
	<script src="../js/bootstrap.min.js" type="text/javascript"></script>
	<script src="../js/material.min.js"></script>

	
	<script src="../js/nouislider.min.js" type="text/javascript"></script>

	
	<script src="../js/bootstrap-datepicker.js" type="text/javascript"></script>

	
	<script src="../js/material-kit.js" type="text/javascript"></script>

</html>
