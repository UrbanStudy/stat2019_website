<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="" />


<title>STAT562 Notes</title>

<script src="note_stat562_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="note_stat562_files/bootstrap-3.3.6/css/bootstrap.min.css" rel="stylesheet" />
<script src="note_stat562_files/bootstrap-3.3.6/js/bootstrap.min.js"></script>
<script src="note_stat562_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<script src="note_stat562_files/navigation-1.1/tabsets.js"></script>
<script src="note_stat562_files/navigation-1.1/codefolding.js"></script>
<link href="note_stat562_files/magnific-popup-1.1.0/magnific-popup.css" rel="stylesheet" />
<script src="note_stat562_files/magnific-popup-1.1.0/jquery.magnific-popup.min.js"></script>
<link href="note_stat562_files/readthedown-0.1/readthedown.css" rel="stylesheet" />
<script src="note_stat562_files/readthedown-0.1/readthedown.js"></script>




</head>

<body>


<div id="content" data-toggle="wy-nav-shift">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<nav id="nav-top" role="navigation" aria-label="top navigation">
    <a role="button" href="#" data-toggle="wy-nav-top"><span class="glyphicon glyphicon-menu-hamburger"></span></a>
</nav>

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span class="text-muted">Code</span> <span class="text-muted caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All</a></li>
</ul>
</div>

<div id="header">
<h1 class="title">STAT562 Notes</h1>
</div>


<div id="table-of-contents">
    <h2><a href="#content">STAT562 Notes</a></h2>
    <div id="text-table-of-contents">
      <ul>
      <li><a href="#multiple-random-variables">4. Multiple Random Variables</a><ul>
      <li><a href="#bivariate-transformations">4.3 Bivariate Transformations</a></li>
      <li><a href="#hierarchical-models-and-mixture-distributions">4.4 Hierarchical Models and Mixture Distributions</a></li>
      <li><a href="#covariance-and-correlation">4.5 Covariance and Correlation</a></li>
      <li><a href="#multivariate-distributions">4.6 Multivariate Distributions</a></li>
      <li><a href="#inequalities">4.7 Inequalities</a></li>
      </ul></li>
      <li><a href="#properties-of-a-random-sample">5. Properties of a Random Sample</a><ul>
      <li><a href="#basic-concepts-of-random-samples">5.1 Basic Concepts of Random Samples</a></li>
      <li><a href="#sums-of-random-variables-from-a-random-sample">5.2 Sums of Random Variables from a Random Sample</a></li>
      <li><a href="#sampling-from-the-normal-distribution">5.3 Sampling from the Normal Distribution</a></li>
      <li><a href="#order-statistics">5.4 Order Statistics</a></li>
      <li><a href="#convergence-concepts">5.5 Convergence Concepts</a></li>
      <li><a href="#generating-a-random-sample">5.6 Generating a Random Sample</a></li>
      </ul></li>
      <li><a href="#principles-of-data-reduction">6. Principles of Data Reduction</a><ul>
      <li><a href="#the-sufficiency-principle">6.2 The Sufficiency Principle</a></li>
      <li><a href="#the-likelihood-principle">6.3 The Likelihood Principle</a></li>
      <li><a href="#the-equivariance-principle">6.4 The Equivariance Principle</a></li>
      </ul></li>
      <li><a href="#point-estimation">7. Point Estimation</a><ul>
      <li><a href="#methods-of-finding-estimators">7.2 Methods of Finding Estimators</a></li>
      <li><a href="#methods-of-evaluating-estimators">7.3 Methods of Evaluating Estimators</a></li>
      </ul></li>
      <li><a href="#hypothesis-testing">8 Hypothesis Testing</a><ul>
      <li><a href="#introduction">8.1 Introduction</a></li>
      <li><a href="#methods-of-finding-tests">8.2 Methods of Finding Tests</a></li>
      </ul></li>
      <li><a href="#asymptotic-evaluations">10 Asymptotic Evaluations</a><ul>
      <li><a href="#point-estimation-1">10.1 Point Estimation</a></li>
      <li><a href="#bootstrap-standard-errors">10.1.4 Bootstrap Standard Errors</a></li>
      <li><a href="#robustness">10.2 Robustness</a></li>
      <li><a href="#hypothesis-testing-1">10.3 Hypothesis Testing</a></li>
      <li><a href="#interval-estimation">10.4 Interval Estimation</a></li>
      </ul></li>
      </ul>
    </div>
</div>

<div id="main">
<div id="multiple-random-variables" class="section level1">
<h1>4. Multiple Random Variables</h1>
<div id="bivariate-transformations" class="section level2">
<h2>4.3 Bivariate Transformations</h2>
<p><strong>Example 4.3.1 (Distribution of the sum of Poisson variables)</strong> Let X and Y be independent Poisson random variables with parameters <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\lambda\)</span>, respectively. Thus the joint pmf of (X,Y) is</p>
<p><span class="math display">\[f_{X,Y}(x,y)=\frac{\theta^xe^{-\theta}}{x!}\frac{\lambda^ye^{-\lambda}}{y!},\quad x=0,1,2,..,y=0,1,2,..\]</span></p>
<p>define <span class="math inline">\(U=X+Y\)</span> and <span class="math inline">\(V=Y\)</span></p>
<p><span class="math display">\[f_{U,V}(u,v)=f_{X,Y}(u-v,v)=\frac{\theta^{u-v}e^{-\theta}}{(u-v)!}\frac{\lambda^ve^{-\lambda}}{v!},\quad v=0,1,2,..,u=v,v+1,v+2,..\]</span></p>
<p><strong>Theorem 4.3.2</strong> if <span class="math inline">\(X\sim Poisson(\theta)\)</span> and <span class="math inline">\(Y\sim Poisson(\lambda)\)</span> and X and Y are indepedent, then <span class="math inline">\(X+Y\sim Poisson(\theta+\lambda)\)</span>.</p>
<p><code>2019.01.08</code> <code>p.12</code></p>
<p>Using Moment Generating Function (Theorem 4.2.12)</p>
<p><span class="math display">\[M_W(t)=M_X(t)M_Y(t)=e^{\mu_1(e^t-1)}e^{\mu_2(e^t-1)}=e^{(\mu_1+\mu_2)(e^t-1)}\]</span></p>
<p>So <span class="math inline">\(W\sim Poisson(\mu_1+\mu_2)\)</span></p>
<p>If (X,Y) is a continuous random vector with joint pdf <span class="math inline">\(f_{X,Y}(x,y)\)</span>, then the joint pdf of (U, V) can be expressed in terms of <span class="math inline">\(f_{X,Y}(x,y)\)</span> in a manner analogous to (2.1.8). As before, <span class="math inline">\(A=\{(x,y):f_{X,Y}(x,y)&gt;0\}\)</span> and <span class="math inline">\(B=\{(u,v): u=g_1(x,y)\ \text{and}\ v=g_2(x,y)\ \text{for some}\ (x,y)\in A\}\)</span>. The joint pdf <span class="math inline">\(f_{U,V}(u,v)\)</span> will be positive on the set B. For the simplest version of this result we assume that the transformation <span class="math inline">\(u=g_1(x,y)\)</span> and <span class="math inline">\(v=g_2(x,y)\)</span> defines a one-to-one transformation of A onto B. The transformation is onto because of the definition of B. We are assuming that for each <span class="math inline">\((u,v)\in B\)</span> there is only one <span class="math inline">\((x,y)\in A\)</span> such that <span class="math inline">\((u,v)=(g_1(x,y),g_2(x,y))\)</span>. For such a one-to-one, onto transformation, we can solve the equations <span class="math inline">\(u=g_1(x,y)\)</span> and <span class="math inline">\(v=g_2(x,y)\)</span> for x and y in terms of u and v. We will denote this inverse transformation by <span class="math inline">\(x=h_1(u,v)\)</span> and <span class="math inline">\(y=h_2(u,v)\)</span>. The role played by a derivative in the univariate case is now played by a quantity called the Jacobian of the transformation. This function of (u,v), denoted by J, is the determinant of a matrix of partial derivatives. It is defined by</p>
<p><span class="math display">\[J=\begin{vmatrix}\frac{\partial x}{\partial u} &amp; \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} &amp; \frac{\partial y}{\partial v} \end{vmatrix}=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial y}{\partial u}\frac{\partial x}{\partial v}\]</span> where</p>
<p><span class="math display">\[\frac{\partial x}{\partial u}=\frac{\partial h_1(u,v)}{\partial u},\ \frac{\partial x}{\partial v}=\frac{\partial h_1(u,v)}{\partial v},\ \frac{\partial y}{\partial u}=\frac{\partial h_2(u,v)}{\partial u},\ \frac{\partial y}{\partial v}=\frac{\partial h_2(u,v)}{\partial v}\]</span></p>
<p>We assume that J is not identically 0 on B. Then the joint pdf of (U,V) is 0 outside the set B and on the set B is given by</p>
<p><span class="math display">\[f_{U,V}(u,v)=f_{X,Y}(h_1(u,v),\ h_2(u,v))|J|,\]</span></p>
<p>The “reproductive” property: adding independent r.v.s from a family of distribution produces a new r.v. from the same family.</p>
<p><strong>Example 4.3.3 (Distribution of the product of beta variables)</strong> Let <span class="math inline">\(X\sim Beta(α,β)\)</span> and <span class="math inline">\(Y\sim Beta(α+β,\gamma)\)</span> be independent random variables. The joint pdf of <span class="math inline">\((X,Y)\)</span> is</p>
<p><span class="math inline">\(X\sim Beta(\alpha,\beta)\)</span>, <span class="math inline">\(Y\sim Beta(\alpha+\beta,\gamma)\)</span> <span class="math inline">\(X,Y\)</span> indep, <span class="math inline">\(U=XY\)</span>, <span class="math inline">\(f_U(u)\sim n(\alpha,\beta+\gamma)\)</span></p>
<p><strong>Example 4.3.4 (Sum and difference of normal variables)</strong>:</p>
<p><span class="math inline">\(X\sim n(0,1)\)</span>, <span class="math inline">\(Y\sim n(0,1)\)</span> <span class="math inline">\(X,Y\)</span> indep, <span class="math inline">\(X-Y\sim n(0,2)\)</span>, <span class="math inline">\(X-Y\sim n(0,2)\)</span></p>
<p><strong>Example</strong> <code>2019.01.10</code> <code>p.1-5</code></p>
<p>Let X and Y have joint pdf <span class="math inline">\(f(x,y)=\frac14e^{-\frac{x+y}2}\)</span>, <span class="math inline">\(0&lt;x&lt;\infty,0&lt;y&lt;\infty\)</span>. Find the pdf for <span class="math inline">\(u=\frac{X-Y}2\)</span>.</p>
<p>Let <span class="math inline">\(v=Y\)</span> so that the system is invertible. <span class="math inline">\(X=2u+v, Y=v\)</span></p>
<p><span class="math display">\[J=\begin{vmatrix}\frac{\partial x}{\partial u} &amp; \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} &amp; \frac{\partial y}{\partial v} \end{vmatrix}=\begin{vmatrix}2 &amp; 1 \\ 0 &amp; 1 \end{vmatrix}=2\]</span></p>
<p><span class="math display">\[g(u,v)=f(x,y)|J|=\frac14e^{-\frac{x+y}2}2=\frac12e^{-\frac{2u+v+v}2}=\frac12e^{-(u+v)}\]</span></p>
<p><img src="note_stat562_files/figure-html/unnamed-chunk-1-1.png" width="768" /></p>
<p><span class="math display">\[0&lt;x&lt;\infty,0&lt;y&lt;\infty\implies0&lt;2u+v&lt;\infty,0&lt;v&lt;\infty\implies v&gt;-2u\]</span></p>
<p><span class="math display">\[g_U(u)=\left\{ \begin{array}\ \int_{-2u}^\infty\frac12e^{-(u+v)}dv=\frac12e^{-u}\int_{-2u}^\infty e^{-v}dv=\frac12e^{-u}\left[-e^{-v}\right]_{-2u}^\infty=\frac12e^{-u}\left[0+e^{2u}\right] &amp; u&lt;0 \\ \int_{0}^\infty\frac12e^{-(u+v)}dv=\frac12e^{-u}\int_{0}^\infty e^{-v}dv=\frac12e^{-u}\left[-e^{-v}\right]_{0}^\infty=\frac12e^{-u}\left[0+1\right] &amp; u\ge0 \end{array} \right\}=\frac12e^{|u|}\]</span> which is Double Exponential or Laplace Distribution</p>
<p><code>2019.01.15``p.11-14</code></p>
<p><strong>Theorem 4.3.5</strong> Let X and Y be independent random variables. Let <span class="math inline">\(g(x)\)</span> be a function only of a; and <span class="math inline">\(h(y)\)</span> be a function only of y. Then the random variables <span class="math inline">\(U=g(X)\)</span> and <span class="math inline">\(V=h(Y)\)</span> are independent.</p>
<p>Proof</p>
<p>We will prove the theorem assuming U and V are continuous random variables. For any <span class="math inline">\(u\in R,v\in R\)</span>,define</p>
<p><span class="math display">\[\begin{array}\mathcal{A_u}=\{x:g(x)\le u\}\\ \mathcal{B_v}=\{y:h(y)&gt;v\}\end{array}\]</span></p>
<p>Then the joint cdf of (U,V) is</p>
<p><span class="math display">\[F_{U,V}(u,v)\begin{array}{l} =P(U\le u,V\le v) &amp;  \text{definition of cdf} \\
= P(X\in A_u, Y\in B_v) &amp; \text{definition of U and V} \\
= P(X\in A_u)P(Y\in B_v) &amp; \text{Theorem 4.2.10} \end{array}\]</span></p>
<p>The joint pdf of (U,V) is</p>
<p><span class="math display">\[F_{UV}(u,v)=\frac{\partial^2}{\partial{u}\partial{v}}F_{UV}(u,v)=\left(\frac{d}{du}P(X\in A_u)\right) \left(\frac{d}{dv}P(Y\in B_v)\right)\]</span></p>
<p>where, as the notation indicates, the first factor is a function only of u and the second factor is a function only of v. Hence, by Lemma 4.2.7, U and V are independent.</p>
<p><span class="math display">\[f_{U,V}(u,v)=\sum_{i=1}^kf_{X,Y}(h_{1i}(u,v),\ h_{2i}(u,v))|J_i|\]</span></p>
<p><code>2019.01.10``p.6-9</code></p>
<p><strong>Example 4.3.6 (Distribution of the ratio of normal variables)</strong> Let X and Y be independent <span class="math inline">\(n(0,1)\)</span> random variables. Consider the transformation <span class="math inline">\(U =\frac{X}Y\)</span> (<em>and V=|Y| in textbook</em>) find the pdf of u</p>
<p><span class="math display">\[\left.\begin{array}\mathcal{A_0}=\{(x,y):y=0\}\\ \mathcal{A_1}=\{(x,y):y&gt;0\} \\ \mathcal{A_2}=\{(x,y):y&lt;0\} \\ \end{array}\right\}\to\mathcal{B}=\{(u,v):v&gt;0\}\]</span></p>
<p>Let <span class="math inline">\(V=Y\)</span>, <span class="math inline">\(Y=V\)</span>,<span class="math inline">\(U=\frac XV\)</span>,<span class="math inline">\(X=UV\)</span></p>
<p><span class="math display">\[J=\begin{vmatrix}v &amp; u \\ 0 &amp; 1 \end{vmatrix}=v\]</span></p>
<p><span class="math display">\[g(u,v)=f(x,y)|J|=\frac1{\sqrt{2\pi}}e^{-\frac{x^2}2}\frac1{\sqrt{2\pi}}e^{-\frac{y^2}2}|V|=\frac1{2\pi}e^{-\frac{u^2v^2+v^2}2}|V|=\frac1{2\pi}e^{-\frac{(u^2+1)v^2}2}|V|\]</span></p>
<p><span class="math display">\[-\infty&lt;x&lt;\infty,-\infty&lt;y&lt;\infty\implies-\infty&lt;uv&lt;\infty,-\infty&lt;v&lt;\infty\]</span> Because the inegrated was an even function</p>
<p><span class="math display">\[g_U(u)=\int_{-\infty}^{\infty}\frac1{2\pi}e^{-\frac{(u^2+1)v^2}2}|V|dv=2\int_{0}^{\infty}\frac1{2\pi}e^{-\frac{(u^2+1)v^2}2}vdv\]</span></p>
<p>Let <span class="math inline">\(s=\frac{(u^2+1)v^2}2\)</span>, <span class="math inline">\(ds=(u^2+1)vdv\)</span></p>
<p><span class="math display">\[g_U(u)=\frac1\pi\int_0^{\infty}e^{-s}\frac1{u^2+1}ds=\frac1\pi\frac1{u^2+1}\left[-e^{-s}\right]_0^\infty=\frac1\pi\frac1{u^2+1},-\infty&lt;u&lt;\infty\]</span> which is the Cauchy distribution.</p>
</div>
<div id="hierarchical-models-and-mixture-distributions" class="section level2">
<h2>4.4 Hierarchical Models and Mixture Distributions</h2>
<p><code>2019.01.08</code> <code>p.1</code></p>
<p><span class="math inline">\(E[X]=\int_{-\infty}^{\infty}xf(x)dx\)</span> is a real number.</p>
<p><span class="math inline">\(E[X|y]=\int_{-\infty}^{\infty}xf(x|y)dx\)</span> is a function of y.</p>
<p><span class="math inline">\(E[X|Y]=\int_{-\infty}^{\infty}xf(x|y)dx\)</span> is a random variable, and is a function of the r.v Y.</p>
<p>Definition: <span class="math inline">\(V[X|Y]=E[(X-E[X|Y])^2|Y]\)</span></p>
<p><code>2019.01.08``p.2-3</code></p>
<p><strong>Theorem 4.4.3 Law of iterated Expectation</strong> if X and Y are any two random variables, then</p>
<p><span class="math display">\[EX=E(E(X|Y))\]</span></p>
<p>provided that the expectation exist.</p>
<p>Proof:<strong>Law of iterated expectations</strong></p>
<p><span class="math display">\[E(E(X|Y))=\int_{-\infty}^{\infty}E[X|y]g(y)dy=\int_{-\infty}^{\infty}\left[\int_{-\infty}^{\infty}xf(x|y)dx\right]g(y)dy=\int_{-\infty}^{\infty}\left[\int_{-\infty}^{\infty}x\frac{f(x,y)}{g(y)}dx\right]g(y)dy\]</span></p>
<p><span class="math display">\[=\int_{-\infty}^{\infty}\left[\int_{-\infty}^{\infty}{f(x,y)dy}\right]xdx=\int_{-\infty}^{\infty}f_X(x)xdx=E[X]\]</span></p>
<p><strong>Theorem 4.4.7 (Conditional variance identity)</strong> For any two random varibles X and Y,</p>
<p><code>2019.01.08``p.4-6</code></p>
<p><span class="math display">\[VarX=E(Var(X|Y))+Var(E(X|Y))\]</span></p>
<p>provided that the expectation exist.</p>
<p>Proof:</p>
<p><span class="math display">\[V[X]=E[(X-\mu_x)^2]=E[(X-E[X|Y]+E[X|Y]-\mu_x)^2]=E[(X-E[X|Y])^2]+E[(E[X|Y]-\mu_x)^2]+2E[(X-E[X|Y])(E[X|Y]-\mu_x)]\]</span></p>
<p><span class="math display">\[2E[(x-E[X|y])(E[X|y]-\mu_x)]=2\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x-E[X|y])(E[X|y]-\mu_x)f(x,y)dxdy=2\int_{-\infty}^{\infty}(E[X|y]-\mu_x)\left\{\int_{-\infty}^{\infty}(x-E[X|y])f(x,y)dx\right\}dy=0\]</span></p>
<p><span class="math display">\[\int_{-\infty}^{\infty}(x-E[X|y])f(x,y)dx=\int_{-\infty}^{\infty}xf(x,y)dx-E[X|y]\int_{-\infty}^{\infty}f(x,y)dx=\int_{-\infty}^{\infty}xf(x|y)g(y)dx-E[X|y]g(y)=g(y)E[X|y]-E[X|y]g(y)\]</span></p>
<p>(using law of iterated expcetations) <span class="math inline">\(E[(X-E[X|Y])^2]=E[E(X-E[X|Y])^2|Y]=E(Var(X|Y))\)</span></p>
<p>Let <span class="math inline">\(W=E[X|Y]\)</span>, then <span class="math inline">\(E[w]=E(E[X|Y])=E[X]=\mu_x\)</span>, <span class="math inline">\(E[(E[X|Y]-\mu_x)^2]=E[(W-E[w])^2]=V[W]=V[E(X|Y)]\)</span></p>
</div>
<div id="covariance-and-correlation" class="section level2">
<h2>4.5 Covariance and Correlation</h2>
<p><code>2019.01.08``p.8</code></p>
<p><strong>Definition 4.5.1</strong> The covariance of X and Y is the number defined by</p>
<p><span class="math display">\[Cov(X,Y)=E((X-\mu_X)(Y-\mu_Y))=\sigma_{XY}\]</span></p>
<p><code>p.10</code></p>
<p><strong>Definition 4.5.2</strong> The correlation of X and Y is the number defined by</p>
<p><span class="math display">\[\rho_{XY}=\frac{\sigma_{XY}}{\sigma_X\sigma_Y};\quad Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\]</span></p>
<p><code>p.9</code> Alternate formate:</p>
<p><strong>Theorem 4.5.3</strong> For any two random varibles X and Y,</p>
<p><span class="math display">\[\sigma_{XY}=Cov(X,Y)=E[XY]-\mu_X\mu_Y\]</span></p>
<p><span class="math inline">\(\sigma_{XY}=E[(X-\mu_X)(Y-\mu_Y)]=E[XY-Y\mu_X-X\mu_Y+\mu_X\mu_Y]=E[XY]-E[Y]\mu_X-E[X]\mu_Y+\mu_X\mu_Y=E[XY]-\mu_Y\mu_X-\mu_X\mu_Y+\mu_X\mu_Y=E[XY]-\mu_X\mu_Y\)</span></p>
<p><code>2019.01.15``p.2-3</code></p>
<p>Properties of <span class="math inline">\(Cov(x,y)\)</span></p>
<p><span class="math display">\[\begin{array}{l} Cov(aX,bY)=abCov(X,Y) &amp; (1)\\
Cov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)  &amp; (2) \\
Cov(X,c)=0  &amp; (3) \end{array}\]</span></p>
<p><span class="math display">\[Cov(X,X)=E[X^2]-\mu_X^2\]</span></p>
<p>Properties (1) and (2) make <span class="math inline">\(Cov(x,y)\)</span> a bilinear operator.</p>
<p><code>2019.01.10``p.10-13</code></p>
<p><strong>Example 4.5.4 (Correlation-I)</strong></p>
<p><span class="math inline">\(f(x,y)=x+y\)</span>, <span class="math inline">\(0&lt;x&lt;1,0&lt;y&lt;1\)</span>, find <span class="math inline">\(\rho_{XY}\)</span></p>
<p><span class="math inline">\(E[XY]=\int_0^1\int_0^1xy(x+y)dxdy=\int_0^1[\frac13x^3y+\frac12x^2y^2]_{x=0}^1dy=\int_0^1[\frac13y+\frac12y^2]dy=[\frac16y^2+\frac16y^3]_{x=0}^1=\frac13\)</span></p>
<p><span class="math display">\[f_X(x)=\int_0^1(x+y)dy=\left.xy+\frac12y^2\right|_{y=0}^1=x+\frac12, 0&lt;x&lt;1\]</span></p>
<p><span class="math inline">\(E[X]=\int_0^1x(x+\frac12)dx=\left.\frac13x^3+\frac14x^2\right|_{0}^1=\frac7{12}\)</span></p>
<p><span class="math inline">\(E[X^2]=\int_0^1x^2(x+\frac12)dx=\left.\frac14x^4+\frac16x^3\right|_{0}^1=\frac5{12}\)</span></p>
<p>Because X and Y were interchangeable in <span class="math inline">\(f(x,y)\)</span>, we can conclude</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>marginal pdf</th>
<th>E[]</th>
<th><span class="math inline">\(E[^2]\)</span></th>
<th><span class="math inline">\(\sigma=\sqrt{E[^2]-(E[])^2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>X</td>
<td><span class="math inline">\(x+\frac12\)</span></td>
<td><span class="math inline">\(\frac7{12}\)</span></td>
<td><span class="math inline">\(\frac5{12}\)</span></td>
<td><span class="math inline">\(\frac{\sqrt{11}}{12}\)</span></td>
</tr>
<tr class="even">
<td>Y</td>
<td><span class="math inline">\(y+\frac12\)</span></td>
<td><span class="math inline">\(\frac7{12}\)</span></td>
<td><span class="math inline">\(\frac5{12}\)</span></td>
<td><span class="math inline">\(\frac{\sqrt{11}}{12}\)</span></td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(\sigma_{XY}=\frac13-\frac7{12}\frac5{12}=-\frac1{144}\)</span></p>
<p><span class="math inline">\(\sigma_{X}=\sigma_{Y}=\sqrt{\frac5{12}-(\frac7{12})^2}=\frac{\sqrt{11}}{12}\)</span>$</p>
<p><span class="math display">\[\rho_{XY}=\frac{-\frac1{144}}{\frac{\sqrt{11}}{12}\frac{\sqrt{11}}{12}}=-\frac1{11}\]</span></p>
<p><strong>Theorem 4.5.5</strong> If X and Y are independent (uncorrelated) random variables, then <span class="math inline">\(Cov(X,Y)=0\)</span> and <span class="math inline">\(\rho_{XY}=0\)</span></p>
<p>Always check if X and Y are independent. When <span class="math inline">\(y=x^2\)</span>, X and Y are dependent, but <span class="math inline">\(Cov(X,Y)=0\)</span></p>
<p><strong>Theorem 4.5.6</strong> If X and Y are any two random variables and a and b are any two constants, then</p>
<p><span class="math display">\[Var(aX+bY)=a^2VarX+b^2VarY+2abCov(X,Y)\]</span> If X and Y are independent random variables, then</p>
<p><span class="math display">\[Var(aX+bY)=a^2VarX+b^2VarY\]</span></p>
<p><code>2019.01.15``p.7</code></p>
<p><span class="math display">\[Var(X+Y)=VarX+VarY+2Cov(X,Y)\]</span></p>
<p>If X and Y are uncorrelated, then</p>
<p><span class="math display">\[Var(X+Y)=VarX+VarY\]</span></p>
<p><strong>Theorem 4.5.7</strong> For any random variables X and Y,</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(-1\le \rho_{XY}\le1\)</span>.</li>
<li><span class="math inline">\(|\rho_{XY}|=1\)</span> if and only if there exist numbers <span class="math inline">\(a\neq0\)</span> and b such that <span class="math inline">\(P(Y=aX+b)=1\)</span>. If <span class="math inline">\(\rho_{XY}=1\)</span>, then <span class="math inline">\(a&gt;0\)</span>, and if <span class="math inline">\(\rho_{XY}=-1\)</span>, then <span class="math inline">\(a&lt;0\)</span>.</li>
</ol>
<p><code>2019.01.10``p.14-17</code></p>
<p><em>What does actually measure?</em></p>
<p>Suppose <span class="math inline">\(E[Y|X]\)</span> is a linear function of X, or suppose that <span class="math inline">\(E[Y|X]=a+bx\)</span>,</p>
<p>then <span class="math inline">\(E[Y]=E[E[Y|X]]=E[a+bX]=a+bE[X]\)</span>, by <strong>4.4.3</strong></p>
<p>Also, <span class="math inline">\(E[XE[Y|X]]=E[X(a+bX)]=aE[X]+bE[X^2]\)</span></p>
<p>Alternately, <span class="math inline">\(E[XE[Y|X]]=\int_{-\infty}^{\infty}xE[Y|x]f_X(x)dx\)</span>, by <strong>2.2.1</strong></p>
<p><span class="math inline">\(=\int_{-\infty}^{\infty}x\left[\int_{-\infty}^{\infty}yf_Y(y|x)dy\right]f_X(x)dx\)</span>, by <strong>4.2.3</strong></p>
<p><span class="math inline">\(=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyf(x,y)dydx=E[XY]\)</span>, by <strong>4.1.10</strong>. Now</p>
<p><span class="math display">\[\sigma_{XY}=E[XY]-\mu_X\mu_Y=a\mu_X+bE[X^2]-\mu_X\mu_Y=a\mu_X+b(\sigma_X^2+\mu_X^2)-\mu_X(a+b\mu_X)=b\sigma_X^2\]</span></p>
<p><span class="math display">\[\rho_{XY}=\frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}}=\frac{b\sigma_X^2}{\sigma_{X}\sigma_{Y}}=b\frac{\sigma_X}{\sigma_Y}\]</span></p>
<p><span class="math inline">\(\rho_{XY}\)</span>, <span class="math inline">\(Cov(X,Y)\)</span>, and b have same sign.</p>
<p><code>2019.01.15``p.6</code></p>
<p>Cauchy-Schuwarz also says that equality occurs iff <span class="math inline">\(\vec u= c\vec v\)</span> for some c</p>
<p>So <span class="math inline">\(|\rho_{XY}|=1\)</span> iff <span class="math inline">\(Y-\mu_Y=c(X-\mu_X)\)</span>, <span class="math inline">\(Y=cX-\mu_Y-c\mu_X)\)</span></p>
<p>Therefore, the correlation equals <span class="math inline">\(\pm1\)</span> iff Y is a perfect linear function of X.</p>
<p><strong>Example 4.5.8 (Correlation-II)</strong></p>
<p><strong>Example 4.5.9 (Correlation-III)</strong></p>
<p><strong>Definition 4.5.10</strong> bivariate normal pdf with <span class="math inline">\(\mu_X,\mu_Y,\sigma_X^2,\sigma_Y^2, \rho\)</span></p>
<p><span class="math inline">\(f_X(x)\sim n(\mu_X,\sigma^2_X)\)</span> <span class="math inline">\(f_Y(y)\sim n(\mu_Y,\sigma^2_Y)\)</span></p>
<p><span class="math inline">\(f_{Y|X}(y|x) \sim n(\mu_Y+\rho\frac{\sigma_Y}{\sigma_X})(x−\mu_X),\sigma_Y^2(1-\rho^2)\)</span></p>
<p><span class="math inline">\(aX+bY\sim n(a\mu_X+b\mu_Y,a^2\mu_X^2+b^2\mu_Y^2+2ab\rho\sigma_X\sigma_Y\)</span></p>
</div>
<div id="multivariate-distributions" class="section level2">
<h2>4.6 Multivariate Distributions</h2>
<p><strong>Definition 4.6.2</strong> multinomial distribution with m trials and cell probabilities</p>
<p><strong>Theorem 4.6.4 (Multinomial Theorem)</strong></p>
<p><strong>Definition 4.6.5</strong> mutually independent random vectors</p>
<p><strong>Theorem 4.6.6 (Generalization of Theorem 4.2. 10)</strong></p>
<p><span class="math display">\[E(g_1(X_1)\cdots g_n(X_n))=(E(g_1(X_1))\cdots(E(g_n(X_n))\]</span></p>
<p><strong>Theorem 4.6.7 (Generalization of Theorem 4.2.12)</strong></p>
<p><span class="math display">\[M_Z(t)=(M_X(t))^n\]</span></p>
<p><strong>Theorem 4.6.11 (Generalization of Lemma 4.2.1)</strong> Let <span class="math inline">\(X_1,.., X_n\)</span> be random vectors. Then <span class="math inline">\(X_1,.., X_n\)</span> are mutually independent random vectors if and only if there exist functions <span class="math inline">\(g_i(X_i),i=1,..,n\)</span>, such that the joint pdf or pmf of (<span class="math inline">\(X_1,..,X_n\)</span>) can be written as</p>
<p><span class="math display">\[f(x_1,..,x_n) =g_1(x_1)\cdots g_n(x_n)\]</span></p>
<p><strong>Theorem 4.6.12 (Generalization of Theorem 4.3.5)</strong> Let <span class="math inline">\(X_1,.., X_n\)</span> be independent random vectors. Let <span class="math inline">\(g_i(X_i)\)</span> be a function only of <span class="math inline">\(X_i,i=1,..,n\)</span>. Then the random variables <span class="math inline">\(U_i=g_i(X_i),i=1,..,n\)</span>, are <strong>mutually independent</strong>.</p>
<p><strong>Example 4.6.13 (Multivariate change of variables)</strong></p>
</div>
<div id="inequalities" class="section level2">
<h2>4.7 Inequalities</h2>
<div id="numerical-inequalities" class="section level3">
<h3>4.7.1 Numerical Inequalities</h3>
<p><strong>Theorem 4.7.3 (Cauchy-Schwarz Inequality)</strong> For any two random variables X and Y,</p>
<p><span class="math display">\[|EXY|\le E|XY|\le(E|X|^2)^{\frac12}(E|X|^2)^{\frac12}\]</span> <code>2019.01.15``p.4</code></p>
<p>Consider <span class="math inline">\(\{X-\mu_X| X \text{ is a random variable}\}\)</span>. That is, the set of all random variables with mean 0.</p>
<p>This is a well-defined vector space, where the scalars are the real number.</p>
<p>The covariance operator will satisfy the definition of an inner product on this vector space.</p>
<p>The Cauchy–Schwarz inequality says</p>
<p>The Cauchy–Schwarz inequality states that for all vectors u and v of an inner product space it is true that</p>
<p><span class="math display">\[|\langle\vec{u},\vec{v}\rangle|^{2}\le \langle \vec{u} ,\vec{u} \rangle \cdot \langle \vec{v},\vec{v}\rangle \]</span> where <span class="math inline">\(\langle \cdot ,\cdot \rangle\)</span> is the inner product. Examples of inner products include the real and complex dot product, see the examples in inner product. Equivalently, by taking the square root of both sides, and referring to the norms of the vectors, the inequality is written as</p>
<p><span class="math display">\[|\langle\vec{u},\vec{v}\rangle|\le \|\vec{u} \|\|\vec{v} \|\]</span></p>
<p><code>2019.01.15``p.5</code></p>
<p>Apply this to the covariance operator:</p>
<p><span class="math display">\[|Cov(X-\mu_X,Y-\mu_Y)|\ge \sqrt{Cov(X-\mu_X,X-\mu_X)}\cdot\sqrt{Cov(Y-\mu_Y,Y-\mu_Y)}\]</span> <span class="math display">\[|Cov(X,Y)|\le \sqrt{Var(X)Var(Y)}\implies |\sigma_{XY}|\le \sigma_{X}\sigma_{Y}\quad \therefore |\rho_{XY}|\le1\]</span></p>
<p>proof <strong>4.5.7</strong></p>
<p><strong>Example 4.7.4 (Covariance inequality)</strong> If X and Y have means <span class="math inline">\(\mu_X,\mu_Y\)</span> and variances <span class="math inline">\(\sigma_X^2,\sigma_Y^2\)</span> , respectively, we can apply the Cauchy-Schwarz Inequality to get</p>
<p><span class="math display">\[E|(X-\mu_X)(Y-\mu_Y)|\le\{E(X-\mu_X)^2\}^{\frac12}\{E(Y-\mu_Y)^2\}^{\frac12}\]</span></p>
<p>Squaring both sides and using statistical notation, we have <span class="math inline">\((Cov(X, y))^2\le \sigma_X^2\sigma_Y^2\)</span></p>
<p>Recalling the definition of the correlation coefficient,<span class="math inline">\(\rho\)</span>, we have proved that $-1^21$1. Furthermore, the condition for equality in Lemma 4.7.1 still carries over, and equality is attained here only if <span class="math inline">\((X-\mu_X)=c(Y-\mu_Y)\)</span>, for some constant c. That is, the correlation is <span class="math inline">\(\pm1\)</span> <strong>if and only if X and Y are linearly related</strong>.</p>
</div>
<div id="functional-inequalities" class="section level3">
<h3>4.7.2 Functional Inequalities</h3>
<p><strong>Theorem 4.7.7 (Jensen’s Inequality)</strong></p>
<p><strong>Example 4.7.8 (An inequality for means)</strong> Jensen’s Inequality can be used to prove an inequality between three different kinds of means. If <span class="math inline">\(a_1,..,a_n\)</span> are positive numbers, define</p>
<p><span class="math display">\[\begin{array}{l} a_A=\frac1n(a_1+a_2+..+a_n)&amp;\text{(arithmetic mean)}\\a_G=[a_1a_2\cdots a_n]^{\frac1n}&amp;\text{(geometric mean)}\\a_H=\frac{1}{\frac1n(\frac1{a_1}+\frac1{a_2}+..+\frac1{a_n})}&amp;\text{(harmonic mean)} \end{array}\]</span></p>
<p>An inequality relating these means is <span class="math inline">\(a_H\le a_G\le a_A\)</span></p>
<p><strong>Theorem 4.7.9 (Covariance Inequality)</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>If <span class="math inline">\(g(x)\)</span> is a nondecreasing function and h(x)$ is a nonincreasing junction, then <span class="math display">\[E(g(X)h(X))\le(Eg(X))(Eh(X))\]</span></p></li>
<li><p>If <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(h(x)\)</span> are either both nondecreasing or both nonincreasing, then <span class="math display">\[E(g(X)h(X))\ge(Eg(X))(Eh(X))\]</span></p></li>
</ol>
</div>
</div>
</div>
<div id="properties-of-a-random-sample" class="section level1">
<h1>5. Properties of a Random Sample</h1>
<p><code>2019.01.15``p.8</code></p>
<div id="basic-concepts-of-random-samples" class="section level2">
<h2>5.1 Basic Concepts of Random Samples</h2>
<p><strong>Definition 5.1.1</strong> The random variables <span class="math inline">\(X_1,..,X_n\)</span> are called a random sample of size n from the population <span class="math inline">\(f(x)\)</span> if <span class="math inline">\(X_1,..,X_n\)</span> are mutually independent random variables and the marginal pdf or pmf of each <span class="math inline">\(X_i\)</span> is the same function <span class="math inline">\(f(x)\)</span>. Alternatively, <span class="math inline">\(X_1,..,X_n\)</span> are called independent and identically distributed random variables with pdf or pmf <span class="math inline">\(f(x)\)</span>. This is commonly abbreviated to <span class="math inline">\(iid\)</span> random variables.</p>
<p><span class="math display">\[f(x_1,..,x_n) =f(x_1)\cdots f(x_n)=\prod_{i=1}^nf(x_i)\]</span></p>
</div>
<div id="sums-of-random-variables-from-a-random-sample" class="section level2">
<h2>5.2 Sums of Random Variables from a Random Sample</h2>
<p><strong>Definition 5.2.1</strong> Let <span class="math inline">\(X_1,..,X_n\)</span> be a random sample of size n from a population and let <span class="math inline">\(T(x_1,..,x_n)\)</span> be a real-valued or vector-valued function whose domain includes the sample space of (<span class="math inline">\(X_1,..,X_n\)</span>). Then the random variable or random vector <span class="math inline">\(Y=T(X_1,..,X_n)\)</span> is called a statistic. The probability distribution of a statistic Y is called the sampling distribution of Y.</p>
<p><code>2019.01.15``p.9</code></p>
<p><strong>Definition 5.2.2</strong> The sample mean is the arithmetic average of the values in a random sample. It is usually denoted by</p>
<p><span class="math display">\[\bar X=\frac{X_1+..+X_n}{n}=\frac1n\sum_{i=1}^nX_i\]</span></p>
<p><strong>Definition 5.2.3</strong> The sample variance is the statistic defined by</p>
<p><span class="math display">\[S^2=\frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2=\frac1{n-1}(\sum_{i=1}^nX_i^2-n\bar X^2)\]</span> The sample standard deviation is the statistic defined by <span class="math inline">\(S=\sqrt{S^2}\)</span></p>
<p><strong>Theorem 5.2.4</strong> a. <span class="math inline">\(\sum_{i=1}^n(X_i-a)^2\)</span> is minimized when <span class="math inline">\(a=\bar x\)</span></p>
<p>Proof</p>
<p>Let <span class="math inline">\(g(a)=\sum_{i=1}^n(X_i-a)^2\)</span>, set</p>
<p><span class="math display">\[g&#39;(a)=\sum_{i=1}^n2(X_i-a)(-1)=0\implies a=\frac1n\sum_{i=1}^nX_i=\bar x\]</span></p>
<p><span class="math display">\[(n-1)s^2=\sum_{i=1}^n(x_i-\bar x)^2=\sum_{i=1}^nx_i^2-n\bar x^2\]</span></p>
<p><strong>Lemma 5.2.5</strong></p>
<p><span class="math display">\[(5.2.1)\quad E\left(\sum_{i=1}^ng(X_i) \right)=nE(g(X_1))\]</span> (5.2.1) is true for any collection of n identically distributed random variables.</p>
<p><span class="math display">\[Var\left(\sum_{i=1}^ng(X_i) \right)=nVar(g(X_1))\]</span></p>
<p><strong>Theorem 5.2.6</strong></p>
<p><span class="math display">\[E\bar X=\mu\]</span> <span class="math display">\[Var\bar X=\frac{\sigma^2}n\]</span> <span class="math display">\[ES^2=\sigma^2\]</span></p>
<p><strong>Theorem 5.2.7</strong></p>
<p><span class="math display">\[M_{\bar X}(t)=[M_X(\frac{t}n)]^n\]</span></p>
<p><strong>Example 5.2.8 (Distribution of the mean)</strong> Let <span class="math inline">\(X_1,..,X_n\)</span> be a random sample from a <span class="math inline">\(n(\mu,\sigma^2)\)</span> population. Then the mgf of the sample mean is</p>
<p><span class="math display">\[M_{\bar X}(t)=\left[e^{\mu\frac{t}n+\frac{\sigma^2{(\frac{t}n)}^2}2}\right]^n=e^{\mu t+\frac{(\frac{\sigma^2}{n})t^2}2}\]</span></p>
<p>Thus, <span class="math inline">\(\bar X\)</span> has a <span class="math inline">\(n(\mu,\frac{\sigma^2}n)\)</span> distribution.</p>
<p>Another simple example is given by a <span class="math inline">\(Gamma(α,β)\)</span> random sample (4.6.8). Here, we can also easily derive the distribution of the sample mean. The mgf of the sample mean is</p>
<p><span class="math display">\[M_{\bar X}(t)=\left[\left(\frac1{1-\beta(\frac{t}n)}\right)^{a}\right]^n=\left(\frac1{1-\beta(\frac{t}n)}\right)^{na}\]</span></p>
<p>which we recognize as the mgf of a <span class="math inline">\(gGamma(nα,β/n)\)</span>, the distribution of <span class="math inline">\(\bar X\)</span>.</p>
<p><span class="math display">\[\mu_{\bar X}=nα\cdot\fracβn=αβ=\mu\]</span> <span class="math display">\[\sigma^2_{\bar X}=nα\cdot(\fracβn)^2=\frac{αβ^2}n=\frac{\sigma^2}n\]</span></p>
<p>If Theorem 5.2.7 is not applicable, because either the resulting mgf of <span class="math inline">\(\bar X\)</span> is unrecognizable or the population mgf does not exist, then the transformation method of Section 4.3 and 4.6 might be used to find the pdf of <span class="math inline">\(Y=(X_1+..+X_n)\)</span> and <span class="math inline">\(\bar X\)</span>. In such cases, the following convolution formula is useful.</p>
<p><strong>Theorem 5.2.9 Convolution formula</strong> If X and Y are independent continuous random variables with pdfs <span class="math inline">\(f_X(x)\)</span> and <span class="math inline">\(f_Y(y)\)</span>, then the pdf of <span class="math inline">\(Z=X+Y\)</span> is <span class="math display">\[f_Z(z) =\int_{-\infty}^{\infty}f_X(w)f_Y(z-w)dw\]</span></p>
<p><strong>Example 5.2.10 (Sum of Cauchy random variables)</strong> As an example of a situation where the mgf technique fails, consider sampling from a Cauchy distribution. We will eventually derive the distribution of <span class="math inline">\(\bar Z\)</span>, the mean of <span class="math inline">\(Z_1,..,Z_n\)</span> iid <span class="math inline">\(Cauchy(0,1)\)</span> observations. We start, however, with the distribution of the sum of two independent Cauchy random variables and apply formula (5.2.3). Let U and V be independent Cauchy random variables, <span class="math inline">\(U\sim Cauchy(0,\sigma)\)</span> and <span class="math inline">\(V\sim Cauchy(0,\tau)\)</span>; that is,</p>
<p><span class="math display">\[f_U(u)=\frac1{\pi\sigma}\frac1{1+(\frac{u}\sigma)^2},\quad f_V(v)=\frac1{\pi\tau}\frac1{1+(\frac{v}\tau)^2},\quad\begin{array}\ -\infty&lt;u&lt;\infty\\-\infty&lt;v&lt;\infty\end{array}\]</span></p>
<p>Based on formula (5.2.3), the pdf of <span class="math inline">\(Z=U+V\)</span> is given by</p>
<p><span class="math display">\[f_Z(z)=\int_{-\infty}^{\infty}\frac1{\pi\sigma}\frac1{1+(\frac{w}\sigma)^2}\frac1{\pi\tau}\frac1{1+(\frac{z-w}\tau)^2}dw,\quad -\infty&lt;z&lt;\infty\]</span></p>
<p>This integral is somewhat involved but can be solved by a partial fraction decomposition and some careful anti differentiation (see Exercise 5.7) . The result is</p>
<p><span class="math display">\[f_Z(z)=\frac1{\pi(\sigma+\tau)}\frac1{1+(\frac{z}{\sigma+\tau})^2},\quad -\infty&lt;z&lt;\infty\]</span></p>
<p>Thus, the sum of two independent Cauchy random variables is again a Cauchy, with the scale parameters adding. It therefore follows that if <span class="math inline">\(Z_1,..,Z_n\)</span> iid <span class="math inline">\(Cauchy(0,1)\)</span> random variables, then <span class="math inline">\(\sum Z_i\)</span> is <span class="math inline">\(Cauchy(0,n)\)</span> and also <span class="math inline">\(\bar Z\)</span> is <span class="math inline">\(Cauchy(0,1)\)</span>. The sample mean has the same distribution as the individual observations. (See Example A.0.5 in Appendix A for a computer algebra version of this calculation.)</p>
<p>Suppose <span class="math inline">\(X_1,..,X_n\)</span> is a random sample from <span class="math inline">\((\frac1\sigma)f(\frac{x-\mu}{\sigma})\)</span>, a member of a location-scale family. Then the distribution of <span class="math inline">\(\bar X\)</span> has a simple relationship to the distribution of <span class="math inline">\(\bar Z\)</span>, the sample mean from a random sample from the standard pdf <span class="math inline">\(f(z)\)</span>. To see the nature of this relationship, note that from Theorem 3.5.6 there exist random variables <span class="math inline">\(Z_1,..,Z_n\)</span> such that <span class="math inline">\(Xi=\sigma Z_i+\mu\)</span> and the pdf of each <span class="math inline">\(\bar Z_i\)</span> is <span class="math inline">\(f(z)\)</span>. Furthermore, we see that <span class="math inline">\(Z_1,..,Z_n\)</span> are mutually independent. Thus <span class="math inline">\(Z_1,..,Z_n\)</span> is a random sample from <span class="math inline">\(f(z)\)</span>. The sample means <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(\bar Z\)</span> are related by</p>
<p><span class="math display">\[\bar X=\frac1n\sum_{i=1}^nX_i=\frac1n\sum_{i=1}^n(\sigma Z_i+\mu)=\frac1n(\sigma \sum_{i=1}^nZ_i+n\mu)=\sigma\bar Z+\mu\]</span></p>
<p>Thus, again applying Theorem 3.5.6, we find that if <span class="math inline">\(g(z)\)</span> is the pdf of <span class="math inline">\(\bar Z\)</span>, then <span class="math inline">\((\frac1\sigma)g(\frac{x-\mu}{\sigma})\)</span> is the pdf of <span class="math inline">\(\bar X\)</span>. It may be easier to work first with <span class="math inline">\(Z_1,..,Z_n\)</span> and <span class="math inline">\(f(z)\)</span> to find the pdf <span class="math inline">\(g(z)\)</span> of <span class="math inline">\(\bar Z\)</span>. If this is done, the parameters <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\sigma\)</span> do not have to be dealt with, which may make the computations less messy. Then we immediately know that the pdf of <span class="math inline">\(\bar X\)</span> is <span class="math inline">\((\frac1\sigma)g(\frac{x-\mu}{\sigma})\)</span>.</p>
<p>In Example 5.2. 10, we found that if <span class="math inline">\(Z_1,..,Z_n\)</span> is a random sample from a <span class="math inline">\(Cauchy(0,1)\)</span> distribution, then Z also has a <span class="math inline">\(Cauchy(0,1)\)</span> distribution. Now we can conclude that if <span class="math inline">\(X_1,..,X_n\)</span> is a random sample from a <span class="math inline">\(Cauchy(\mu,\sigma)\)</span> distribution, then <span class="math inline">\(\bar X\)</span> also has a <span class="math inline">\(Cauchy(\mu,\sigma)\)</span> distribution. It is important to note that the dispersion in the distribution of <span class="math inline">\(\bar X\)</span>, as measured by <span class="math inline">\(\sigma\)</span>, is the same, regardless of the sample size <span class="math inline">\(n\)</span>. This is in sharp contrast to the more common situation in Theorem 5.2.6 (the population has finite variance) , where <span class="math inline">\(Var\bar X=\frac{\sigma^2}n\)</span> decreases as the sample size increases.</p>
</div>
<div id="sampling-from-the-normal-distribution" class="section level2">
<h2>5.3 Sampling from the Normal Distribution</h2>
<div id="properties-of-the-sample-mean-and-variance" class="section level3">
<h3>5.3.1 Properties of the Sample Mean and Variance</h3>
<p><strong>Theorem 5.3.1</strong></p>
<p><span class="math display">\[ f_{\chi^2}(x)=\frac{x^{\frac{p}2-1}}{\Gamma \frac{p}2 2^{\frac{p}2}}e^{-\frac{x}2}, x&gt;0\]</span></p>
<p><strong>Lemma 5.3.2</strong></p>
<p><strong>Lemma 5.3.3</strong> <code>2019.01.24``p.1</code></p>
<p><span class="math inline">\(U=\sum_{i=1}^na_ix_i, V=\sum_{j=1}^nb_jx_j\)</span></p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(Cov(U,V)=\sum_{i=1}^n\sum_{j=1}^na_ib_jCov(x_i,x_j)=0\)</span></p></li>
<li></li>
</ol>
</div>
<div id="the-derived-distributions-students-t-and-snedecors-f" class="section level3">
<h3>5.3.2 The Derived Distributions: Student’s t and Snedecor’s F</h3>
<p><strong>Definition 5.3.4</strong> T</p>
<p><span class="math display">\[f_{T}(x)=\frac{\Gamma(\frac{p+1}2)}{\Gamma(\frac{p}2)\sqrt{p\pi}}\left (1+\frac{x^2}{p}\right)^{-\frac{p+1}2},-\infty&lt;x&lt;\infty\]</span></p>
<p><strong>Definition 5.3.6</strong></p>
<p><span class="math display">\[f_{F}(x)=\frac{\Gamma(\frac{p+q}2)}{\Gamma(\frac{p}2)\Gamma (\frac{q}2)}(\frac{p}q)^{\frac{p}2}\frac{x^{\frac{p}2-1}}{(1+\frac{p}qx)^{\frac{p+q}2}}, x&gt;0\]</span></p>
<p><strong>Theorem 5.3.8</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(X\sim F_(p,q)\)</span> <span class="math inline">\(\frac{1}X \sim F_(q,p)\)</span></p></li>
<li><p><span class="math inline">\(X\sim T_(q)\)</span> <span class="math inline">\(X^2 \sim F_(1,q)\)</span></p></li>
<li><p><span class="math inline">\(X\sim F_(p,q)\)</span> <span class="math inline">\(\frac{\frac{p}{q}X}{1+\frac{p}{q}X} \sim Beta(\frac{p}2,\frac{q}2)\)</span></p></li>
</ol>
</div>
</div>
<div id="order-statistics" class="section level2">
<h2>5.4 Order Statistics</h2>
<p><strong>Definition 5.4.2</strong> The notation <span class="math inline">\(\{b\}\)</span>, when appearing in a subscript, is defined to be the number <span class="math inline">\(b\)</span> rounded to the nearest integer in the usual way. More precisely, if <span class="math inline">\(i\)</span> is an integer and <span class="math inline">\(i-.5\le b&lt;i+.5\)</span>, then <span class="math inline">\(\{b\}=i\)</span>.</p>
<p><strong>Theorem 5.4.3</strong> Let <span class="math inline">\(X_1,..,X_n\)</span> be a random sample from a discrete distribution with pmf <span class="math inline">\(f_X(x_i)=P_i\)</span>, where <span class="math inline">\(x1&lt;x2&lt;..\)</span> are the possible values of <span class="math inline">\(X\)</span> in ascending order.</p>
<p>Let <span class="math inline">\(X_{(1)},..,X_{(n)}\)</span> denote the order statistics from the sample. Then</p>
<p><span class="math display">\[P(X_{(j)}\le X_i)=\sum^n_{k=j}\binom{n}{k}P^k_i(1-P_i)^{n-k}\]</span> <span class="math display">\[P(X_{(j)}=X_i)=\sum^n_{k=j}\binom{n}{k}[P^k_i(1-P_i)^{n-k}-P^k_{i-1}(1-P_{i-1})^{n-k}]\]</span></p>
<p><strong>Theorem 5.4.4</strong> <code>2019.01.29``p.9</code> Let <span class="math inline">\(X_{(1)},..,X_{(n)}\)</span> denote the order statistics of a random sample, <span class="math inline">\(X_1,..,X_n\)</span> from a continuous population with cdf <span class="math inline">\(Fx (x)\)</span> and pdf <span class="math inline">\(f_X(x)\)</span>. Then the pdf of <span class="math inline">\(X_{(n)}\)</span> is</p>
<p><span class="math display">\[(5.4.4)\quad f_{X_{(j)}}(x)=\frac{n!}{(j-1)!(n-j)!}f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}\]</span></p>
<p><span class="math display">\[f_{K}(x)=\frac{n!}{(j-1)!(n-j)!}K[F_X(x)]^{k-1}[1-F_X(x)]^{n-k}f(x)\]</span></p>
<p><strong>Example 5.4.5 (Uniform order statistic pdf)</strong> Let <span class="math inline">\(X_{(1)},..,X_{(n)}\)</span> be iid <span class="math inline">\(Unif(0,1)\)</span> , so <span class="math inline">\(f_X(x)=1\)</span> for <span class="math inline">\(x\in(0,1)\)</span> and <span class="math inline">\(F_X(x)=x\)</span> for <span class="math inline">\(x\in(0,1)\)</span>. Using (5.4.4), we see that the pdf of the <span class="math inline">\(j^{th}\)</span> order statistic is</p>
<p><span class="math display">\[f_{X_{(j)}}(x)=\frac{n!}{(j-1)!(n-j)!}x^{j-1}(1-x)^{n-j}=\frac{\Gamma(n+1)}{\Gamma(j)\Gamma(n-j+1)}x^{j-1}(1-x)^{n-j+1-1}\]</span></p>
<p><span class="math display">\[E{X_{(j)}}=\frac{j}{n+1},\quad Var{X_{(j)}}=\frac{j(n-j+1)}{(n+1)^2(n+2)}\]</span></p>
<p><strong>Theorem 5.4.6</strong></p>
<p><span class="math display">\[f_{X_{(i)},X_{(j)}}(u,v)=\frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_X(u)f_X(v)[F_X(u)]^{i-1}[F_X(v)-F_X(u)]^{j-1-i}[1-F_X(v)]^{n-j}, -\infty&lt;u&lt;v&lt;\infty\]</span></p>
<p><strong>Example 5.4.7 (Distribution of the midrange and range)</strong></p>
</div>
<div id="convergence-concepts" class="section level2">
<h2>5.5 Convergence Concepts</h2>
<div id="convergence-in-probability" class="section level3">
<h3>5.5.1 Convergence in Probability</h3>
<p><strong>Definition 5.5.1</strong> A sequence of random variables, <span class="math inline">\(X_1,X_2,..\)</span>, <span class="math inline">\(\underrightarrow{\mathcal{P}}\)</span> <span class="math inline">\(X\)</span> if, for every <span class="math inline">\(\varepsilon&gt;0\)</span>,</p>
<p><span class="math display">\[\begin{cases}\lim_{n\to\infty}P(|X_n-X|\ge\epsilon)=0\\\lim_{n\to\infty}P(|X_n-X|&lt;\epsilon)=1\end{cases}\]</span></p>
<p><strong>Theorem 5.5.2 (Weak Law of Large Numbers)</strong> Let <span class="math inline">\(X_1,X_2,..\)</span> be iid random variables with <span class="math inline">\(EX_i=\mu\)</span> and <span class="math inline">\(Var Xi =\sigma^2&lt;\infty\)</span>. Define <span class="math inline">\(\bar X_n=\frac1n\sum_{i=1}^nX_i\)</span> Then, for every <span class="math inline">\(\epsilon&gt;0\)</span></p>
<p><span class="math display">\[\lim_{n\to\infty}P(|\bar X_n-\mu|&lt;\epsilon)=1\]</span> that is, <span class="math inline">\(\bar X_n\)</span> converges in probability to <span class="math inline">\(\mu\)</span>.</p>
<p><strong>Example 5.5.3 (Consistency of <span class="math inline">\(S^2\)</span>)</strong> Suppose we have a sequence<span class="math inline">\(X_1,X_2,..\)</span> of iid random variables with <span class="math inline">\(EX_i=\mu\)</span> and <span class="math inline">\(Var Xi =\sigma^2&lt;\infty\)</span>. If we define <span class="math inline">\(S_n^2=\frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2\)</span></p>
<p>a sufficient condition that <span class="math inline">\(VarS^2_n\)</span> converges in probability to <span class="math inline">\(\sigma^2\)</span> is that <span class="math inline">\(VarS^2_n\to0\)</span> as <span class="math inline">\(n\to\infty\)</span></p>
<p><strong>Theorem 5.5.4</strong> Suppose that <span class="math inline">\(X_1, X2,..\)</span> converges in probability to a random variable X and that h is a continuous function. Then <span class="math inline">\(h(X_1),h(X2),..\)</span> converges in probability to <span class="math inline">\(h(X)\)</span>.</p>
<p><strong>Example 5.5.5 (Consistency of S)</strong> If <span class="math inline">\(S^2_n\)</span> is a consistent estimator of <span class="math inline">\(\sigma^2\)</span>, then by Theorem 5.5.4, the sample standard deviation <span class="math inline">\(S_n=\sqrt{S^2_n}=h(S^2_n)\)</span> is a consistent estimator of <span class="math inline">\(\sigma\)</span>. Note that <span class="math inline">\(S_n\)</span> is, in fact, a biased estimator of <span class="math inline">\(\sigma\)</span> (see Exercise 5.11), but the bias disappears asymptotically.</p>
</div>
<div id="almost-sure-convergence" class="section level3">
<h3>5.5.2 Almost Sure Convergence</h3>
<p><strong>Definition 5.5.6</strong> A sequence of random variables, <span class="math inline">\(X_1,X_2,..\)</span>, converges almost surely to a random variable <span class="math inline">\(X\)</span> if, for every <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math display">\[P(\lim_{n\to\infty}|X_n-X|&lt;\epsilon)=1\]</span></p>
<p><strong>Example 5.5.7 (Almost sure convergence)</strong></p>
<p><strong>Example 5.5.8 (Convergence, not almost surely)</strong></p>
<p><strong>Theorem 5.5.9 (Strong Law of Large Numbers)</strong> Let <span class="math inline">\(X_1,X_2,..\)</span>, be iid random variables with <span class="math inline">\(EX_i=\mu\)</span> and <span class="math inline">\(Var Xi =\sigma^2&lt;\infty\)</span>, and define <span class="math inline">\(\bar X_n=\frac1n\sum_{i=1}^nX_i\)</span>. Then, for every <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math display">\[P(\lim_{n\to\infty}|\bar X_n-\mu|&lt;\epsilon)=1\]</span> that is, <span class="math inline">\(\bar X_n\)</span> converges almost surely to <span class="math inline">\(\mu\)</span></p>
</div>
<div id="convergence-in-distribution" class="section level3">
<h3>5.5.3 Convergence in Distribution</h3>
<p><strong>Definition 5.5.10</strong> A sequence of random variables, <span class="math inline">\(X_1,X_2,..\)</span>, converges in distribution to a random variable <span class="math inline">\(X\)</span> if <span class="math display">\[\lim_{n\to\infty}F_{X_n}(x)=F_X(x)\]</span> at all points <span class="math inline">\(x\)</span> where <span class="math inline">\(F_X(x)\)</span> is continuous.</p>
<p><strong>Example 5.5.11 (Maximum of uniforms)</strong></p>
<p><strong>Theorem 5.5.12</strong> If the sequence of random variables, <span class="math inline">\(X_1,X_2,..\)</span> converges in probability to a random variable <span class="math inline">\(X\)</span> the sequence also converges in distribution to <span class="math inline">\(X\)</span>.</p>
<p><strong>Theorem 5.5.13</strong> The sequence of random variables, <span class="math inline">\(X_1,X_2,..\)</span>, converges in probability to a constant <span class="math inline">\(\mu\)</span> if and only if the sequence also converges in distribution to <span class="math inline">\(\mu\)</span>. That is, the statement</p>
<p><span class="math display">\[\forall\varepsilon&gt;0, P(|X_n-\mu|&gt;\varepsilon)\to0\iff P(X_n\le x)\to\begin{cases}0&amp;x&lt;\mu\\1&amp;x\ge\mu\end{cases}\]</span></p>
<p><strong>Theorem 5.5.14 (Central Limit Theorem)</strong> Let <span class="math inline">\(X_1,X_2,..\)</span> be a sequence of iid random variables whose mgfs exist in a neighborhood of 0 (that is, <span class="math inline">\(M_{X_i}(t)\)</span> exists for <span class="math inline">\(|t|&lt;h\)</span>, for some positive h). Let <span class="math inline">\(EX_i =\mu\)</span> and <span class="math inline">\(VarX_i=\sigma^2&gt;0\)</span>. (Both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are finite since the mgf exists.) Define <span class="math inline">\(\bar X_n=(\frac1n)\sum_{i=1}^nX_i\)</span>. Let <span class="math inline">\(G_n(x)\)</span> denote the cdf of <span class="math inline">\(\frac{\sqrt n(\bar X_n-\mu)}{\sigma}\)</span>. Then, for any x, <span class="math inline">\(-\infty&lt; x &lt;\infty\)</span>,</p>
<p><span class="math display">\[\lim_{n\to\infty}G_n(x)=\int_{-\infty}^x\frac1{\sqrt{2\pi}}e^{-\frac{y^2}2}dy\]</span></p>
<p>that is, <span class="math inline">\(\frac{\sqrt n(\bar X_n-\mu)}{\sigma}\)</span> has a limiting standard normal distribution.</p>
<p><span class="math display">\[\underrightarrow{p}\quad \lim_{\substack{x\to\infty\\y\to\infty}}()\]</span></p>
<p><strong>Theorem 5.5.15 (Stronger (orm of the Central Limit Theorem)</strong></p>
<p><strong>Example 5.5.16 (Normal approximation to the negative binomial)</strong></p>
<p><strong>Theorem 5.5.17 (Slutsky’s Theorem)</strong> If <span class="math inline">\(X_n\to X\)</span> in distribution and <span class="math inline">\(Y_n\to a\)</span>, a constant, in probability, then</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(Y_nX_n\to aX\)</span> in distribution</p></li>
<li><p><span class="math inline">\(X_n+Y_n\to X+a\)</span> in distribution</p></li>
</ol>
</div>
<div id="the-delta-method" class="section level3">
<h3>5.5.4 The Delta Method</h3>
<p><strong>Example 5.5.19 (Estimating the odds)</strong></p>
<p><strong>Definition 5.5.20</strong></p>
<p><strong>Theorem 5.5.21 (Taylor)</strong></p>
<p><strong>Example 5.5.22 (Continuation of Example 5.5.19)</strong></p>
<p><strong>Example 5.5.23 (Approximate mean and variance)</strong></p>
<p><strong>Theorem 5 .5.24 (Delta Method)</strong></p>
<p><strong>Example 5.5.25 (Continuation of Example 5.5.23)</strong></p>
<p><strong>Theorem 5.5.26 (Second-order Delta Method)</strong></p>
<p><strong>Example 5.5.27 (Moments of a ratio estimator)</strong></p>
<p><strong>Theorem 5.5.28 (Multivariate Delta Method)</strong></p>
</div>
</div>
<div id="generating-a-random-sample" class="section level2">
<h2>5.6 Generating a Random Sample</h2>
<div id="direct-methods" class="section level3">
<h3>5.6.1 Direct Methods</h3>
<p><strong>Example 5.6.1 (Exponential lifetime)</strong></p>
<p><strong>Example 5.6.2 (Continuation of Example 5.6.1)</strong></p>
<p><strong>Example 5.6.3 (Probability Integral Transform)</strong></p>
<p>The relationship between the exponential and other distributions allows the quick generation of many random variables. For example, if <span class="math inline">\(U_j\)</span> are iid <span class="math inline">\(uniform(0,1)\)</span> random variables, then</p>
<p><span class="math display">\[Y_j=-\lambda\ln(u_j)\sim Expo(\lambda)\]</span> <span class="math display">\[Y=-2\sum_{j=1}^\nu\ln U_j\sim \chi^2_{2\nu}\]</span> <span class="math display">\[Y=-\beta\sum_{j=1}^\alpha\ln U_j\sim Gama(\alpha,\beta)\]</span> <span class="math display">\[Y=\frac{\sum_{j=1}^a\ln U_j}{\sum_{j=1}^{a+b}\ln U_j}\sim Beta(a,b)\]</span></p>
<p><strong>Example 5.6.5 (Binomial random variable generation)</strong></p>
<p><strong>Example 5.6.6 (Distribution of the Poisson variance)</strong></p>
</div>
<div id="indirect-methods" class="section level3">
<h3>5.6.2 Indirect Methods</h3>
<p><strong>Example 5.6.7 (Beta random variable generation-I)</strong> generates a <span class="math inline">\(Beta(a,b)\)</span> random variable.</p>
<p>If both a and b are integers, use the direct transformation method (5.6.5). Set <span class="math inline">\(U_j\)</span> are iid <span class="math inline">\(uni(0,1)\)</span> random variables, then <span class="math inline">\(Y=\frac{\sum_{j=1}^a\ln U_j}{\sum_{j=1}^{a+b}\ln U_j}\sim Beta(a,b)\)</span></p>
<p>If both a and b are not integers, To generate <span class="math inline">\(Y\sim beta(a, b)\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Generate <span class="math inline">\((U,V)\)</span> independent <span class="math inline">\(uni(0,1)\)</span>.</p></li>
<li><p>If <span class="math inline">\(U&lt;\frac1cf_Y(V)\)</span>, set <span class="math inline">\(Y=V\)</span>; otherwise, return to step (a).</p></li>
</ol>
<p>(See HW7)</p>
</div>
<div id="the-acceptreject-algorithm" class="section level3">
<h3>5.6.3 The Accept/Reject Algorithm</h3>
<p><strong>Example 5.6.8</strong> (See HW7)</p>
<p><strong>Example 5.6.9 (Beta random variable generation-II)</strong></p>
</div>
</div>
</div>
<div id="principles-of-data-reduction" class="section level1">
<h1>6. Principles of Data Reduction</h1>
<div id="the-sufficiency-principle" class="section level2">
<h2>6.2 The Sufficiency Principle</h2>
<div id="sufficient-statistics" class="section level3">
<h3>6.2.1 Sufficient Statistics</h3>
<p><strong>Definition 6.2.1</strong> A satistic <span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> if the conditional distribution of the sample <span class="math inline">\(\mathbf{X}\)</span> given the value of <span class="math inline">\(T(\mathbf{X})\)</span> does not depend on <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Theorem 6.2.2</strong> If <span class="math inline">\(p(\mathbf{x}|\theta)\)</span> is the joint pdf or pmf of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(q(t|\theta)\)</span> is the pdf or pmf of <span class="math inline">\(T(\mathbf{X})\)</span>, then <span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> if, for every <span class="math inline">\(\mathbf{x}\)</span> in the sample space, the ratio <span class="math inline">\(p(\mathbf{x}|\theta)/q(T(\mathbf{X})|\theta)\)</span> is constant as a function of <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Example 6.2.3 (Binomial sufficient statistic)</strong></p>
<p><strong>Example 6.2.4 (Normal sufficient statistic)</strong></p>
<p><strong>Example 6.2.5 (Sufficient order statistics)</strong></p>
<p>Example 6.2.5 (Sufficient order statistics) Let <span class="math inline">\(X_1,..,X_n\)</span> be iid from a pdf <span class="math inline">\(f\)</span>, where we are unable to specify any more information about the pdf (as is the case in nonparametric estimation). It then follows that the sample density is given by</p>
<p><span class="math display">\[f(\mathbf{x}|\theta)=\prod_{i=1}^nf(x_i)=\prod_{i=1}^nf(x_{(i)})\]</span></p>
<p>where <span class="math inline">\(x_{(1)}\le x_{(2)}\le...\le x_{(n)}\)</span> are the order statistics.</p>
<p><strong>Theorem 6.2.6 (Factorization Theorem)</strong> Let <span class="math inline">\(f(\mathbf{x}|\theta)\)</span> denote the joint pdf or pmf of a sample <span class="math inline">\(\mathbf{X}\)</span>. A statistic <span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> if and only if there exist functions <span class="math inline">\(g(x|\theta)\)</span> and <span class="math inline">\(h(\mathbf{x})\)</span> such that, for all sample points <span class="math inline">\(\mathbf{x}\)</span> and all parameter points <span class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[f(\mathbf{x}|\theta)= g(T(\mathbf{x})|\theta)h(\mathbf{x})\]</span> <strong>Example 6.2.8 (Uniform sufficient statistic)</strong></p>
<p><strong>Example 6.2.9 (Normal sufficient statistic, both parameters unknown)</strong></p>
</div>
<div id="minimal-sufficient-statistics" class="section level3">
<h3>6.2.2 Minimal Sufficient Statistics</h3>
<p><strong>Definition 6.2.11</strong> A sufficient statistic <span class="math inline">\(T(\mathbf{X})\)</span> is called a minimal sufficient statistic if, for any other sufficient statistic <span class="math inline">\(T&#39;(\mathbf{X})\)</span>, <span class="math inline">\(T(\mathbf{x})\)</span> is a function of <span class="math inline">\(T&#39;(\mathbf{X})\)</span>.</p>
<p><strong>Definition 6.2.13</strong> Let <span class="math inline">\(f(x|\theta)\)</span>, be the pmf or pdf of a sample <span class="math inline">\(\mathbf{X}\)</span>. Suppose there exists a function <span class="math inline">\(T&#39;(\mathbf{x})\)</span> such that, for every two sample points x and y, the ratio <span class="math inline">\(\frac{f(x|\theta)}{f(y|\theta)}\)</span> is constant as a function of <span class="math inline">\(\theta\)</span> if and only if <span class="math inline">\(T(\mathbf{x})=T(\mathbf{y})\)</span>, then <span class="math inline">\(T(\mathbf{x})\)</span> is a minimal sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="ancillary-statistics" class="section level3">
<h3>6.2.3 Ancillary Statistics</h3>
<p><strong>Definition 6.2.16</strong> A satistic <span class="math inline">\(S(\mathbf{X})\)</span> whose distribution does not depend on the parameter <span class="math inline">\(\theta\)</span> is called an ancillary statistic.</p>
</div>
<div id="sufficient-ancillary-and-complete-statistics" class="section level3">
<h3>6.2.4 Sufficient, Ancillary, and Complete Statistics</h3>
<p><strong>Definition 6.2.21</strong> Let <span class="math inline">\(f(x|\theta)\)</span> be a family of pdfs or pmfs for a statistic <span class="math inline">\(T(\vec X)\)</span>. The family of probability distributions is called complete if <span class="math inline">\(E_\theta[g(T)]=0, \forall\theta\)</span> implies <span class="math inline">\(P_\theta(g(T)=0)=1\)</span> for all <span class="math inline">\(\theta\)</span>. Equivalently, <span class="math inline">\(T(\vec X)\)</span> is called a <em>complete statistic</em>.</p>
<p><strong>Example 6.2.22 (Binomial complete sufficient statistic)</strong></p>
<p><strong>Example 6.2.23 (Uniform complete sufficient statistic)</strong></p>
<p><strong>Theorem 6.2.24 (Basu’s Theorem)</strong> If <span class="math inline">\(T(\mathbf{X})\)</span> is a complete and minimal sufficient statistic, then <span class="math inline">\(T(\mathbf{X})\)</span> is independent of every ancillary statistic.</p>
<p><strong>Theorem 6.2.25 (Complete statistics in the exponential family)</strong></p>
<p><strong>Example 6.2.26 (Using Basu’s Theorem-I)</strong></p>
<p><strong>Example 6.2.27 (Using Basu’s Theorem-II)</strong></p>
<p><strong>Theorem 6.2.28</strong> If a minimal sufficient statistic exists, then any complete statistic is also a minimal sufficient statistic.</p>
</div>
</div>
<div id="the-likelihood-principle" class="section level2">
<h2>6.3 The Likelihood Principle</h2>
<div id="the-likelihood-function" class="section level3">
<h3>6.3.1 The Likelihood Function</h3>
<p><strong>Definition 6.3.1</strong> Let <span class="math inline">\(f(\vec x|\theta)\)</span> denote the joint pdf or pmf of the sample <span class="math inline">\(X=(X_1,..,X_n)\)</span>. Then, given that <span class="math inline">\(\vec X=\vec x\)</span> is observed, the function of <span class="math inline">\(\theta\)</span> defined by <span class="math inline">\(L(\theta|\vec x)=f(\vec x|\theta)\)</span> is called the likelihood function.</p>
<p><strong>Example 6.3.2 (Negative binomial likelihood)</strong></p>
<blockquote>
<p>LIKELIHOOD PRINCIPLE: If x and y are two sample points such that <span class="math inline">\(L(\theta|\vec x)\)</span> is proportional to <span class="math inline">\(L(\theta|\vec y)\)</span>, that is, there exists a constant <span class="math inline">\(C(\vec x,\vec y)\)</span> such that <span class="math inline">\(L(\theta|\vec x)=C(\vec x,\vec y)L(\theta|\vec y) \forall\theta\)</span>, then the conclusions drawn from x and y should be identical.</p>
</blockquote>
<p><strong>Example 6.3.3 (Normal fiducial distribution)</strong></p>
</div>
<div id="the-formal-likelihood-principle" class="section level3">
<h3>6.3.2 The Formal Likelihood Principle</h3>
<p><strong>Example 6.3.4 (Evidence function)</strong></p>
<blockquote>
<p>CONDITIONALITY PRINCIPLE</p>
</blockquote>
<p><strong>Example 6.3.4 (Binomial/negative binomial experiment)</strong></p>
<p><strong>Theorem 6.3.6 (Birnbaum’s Theorem)</strong> The Formal Likelihood Principle follows from the Formal Sufficiency Principle and the Conditionality Principle. The converse is also true.</p>
<p><strong>Example 6.3.7 ( Continuation of Example 6.3.5)</strong></p>
</div>
</div>
<div id="the-equivariance-principle" class="section level2">
<h2>6.4 The Equivariance Principle</h2>
<blockquote>
<p>Equivariance Principle: if <span class="math inline">\(Y=g(\mathbf{X}\)</span> is a change of measurement scale such that the model for <span class="math inline">\(\mathbf{Y}\)</span> has the same formal structure as the model for <span class="math inline">\(\mathbf{X}\)</span>, then an inference procedure should be both measurement equivariant and formally equivariant.</p>
</blockquote>
<p><strong>Example 6.4.1 (Binomial equivariance)</strong></p>
<p><strong>Definition 6.4.2</strong> A set of functions <span class="math inline">\(\{g(\vec x):g\in\mathcal{G}\}\)</span> from the sample space <span class="math inline">\(\mathcal{X}\)</span> onto <span class="math inline">\(\mathcal{X}\)</span> is called a group of transformations of <span class="math inline">\(\mathcal{X}\)</span> if</p>
<ol style="list-style-type: lower-roman">
<li><p>(Inverse) For every <span class="math inline">\(g\in\mathcal{G}\)</span> there is a <span class="math inline">\(g&#39;\in\mathcal{G}\)</span> such that <span class="math inline">\(g&#39;(g(\vec x))=\vec x\ \forall\vec x\in\mathcal{X}\)</span>.</p></li>
<li><p>(Composition) For every <span class="math inline">\(g\in\mathcal{G}\)</span> and <span class="math inline">\(g&#39;\in\mathcal{G}\)</span> there exists <span class="math inline">\(g&#39;&#39;\in\mathcal{G}\)</span> such that <span class="math inline">\(g&#39;(g(\vec x))=g&#39;&#39;(\vec x)\ \forall\vec x\in\mathcal{X}\)</span>.</p></li>
</ol>
<p>Sometimes the third requirement,</p>
<ol start="3" style="list-style-type: lower-roman">
<li>(Identity) The identity, <span class="math inline">\(e(\vec x)\)</span>, defined by <span class="math inline">\(e(\vec x)=\vec x\)</span> is an element of <span class="math inline">\(\mathcal{G}\)</span>, is stated as part of the definition of a group. But (iii) is a consequence of (i) and (ii) and need not be verified separately.</li>
</ol>
<p><strong>Example 6.4.3 ( Continuation of Example 6.4.1)</strong></p>
<p><strong>Definition 6.4.4</strong> Let <span class="math inline">\(\mathcal{F}=\{f(\vec x|\theta):\theta\in\Theta\}\)</span> be a set of pdfs or pmfs for <span class="math inline">\(\vec X\)</span>, and let <span class="math inline">\(\mathcal{G}\)</span> be a group of transformations of the sample space <span class="math inline">\(\mathcal{X}\)</span>. Then <span class="math inline">\(\mathcal{F}\)</span> is <em>invariant under the group</em> <span class="math inline">\(\mathcal{G}\)</span> if for every <span class="math inline">\(\theta\in\Theta\)</span> and <span class="math inline">\(g\in\mathcal{G}\)</span> there exists a unique <span class="math inline">\(\theta&#39;\in\Theta\)</span> such that <span class="math inline">\(\vec Y=g(\vec X)\)</span> has the distribution <span class="math inline">\(f(\vec y|\theta&#39;)\)</span> if <span class="math inline">\(\vec X\)</span> has the distribution <span class="math inline">\(f(\vec x|\theta)\)</span>.</p>
<p><strong>Example 6.4.5 (Conclusion of Example 6.4.1)</strong></p>
<p><strong>Example 6.4.6 (Normal location invariance)</strong> Let <span class="math inline">\(X_1,..,X_n\)</span> be iid <span class="math inline">\(n(\mu,\sigma^2)\)</span>, both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> unknown. Consider the group of transformations defined by <span class="math inline">\(\mathcal{G}=\{g_a(\vec x),-\infty&lt;a&lt;\infty\}\)</span>, where <span class="math inline">\(g_a(x_1,..,x_n)=(x_1+a,..,x_n+a)\)</span>.</p>
<p><span class="math inline">\({Y_1,..,Y_n}=g_a(X_1,..,X_n)=(X_1+a,..,X_n+a)\)</span> are iid <span class="math inline">\(n(\mu+a,\sigma^2)\)</span> random variables. Thus, the joint distribution of <span class="math inline">\(\vec Y=g_a(\vec X)\)</span> is in <span class="math inline">\(\mathcal{F}\)</span> and hence <span class="math inline">\(\mathcal{F}\)</span> is invariant under <span class="math inline">\(\mathcal{G}\)</span>.</p>
</div>
</div>
<div id="point-estimation" class="section level1">
<h1>7. Point Estimation</h1>
<div id="methods-of-finding-estimators" class="section level2">
<h2>7.2 Methods of Finding Estimators</h2>
<div id="method-of-moments" class="section level3">
<h3>7.2.1 Method of Moments</h3>
<p><span class="math inline">\(f(\mathbf{x}|\theta)=\)</span>. Thus, <span class="math inline">\(X\sim Beta(\theta,1)\)</span>, <span class="math inline">\(\mu&#39;_1=E[X]=\)</span></p>
<p>Set <span class="math inline">\(\bar X=\frac1n\sum_{i=1}^nX_i=\mu&#39;_1=\)</span>. Therefore, <span class="math inline">\(\hat\theta_{MOM}=\)</span></p>
<p><strong>Example 7.2.1 (Normal method of moments)</strong></p>
<p><strong>Example 7.2.2 (Binomial method of moments)</strong></p>
<p><strong>Example 7.2.3 (Satterthwaite approximation)</strong></p>
</div>
<div id="maximum-likelihood-estimators" class="section level3">
<h3>7.2.2 Maximum Likelihood Estimators</h3>
<p><span class="math display">\[L(\theta|\mathbf{x})=L(\theta_1,..,\theta_k|x_1,..,x_n)=\prod_{i=1}^n f(x_i|\theta_1,..,\theta_k)\]</span></p>
<p><strong>Definition 7.2.4</strong> For each sample point <span class="math inline">\(\vec x\)</span>, let <span class="math inline">\(\hat\theta(\vec x)\)</span> be a parameter value at which <span class="math inline">\(L(\theta|\mathbf{x})\)</span> attains its maximum as a function of <span class="math inline">\(\theta\)</span>, with <span class="math inline">\(\vec x\)</span> held fixed. A <em>maximum likelihood estimator</em> (MLE) of the parameter <span class="math inline">\(\theta\)</span> based on a sample <span class="math inline">\(\vec X\)</span> is <span class="math inline">\(\hat\theta(\vec X)\)</span>.</p>
<p>Let <span class="math inline">\(L(\theta|\mathbf{x})=f(\mathbf{x}|\theta)\)</span>. <span class="math inline">\(\forall\theta\)</span>, <span class="math inline">\(L(\theta|\mathbf{x})=\)</span>. Thus, <span class="math inline">\(\theta_{MLE}=\)</span>.</p>
<p>Derive the log likehood function. <span class="math inline">\(l(\theta|\mathbf{x})=\ln L(\theta|\mathbf{x})=\)</span></p>
<p>Set <span class="math inline">\(\frac{\partial}{\partial\theta_i}l(\theta|\mathbf{x})=0,\quad i=1,..,k\)</span></p>
<p>Set <span class="math inline">\(\frac{\partial}{\partial^2\theta_i}l(\theta|\mathbf{x})=0\)</span>, there is solution or not.</p>
<p>Thus, <span class="math inline">\(\hat\theta_{MLE}=\)</span></p>
<p><strong>Example 7.2.5 (Normal likelihood)</strong></p>
<p><strong>Example 7.2.6 ( Continuation of Example 7.2.5)</strong></p>
<p><strong>Example 7.2.7 (Bernoulli MLE)</strong></p>
<p><strong>Example 7.2.8 ( Restricted range MLE)</strong></p>
<p><strong>Example 7.2.9 ( Binomial MLE, unknown number of trials)</strong></p>
<p>A useful property of maximum likelihood estimators is what has come to be known as the <em>invariance property of maximum likelihood estimators</em> (not to be confused with the type of invariance discussed in Chapter 6).</p>
<p><strong>Theorem 7.2.10 (Invariance property of MLEs)</strong> If <span class="math inline">\(\hat\theta\)</span> is the MLE of <span class="math inline">\(\theta\)</span>, then for any function <span class="math inline">\(\tau(\theta)\)</span>, the MLE of <span class="math inline">\(\tau(\theta)\)</span> is <span class="math inline">\(\tau(\hat\theta)\)</span>.</p>
<p><strong>Example 7.2.11 (Normal MLEs, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> unknown)</strong></p>
<p><strong>Example 7.2.12 (Continuation of Example 1.2.11)</strong></p>
<p><strong>Example 7.2.13 (Continuation of Example 7.2.2)</strong></p>
</div>
<div id="bayes-estimators" class="section level3">
<h3>7.2.3 Bayes Estimators</h3>
<p>In the Bayesian approach <span class="math inline">\(\theta\)</span> is considered to be a quantity whose variation can be described by a probability distribution (called the <em>prior distribution</em>). This is a subjective distribution, based on the experimenter’s belief, and is formulated before the data are seen (hence the name prior distribution). A sample is then taken from a population indexed by <span class="math inline">\(\theta\)</span> and the prior distribution is updated with this sample information. The updated prior is called the <em>posterior distribution</em>. This updating is done with the use of Bayes’ Rule (seen in Chapter 1), hence the name Bayesian statistics.</p>
<p>If we denote the prior distribution by <span class="math inline">\(\pi(\theta)\)</span> and the sampling distribution by <span class="math inline">\(f(\vec x|\theta)\)</span>, then the posterior distribution, the conditional distribution of <span class="math inline">\(\theta\)</span> given the sample, <span class="math inline">\(\vec x\)</span>, is</p>
<p><span class="math display">\[\pi(\theta|\vec x)=\frac{f(\vec x|\theta)\pi(\theta)}{m(\vec x)},\quad f(\vec x|\theta)\pi(\theta)=f(\vec x,\theta),\quad m(\vec x)=\int f(\vec x|\theta)\pi(\theta)d\theta\]</span></p>
<p><strong>Example 7.2.14 (Binomial Bayes estimation)</strong></p>
<p><strong>Definition 7.2.15</strong> Let <span class="math inline">\(\mathcal{F}\)</span> denote the class of pdfs or pmfs <span class="math inline">\(f(\vec x|\theta)\)</span> (indexed by <span class="math inline">\(\theta\)</span>). A class <span class="math inline">\(\Pi\)</span> of prior distributions is a <em>conjugate family</em> for <span class="math inline">\(\mathcal{F}\)</span> if the posterior distribution is in the class <span class="math inline">\(\Pi\ \forall f\in\mathcal{F}\)</span>, all priors in <span class="math inline">\(\Pi\)</span>, and all <span class="math inline">\(x\in\mathcal{X}\)</span>.</p>
<p><strong>Example 7.2.16 (Normal Bayes estimators)</strong> Let <span class="math inline">\(X_1,..,X_n\)</span> be a random sample from a <span class="math inline">\(n(\theta,\sigma^2)\)</span> population, and suppose that the prior distribution on <span class="math inline">\(\theta\)</span> is <span class="math inline">\(n(\mu,\tau^2)\)</span>. Here we assume that <span class="math inline">\(\sigma^2, \mu, \tau^2\)</span> are all known. The posterior distribution of <span class="math inline">\(\theta\)</span> is also normal, with mean and variance given by</p>
<p><span class="math display">\[f(\theta|x)\sim N(\frac{\tau^2x+\sigma^2\mu}{\tau^2+\sigma^2},\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}),\quad E(\theta|x)=\frac{\tau^2}{\tau^2+\sigma^2}x+\frac{\sigma^2}{\sigma^2+\tau^2},\quad Var(\theta|x)=\frac{\sigma^2\tau^2}{\sigma^2+\tau^2}\]</span></p>
</div>
<div id="the-em-algorithm" class="section level3">
<h3>7.2.4 The EM Algorithm</h3>
<p>(Expectation-Maximization)</p>
<p><strong>Example 7.2.17 (Multiple Poisson rates)</strong></p>
<p><strong>Example 7.2.18 (Continuation of Example 7.2.17)</strong></p>
<p><strong>Example 7.2.19 ( Conclusion or Example 7.2.17)</strong></p>
<p><strong>Theorem 7.2.20 (Monotonic EM sequence)</strong> The sequence {<span class="math inline">\(\hat\theta_{(r)}\)</span>} defined by 7.2.20 satisfies</p>
<p><span class="math display">\[L\left(\hat\theta^{(r+1)}|y\right)\ge L\left(\hat\theta^{(r)}|y\right)\]</span></p>
</div>
</div>
<div id="methods-of-evaluating-estimators" class="section level2">
<h2>7.3 Methods of Evaluating Estimators</h2>
<div id="mean-squared-error" class="section level3">
<h3>7.3.1 Mean Squared Error</h3>
<p><strong>Definition 7.3.1</strong> The mean squared error (MSE) of an estimator <span class="math inline">\(W\)</span> of a parameter <span class="math inline">\(\theta\)</span> is the function of <span class="math inline">\(\theta\)</span> defined by <span class="math inline">\(E_{\theta}(W-\theta)^2\)</span>.</p>
<p><span class="math display">\[E_{\theta}(W-\theta)^2=Var_{\theta}W+(E_{\theta}W-\theta)^2=Var_{\theta}W+(Bias_{\theta}W)^2\]</span></p>
<p><strong>Definition 7.3.2</strong> The <em>bias</em> of a point estimator <span class="math inline">\(W\)</span> of a parameter <span class="math inline">\(\theta\)</span> is the difference between the expected value of <span class="math inline">\(W\)</span> and <span class="math inline">\(\theta\)</span>; that is, <span class="math inline">\(Bias_{\theta}W=E_{\theta}W-\theta\)</span>. An estimator whose bias is identically (in a) equal to 0 is called <em>unbiased</em> and satisfies <span class="math inline">\(E_{\theta}W=\theta\)</span> for all <span class="math inline">\(\theta\)</span>.</p>
<p>For an unbiased estimator we have <span class="math display">\[E_{\theta}(W-\theta)^2=Var_{\theta}W\]</span> and so, if an estimator is unbiased, its MSE is equal to its variance.</p>
<p><strong>Example 7.3.3 (Normal MSE)</strong> Let <span class="math inline">\(X_1,..,X_n\sim iid\ n(\mu,\sigma^2)\)</span>. The statistics <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(\sigma^2\)</span> are both unbiased estimators since</p>
<p><span class="math display">\[E\bar X=\mu,\quad ES^2=\sigma^2, \forall\mu \text{ and } \sigma^2\]</span></p>
<p><span class="math display">\[E(\bar X-\mu)^2=Var\bar X=\frac{\sigma^2}{n},\quad E(S^2-\sigma^2)^2=VarS^2=\frac{2\sigma^4}{n-1}\]</span></p>
<p><strong>Example 7.3.4 (Continuation of Example 7.3.3)</strong> An alternative estimator for <span class="math inline">\(\sigma^2\)</span> is the maximum likelihood estimator <span class="math inline">\(\hat\sigma^2=\frac1n\sum_{i=1}^n(X_i-\bar X)^2=\frac{n-1}nS^2\)</span>. It is straightforward to calculate</p>
<p><span class="math display">\[E\hat\sigma^2=E\left(\frac{n-1}nS^2\right)=\frac{n-1}n\sigma^2\]</span></p>
<p>so <span class="math inline">\(\sigma^2\)</span> is a biased estimator of <span class="math inline">\(\hat\sigma^2\)</span>. The variance of <span class="math inline">\(\hat\sigma^2\)</span> can also be calculated as</p>
<p><span class="math display">\[Var\hat\sigma^2=Var\left(\frac{n-1}nS^2\right)=\left(\frac{n-1}n\right)^2VarS^2=\frac{2(n-1)}{n^2}\sigma^4\]</span> and, hence, its MSE is given by</p>
<p><span class="math display">\[E(\hat\sigma^2-\sigma^2)^2=\frac{2(n-1)}{n^2}\sigma^4+\left(\frac{n-1}n\sigma^2-\sigma^2\right)^2=\frac{2n-1}{n^2}\sigma^4\]</span> We thus have <span class="math display">\[E(\hat\sigma^2-\sigma^2)^2=\frac{2n-1}{n^2}\sigma^4&lt;\frac{2\sigma^4}{n-1}=E(S^2-\sigma^2)^2\]</span></p>
<p>showing that <span class="math inline">\(\hat\sigma^2\)</span> has smaller MSE than <span class="math inline">\(S^2\)</span> Thus, by trading off variance for bias, the MSE is improved.</p>
<p><strong>Example 7.3.5 (MSE of binomial Bayes estimator)</strong></p>
<p><strong>Example 7.3.6 (MSE o f equivariant estimators)</strong> Let <span class="math inline">\(X_1,..,X_n\)</span> be iid <span class="math inline">\(f(x-\theta)\)</span>. For an estimator <span class="math inline">\(W(X_1,..,X_n)\)</span> to satisfy <span class="math inline">\(W(g_a(x))=g_a(W(x))\)</span>, we must have</p>
<p><span class="math display">\[(7.3.2)\quad W(x_1,..,x_n)+a=W(x_1+a,..,x_n+a)\]</span> which specifies the equivariant estimators with.respect to the group of transformations defined by <span class="math inline">\(\mathcal{G}=\{g_a(\vec x):-\infty&lt;a&lt;\infty\}\)</span>, where <span class="math inline">\(g_a(x_1,..,x_n)=(x_1+a,..,x_n+a)\)</span>. For these estimators we have</p>
<p><span class="math display">\[(7.3.3)\quad E_\theta(W(X_1,..,X_n)-\theta)^2=\idotsint^\infty_{-\infty}(W(u_1,..,u_n))^2\prod^n_{i=1}f(u_i)du_i\]</span> This last expression does not depend on <span class="math inline">\(\theta\)</span>; hence, the MSEs of these equivariant estimators are not functions of <span class="math inline">\(\theta\)</span>. The MSE can therefore be used to order the equivariant estimators, and an equivariant estimator with smallest MSE can be found. In fact, this estimator is the solution to the mathematical problem of finding the function W that minimizes (7.3.3) subject to (7.3.2).</p>
</div>
<div id="best-unbiased-estimators" class="section level3">
<h3>7.3.2 Best Unbiased Estimators</h3>
<p><strong>Definition 7.3.7</strong> An estimator <span class="math inline">\(W^*\)</span> is a best unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span> if it satisfies <span class="math inline">\(E_\theta{W^*}=\tau(\theta)\)</span> for all <span class="math inline">\(\theta\)</span> and, for any other estimator <span class="math inline">\(W\)</span> with <span class="math inline">\(E_\theta{W}=\tau(\theta)\)</span>, we have Vare <span class="math inline">\(Var_\theta{W^*}&lt;Var_\theta{W}\)</span> for all <span class="math inline">\(\theta\)</span>. <span class="math inline">\(W^*\)</span> is also called a <em>uniform minimum variance unbiased estimator</em> (UMVUE) of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p><strong>Example 7.3.8 (Poisson unbiased estimation)</strong></p>
<p><strong>Cramer-Rao Lower Bound</strong></p>
<p><strong>Theorem 7.3.9 ( Cramer-Rao Inequality)</strong> Let <span class="math inline">\(X_1,..,X_n\)</span> be a sample with pdf <span class="math inline">\(f(\vec x|\theta)\)</span>, and let <span class="math inline">\(W(\vec X)=W(X_1,..,X_n)\)</span> be any estimator satisfying</p>
<p><span class="math display">\[\frac{d}{d\theta}E_\theta W(\vec X)=\int_{\mathcal{X}}\frac{\partial}{\partial\theta}\left[W(\vec x)f(\vec x|\theta)\right]d\vec x,\quad Var_\theta W(\vec X)&lt;\infty\]</span></p>
<p><span class="math display">\[Var_\theta W(\vec X)\ge\frac{(\frac{d}{d\theta}E_\theta W(\vec X))^2}{E_\theta\left[(\frac{\partial}{\partial\theta}\ln f(\vec X|\theta))^2\right]}\]</span></p>
<p><strong>Corollary 7.3.10 (Cramer-Rao Inequality, iid case)</strong></p>
<p><span class="math display">\[Var_\theta W(\vec X)\ge\frac{(\frac{d}{d\theta}E_\theta W(\vec X))^2}{nE_\theta\left[(\frac{\partial}{\partial\theta}\ln f(X|\theta))^2\right]}\]</span></p>
<p>The quantity <span class="math inline">\(E_\theta[(\frac{\partial}{\partial\theta}\ln f(\vec X|\theta))^2]\)</span> is called the <em>information number</em>, or <em>Fisher information</em> of the sample.</p>
<p><strong>Lemma 7.3.11</strong> If <span class="math inline">\(f(x|\theta)\)</span> satisfies</p>
<p><span class="math display">\[\frac{d}{d\theta}E_\theta\left[\frac{\partial}{\partial\theta}\ln f(X|\theta)\right]=\int\frac{\partial}{\partial\theta}\left(\left[\frac{\partial}{\partial\theta}\ln f(x|\theta)\right]f(x|\theta)\right)dx\]</span> (true for an exponential family), then</p>
<p><span class="math display">\[E_\theta\left[(\frac{\partial}{\partial\theta}\ln f(X|\theta))^2\right]=-E_\theta\left[\frac{\partial^2}{\partial\theta^2}\ln f(X|\theta)\right]\]</span></p>
<p><strong>Example 7.3.12 ( Conclusion of Example 7.3.8)</strong></p>
<p><strong>Example 7.3.13 (Unbiased estimator for the scale uniform)</strong></p>
<p>A shortcoming of this approach to finding best unbiased estimators is that, even if the Cramer-Rao Theorem is applicable, there is no guarantee that the bound is sharp. That is to say, the value of the Cramer-Roo Lower Bound may be <em>strictly smaller</em> than the variance of <em>any</em> unbiased estimator. In fact, in the usually favorable case of being a one-parameter exponential family, the most that we can say is that <span class="math inline">\(f(x|\theta)\)</span> exists a parameter <span class="math inline">\(\tau(\theta)\)</span> with an unbiased estimator that achieves the Cramer-Roo Lower Bound. However, in other typical situations, for other parameters, the bound may not be attainable. These situations cause concern because, if we cannot find an estimator that attains the lower bound, we have to decide whether no estimator can attain it or whether we must look at more estimators.</p>
<p><strong>Example 7.3.14 (Normal variance bound)</strong></p>
<p><strong>Corollary 7.3.15 (Attainment)</strong></p>
<p><strong>Example 7.3.16 (Continuation of Example 7.3.14)</strong></p>
</div>
<div id="sufficiency-and-unbiasedness" class="section level3">
<h3>7.3.3 Sufficiency and Unbiasedness</h3>
<p><strong>Theorem 7.3. 17 (Rao-Blackwell)</strong> Let <span class="math inline">\(W\)</span> be any unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>, and let <span class="math inline">\(T\)</span> be a sufficient statistic for <span class="math inline">\(\theta\)</span>. Define <span class="math inline">\(\phi(T)=E[W|T]\)</span>. Then <span class="math inline">\(E_{\theta}\phi(T)=\tau(\theta)\)</span> and <span class="math inline">\(Var_{\theta}\phi(T)\le Var_{\theta}(W)\)</span> for all <span class="math inline">\(\theta\)</span>; that is, <span class="math inline">\(\phi(T)\)</span> is a uniformly better unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p><strong>Theorem 7.3.23 Lehmann-Scheffe</strong> Let <span class="math inline">\(T\)</span> be a complete sufficient statistic for a parameter <span class="math inline">\(\theta\)</span>, and let <span class="math inline">\(\phi(T)\)</span> be any estimator based only on <span class="math inline">\(T\)</span>. Then <span class="math inline">\(\phi(T)\)</span> is the unique best unbiased estimator of its expected value.</p>
</div>
<div id="loss-function-optimality" class="section level3">
<h3>7.3.4 Loss Function Optimality</h3>
<p>Our evaluations of point estimators have been based on their mean squared error performance. Mean squared error is a special case of a function called a <em>loss function</em>. The study of the performance, and the optimality, of estimators evaluated through loss functions is a branch of <em>decision theory</em>.</p>
<p>The loss function is a nonnegative function that generally increases as the distance between a and <span class="math inline">\(\theta\)</span> increases. If <span class="math inline">\(\theta\)</span> is real-valued, two commonly used loss functions are</p>
<p><span class="math display">\[\begin{array}{l} L(\theta,a)=|a-\theta|&amp;\text{absolute error loss}\\L(\theta,a)=(a-\theta)^2&amp;\text{squared error loss}\\
L(\theta,a)=e^{c(a-\theta)}-c(a-\theta)-1,&amp;\text{LINear-EXponential loss E7.65}\\
L(\theta,a_0)=\begin{cases}0&amp;\theta\in\Theta_0\\1&amp;\theta\in\Theta^c_0\end{cases},
L(\theta,a_1)=\begin{cases}1&amp;\theta\in\Theta_0\\0&amp;\theta\in\Theta^c_0\end{cases}&amp;\text{binary loss -8.3.5-}\\
L(\theta,C)=b\text{Length}(C)-I_C(\theta),&amp;\text{CI -9.3.4-} 
\end{array}\]</span></p>
<p>where <span class="math inline">\(c\)</span> is a positive constant. As the constant <span class="math inline">\(c\)</span> varies, the loss function varies from very asymmetric to almost symmetric.</p>
<p><strong>Example 7.3.25 (Binomial risk functions)</strong></p>
<p><strong>Example 7.3.26 (Risk of normal variance)</strong></p>
<p><strong>Example 7.3.27 (Variance estimation using Stein’s loss)</strong></p>
<p><strong>Example 7.3.28 (Two Bayes rules)</strong></p>
<p><strong>Example 7.3.29 (Normal Bayes estimates)</strong></p>
<p><strong>Example 7.3.30 (Binomial Bayes estimates)</strong></p>
</div>
</div>
</div>
<div id="hypothesis-testing" class="section level1">
<h1>8 Hypothesis Testing</h1>
<div id="introduction" class="section level2">
<h2>8.1 Introduction</h2>
<p><strong>Definition 8.1.1</strong> A hypothesis is a statement about a population parameter.</p>
<p><strong>Definition 8.1.2</strong> The two complementary hypotheses in a hypothesis testing problem are called the null hypothesis and the alternative hypothesis. They are denoted by Ho and HI , respectively.</p>
<p><strong>Definition 8.1.3</strong> A hypothesis testing procedure or hypothesis test is a rule that specifies:</p>
<ol style="list-style-type: lower-roman">
<li><p>For which sample values the decision is made to accept llo as true.</p></li>
<li><p>For which sample values Ho is rejected and HI is accepted as true.</p></li>
</ol>
<p>The subset of the sample space for which Ho will be rejected is called the rejection region or critical region. The complement of the rejection region is called the acceptance region.</p>
</div>
<div id="methods-of-finding-tests" class="section level2">
<h2>8.2 Methods of Finding Tests</h2>
<p>###8.2.1 Likelihood Ratio Tests</p>
<p><strong>Definition 8.2.1 The likelihood ratio test</strong> statistic for testing <span class="math inline">\(H_0:\theta\in\Theta_0\)</span> versus <span class="math inline">\(H_1:\theta\ne\in\Theta_0^c\)</span> is</p>
<p><span class="math display">\[\lambda(\vec x)=\frac{\sup\limits_{\Theta_0} L(\theta|\vec x)}{\sup\limits_{\Theta} L(\theta|\vec x)}\]</span></p>
<p>A <em>likelihood ratio test (LRT)</em> is any test that has a rejection region of the form <span class="math inline">\(\{\vec x:\lambda(\vec x)\le c\}\)</span>, where <span class="math inline">\(c\)</span> is any number satisfying <span class="math inline">\(0\le c\le 1\)</span>.</p>
<p><strong>Example 20120412</strong></p>
<p><strong>Corollary 8.3.13</strong> Consider the hypothesis problem posed in <strong>Theorem 8.3.12</strong>. Suppose <span class="math inline">\(T(\vec X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(g(t|\theta_i)\)</span> is the pdf or pmf of <span class="math inline">\(T\)</span> corresponding to <span class="math inline">\(\theta_i,i=0,1\)</span>. Then any test based on <span class="math inline">\(T\)</span> with rejection region <span class="math inline">\(S\)</span> (a subset of the sample space of T) is a UMP level a test if it satisfies <span class="math display">\[t\in S\ \text{ if }\ g(t|\theta_1)&gt;kg(t|\theta)\]</span> and <span class="math display">\[t\in S^c\ \text{ if }\ g(t|\theta_1)&lt;kg(t|\theta)\]</span> for some <span class="math inline">\(k\ge 0\)</span>, where <span class="math inline">\(\alpha=P_{\theta_0}(T\in S)\)</span></p>
<p><strong>Theorem 8.3.17 (Karlin-Rubin)</strong> Consider testing <span class="math inline">\(H_0:\theta\le\theta_0\)</span> versus <span class="math inline">\(H_1:\theta&gt;\theta_0\)</span>. Suppose that T is a sufficient statistic for <span class="math inline">\(\theta\)</span> and the family of pdfs or pmfs <span class="math inline">\(\{g(t|\theta):\theta\in\Theta\}\)</span> of <span class="math inline">\(T\)</span> has an MLR. Then for any <span class="math inline">\(t_0\)</span>, the test that rejects <span class="math inline">\(H_0\)</span> if and only if <span class="math inline">\(T&gt;t_0\)</span> is a UMP level <span class="math inline">\(\alpha\)</span> test, where <span class="math inline">\(\alpha= P_{\theta_0}(T&gt;t_0)\)</span>.</p>
</div>
</div>
<div id="asymptotic-evaluations" class="section level1">
<h1>10 Asymptotic Evaluations</h1>
<div id="point-estimation-1" class="section level2">
<h2>10.1 Point Estimation</h2>
<div id="consistency" class="section level3">
<h3>10.1.1 Consistency</h3>
<p><strong>Definition 10.1.1</strong> A sequence of estimators Wn = Wn(Xl , &quot; . , Xn) is a consistent sequence of estimators of the parameter () if, for every f &gt; 0 and every () E e,</p>
<p><strong>Example 10.1.2 (Consistency of <span class="math inline">\(\bar X\)</span>)</strong> Let Xl , X2, . . . be iid n(e, 1), and consider the sequence</p>
<p><strong>Theorem 10.1.3</strong> If Wn is a sequence of estimators of a parameter () satisfying</p>
<ol style="list-style-type: lower-roman">
<li>limn-&gt; 00 Varo Wn = 0,</li>
</ol>
<p>iL limn-+ooBiaso Wn = 0,</p>
<p>for every () E e, then W n is a consistent sequence of estimators of ().</p>
<p><strong>Example 10.1.4 (Continuation of Example 10.1.2)</strong></p>
<p><strong>Theorem 10.1.5</strong> Let Wn be a consistent sequence of estimators of a parameter ().Let al , a2, . . . and b1 , b2, . . . be sequences of constants satisfying i. limn-+ooan = 1 ,</p>
<p>iL liffin_ 00 bn = O .</p>
<p>Then the sequence Un an Wn + bn is a consistent sequence of estimators of () .</p>
<p><strong>Theorem 10. 1 .6 (Consistency of MLEs)</strong> Let Xb X2 , . . . , be iid f(xl()), and let L(()lx) = nl f(xi l()) be the likelihood function. Let 0 denote the MLE of O. Let r(O) e a continuous function of O . Under the regularity conditions in Miscellanea 1 0. 6.2 on f(xl()) and, hence, L(()lx), for every E &gt; 0 and every 0 E e, limn_ooHI(lr(O) - r(O) 1 ;} = O. T hat is, r(O) is a consistent estimator of r(()).</p>
</div>
<div id="efficiency" class="section level3">
<h3>10.1.2 Efficiency</h3>
<p>*<strong>Definition 10.1.7</strong> For an estimator Tn, if limn_oo kn Var Tn = r2 &lt; 00 , where <span class="math inline">\(\{k_n\}\)</span> is a sequence of constants, then r2 is called the limiting variance or limit of the variances.</p>
<p><strong>Example 10.1.8 (Limiting variances)</strong></p>
<p><strong>Definition 10.1.9</strong> For an estimator Tn, suppose that kn(Tn - T(O)) -+ n(0, a2) in distribution. The parameter 0’2 is called the asymptotic variance or variance of the limit distribution of Tn.</p>
<p><strong>Example 10.1.10 (Large-sample mixture variances)</strong></p>
<p><strong>Definition 10.1.11</strong> A sequence of estimators Wn is asymptotically efficient for a parameter T(O) if v’n[Wn - T(O)] -+ n[O, v(O)] in distribution and</p>
<p><strong>Theorem 10.1.12 (Asymptotic efficiency of MLEs)</strong> Let Xl&gt; X2, • • • , be iid f(xIB), let iJ denote the MLE of B, and let r(O) be a continuous junction of B. Under the regularity conditions in Miscellanea 1 0. 6. 2 on f(xIB) and, hence, L(Blx), v’n[r(iJ) - r(B)] -t n[O, v(B)], where v(B) is the Cramer-Rao Lower Bound. That is, r(iJ) is a consistent and asymptotically efficient estimator of r( 0) .</p>
<p><strong>Definition: relative efficiency</strong> of <span class="math inline">\(\hat\theta_1\)</span>,<span class="math inline">\(\hat\theta_2\)</span> is <span class="math inline">\(\frac{V[\hat\theta_2]}{V[\hat\theta_1]}\)</span></p>
<p><strong>Definition: asymptoticly relative efficiency</strong> of <span class="math inline">\(\hat\theta_1\)</span>,<span class="math inline">\(\hat\theta_2\)</span> is <span class="math inline">\(\lim\limits_{n\to\infty}\frac{V[\hat\theta_2]}{V[\hat\theta_1]}\)</span>, if <span class="math inline">\(\hat\theta_1\)</span> and <span class="math inline">\(\hat\theta_2\)</span> are asymtoticlly unbiased estimators of <span class="math inline">\(\theta\)</span></p>
<p><strong>Definition: efficiency of <span class="math inline">\(\hat\theta\)</span></strong> is <span class="math inline">\(\frac{\text{CRLB}}{V[\hat\theta_1]}\)</span></p>
<p><strong>Definition: asymptotic efficiency of <span class="math inline">\(\hat\theta\)</span></strong> is <span class="math inline">\(\lim\limits_{n\to\infty}\frac{\text{CRLB}}{V[\hat\theta_1]}\)</span></p>
<p><strong>Definition: <span class="math inline">\(\hat\theta\)</span> is efficient </strong> if the efficiency=1</p>
</div>
<div id="calculations-and-comparisons" class="section level3">
<h3>10.1.3 Calculations and Comparisons</h3>
<p><strong>Example 20120405p4-8</strong> <span class="math inline">\(X_1,,X_n\sim Expo(\lambda)\)</span> Compare the MLE of <span class="math inline">\(\lambda\)</span> and an unbiased version of it.</p>
<p>Step 1 <span class="math inline">\(\hat\lambda_{MLE}\)</span></p>
<p><span class="math inline">\(L(\lambda)=\prod\lambda e^{-\lambda x_i}=\lambda^ne^{-\lambda\sum x_i}\)</span></p>
<p><span class="math inline">\(l(\lambda)=n\ln\lambda-\lambda\sum x_i\)</span></p>
<p><span class="math inline">\(l&#39;(\lambda)=\frac{n}\lambda-\sum x_i\overset{\text{set}}{=}0\)</span></p>
<p><span class="math inline">\(\hat\lambda_{MLE}=\frac{n}{\sum x_i}=\frac1{\bar x}\)</span></p>
<p>Step 2 <span class="math inline">\(E[\hat\lambda_{MLE}]\)</span>, <span class="math inline">\(V[\hat\lambda_{MLE}]\)</span></p>
<p><span class="math inline">\(E[\hat\lambda_{MLE}]=\frac{n\lambda}{n-1}\)</span></p>
<p><span class="math inline">\(V[\hat\lambda_{MLE}]=\frac{n^2\lambda^2}{(n-1)^2(n-2)}\)</span></p>
<p>Step 3 <span class="math inline">\(\hat\lambda_{u}\)</span>, <span class="math inline">\(V[\hat\lambda_{u}]\)</span></p>
<p><span class="math inline">\(\hat\lambda_{u}=\frac{n-1}{\sum x_i}\)</span></p>
<p><span class="math inline">\(V[\hat\lambda_{u}]=V[\frac{n-1}n\hat\lambda_{MLE}]=\frac{\lambda^2}{n-2}\)</span></p>
<p>Step 4 Find CRLB for unbiased</p>
<p><span class="math inline">\(\frac{1}{nI(\lambda)}=\frac{1}{n\frac1{\lambda^2}}=\frac{\lambda^2}{n}\)</span></p>
<p>‘Step 5’</p>
<p>The efficiency of <span class="math inline">\(\hat\lambda_{u}\)</span> is <span class="math inline">\(\frac{\lambda^2/n}{\lambda^2/(n-2)}=\frac{n-2}n\)</span></p>
<p>The asymptotic efficiency of <span class="math inline">\(\hat\lambda_{u}=1\)</span></p>
<p>The asymptotic efficiency of <span class="math inline">\(\hat\lambda_{MLE}=1\)</span> is <span class="math inline">\(\lim\limits_{n\to\infty}\frac{\text{CRLB}}{V[\hat\lambda_{MLE}]}\)</span></p>
<p><span class="math inline">\(\hat\lambda_{u}\)</span> is not efficient is asymptotic efficient. <span class="math inline">\(\hat\lambda_{MLE}\)</span> is also asymptotic efficieint.</p>
<p>Extra: MSE comparison</p>
<p><strong>Example 20120405p11</strong> <span class="math inline">\(X_1,X_2\sim\)</span> iid <span class="math inline">\(\frac1\theta e^{-x/\theta}, x&gt;0\)</span>. Let <span class="math inline">\(w=x_1, E[W]=\theta, V[W]=\theta^2\)</span></p>
<p>Step 1: Find a sufficient stat T</p>
<p><span class="math inline">\(L(\theta)=\frac1{\theta^2} e^{-(x_1+x_2)/\theta}\cdot1\)</span> So <span class="math inline">\(T=X_1+X_2\)</span> is sufficient.</p>
<p>Step 2: Find <span class="math inline">\(E[W|T]\)</span></p>
<p>Let <span class="math inline">\(w=X_1, T=X_1+X_2; X_1=w, X_2=T-W\)</span></p>
<p>J=1</p>
<p><span class="math inline">\(g(w,t)=\frac1{\theta^2}e^{-(x_1+x_2)/\theta}=\frac1{\theta^2}e^{-t/\theta}\)</span></p>
<p><span class="math inline">\(t\sim Gamma(\alpha=2,\beta=\theta)\)</span></p>
<p><span class="math inline">\(g(w|t)=\frac{\frac1{\theta^2}e^{-t/\theta}}{h(t)}=\frac{\frac1{\theta^2}e^{-t/\theta}}{\frac1{\theta^2}te^{-t/\theta}}=\frac1t, 0&lt;w&lt;t\)</span></p>
<p>So <span class="math inline">\(W|T\sim Unif(0,T)\)</span></p>
<p><span class="math inline">\(E[W|T]=\frac{T}2=\frac{X_1+X_2}2=\bar X\)</span></p>
<p>Note <span class="math inline">\(E[\bar X]=\theta, V[\bar X]=\theta^2/2\)</span></p>
<p>End: Find a function of T that is unbiased.</p>
</div>
</div>
<div id="bootstrap-standard-errors" class="section level2">
<h2>10.1.4 Bootstrap Standard Errors</h2>
</div>
<div id="robustness" class="section level2">
<h2>10.2 Robustness</h2>
<div id="the-mean-and-the-median" class="section level3">
<h3>10.2.1 The Mean and the Median</h3>
</div>
<div id="m-estimators" class="section level3">
<h3>10.2.2 M-Estimators</h3>
</div>
</div>
<div id="hypothesis-testing-1" class="section level2">
<h2>10.3 Hypothesis Testing</h2>
<div id="asymptotic-distribution-of-lrts" class="section level3">
<h3>10.3.1 Asymptotic Distribution of LRTs</h3>
</div>
<div id="other-large-sample-tests" class="section level3">
<h3>10.3.2 Other Large-Sample Tests</h3>
<p><strong>Example 10.3.5 (Large-sample binomial tests)</strong></p>
<p>Equation (10.3.4) is a special case of another useful large-sample test, the <strong>score test</strong>. The <em>score statistic</em> is defined to be</p>
<p><span class="math display">\[S(\theta)=\frac{\partial}{\partial\theta}\ln f(\vec X|\theta)=\frac{\partial}{\partial\theta}\ln L(\theta|\vec X)\]</span></p>
</div>
</div>
<div id="interval-estimation" class="section level2">
<h2>10.4 Interval Estimation</h2>
<div id="approximate-maximum-likelihood-intervals" class="section level3">
<h3>10.4.1 Approximate Maximum Likelihood Intervals</h3>
</div>
<div id="other-large-sample-intervals" class="section level3">
<h3>10.4.2 Other Large-Sample Intervals</h3>
</div>
</div>
</div>
</div>


</div>

<div id="postamble" data-toggle="wy-nav-shift" class="status">
<p class="date"><span class="glyphicon glyphicon-calendar"></span> Winter 2019</p>
</div>


<script>
$(document).ready(function () {
 	 	$('#content img')
 	  .addClass("image-thumb");
      $('#content img')
 	  .addClass("image-lb");
  $('#content').magnificPopup({
	      type:'image',
	      closeOnContentClick: false,
	      closeBtnInside: false,
	      delegate: 'img',
	      gallery: {enabled: false },
	      image: {
	        verticalFit: true,
          titleSrc: 'alt'
	      }
 	    });
 	});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
