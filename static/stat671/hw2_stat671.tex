
\documentclass[11pt]{article}
\setlength{\textwidth}{7in} \setlength{\textheight}{9.8in}
\setlength{\topmargin}{-1in} \setlength{\oddsidemargin}{-0.25in}
\setlength{\evensidemargin}{-0.25in}

\pagestyle{empty}
\usepackage{graphicx}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{algpseudocode}

\DeclareMathOperator{\trace}{trace}

\begin{document}
\thispagestyle{empty}
%\includegraphics{../../../psulogo_horiz_bw.eps}\hfill\includegraphics{../../../deptlogo}
\vspace{10pt}
\begin{center}
{\large\bf STAT 671}    \\*[5pt] {\Large Statistical Learning I}
\\*[12pt] {\large Fall 2019}
\\ {\large Homework 2}
\\ {\large Due October 28$^{th}$ at the beginning of class}
\end{center}
\vspace{1cm}
\noindent


\section{Kernels}

\begin{enumerate}
\item let $(x,y) \in \mathbb{R}^+ \times \mathbb{R}^+$, where $\mathbb{R}^+=\{x \in \mathbb{R};x \geq 0\}$, the ``french positive\rq\rq{} real numbers. 

\begin{enumerate}
\item  Verify that $\min(x,y) = \int_0^\infty \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt$
where  $\mathbb{I}_A =  \left\{
\begin{tabular}{ll}
1 & \mbox{ if A is true}\\
0 & \mbox{otherwise}
\end{tabular}\right.$

\item Use the previous question to show that $K(x,y)=\min(x,y)$ is a pd kernel over $\mathbb{R}^+$
\end{enumerate}

\vspace{5mm}
$K(x,y)=\min(x,y)=\int_0^\infty \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt=\min(y,x)=K(y,x)$ symmetric

$\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\min(x,y)=\int_0^\infty \sum_{i=1}^n\alpha_i\mathbb{I}_{t\leq x} \sum_{j=1}^n\alpha_j\mathbb{I}_{t\leq y} dt=\int_0^\infty (\sum_{i=1}^n\alpha_i\mathbb{I}_{t\leq x})^2dt\ge0$
\vspace{5mm}

\begin{enumerate}
\item Show that $\max(x,y)$ is not a pd kernel over  $\mathbb{R}^+$. 
\end{enumerate}

\vspace{5mm}
$\max(x,y)=\int_0^\infty \mathbb{I}_{t\geq x} \mathbb{I}_{t\geq y} dt=\max(y,x)=K(y,x)$ symmetric


\vspace{5mm}

\item Consider a probability space $(\Omega,\mathcal{A},P)$



\begin{enumerate}
\item Define for any two events $A$ and $B$, $K_1(A,B)=P(A \cap B)$
where $A \cap B$ is the intersection between the events A and B 
Verify that $K_1$ is positive definite. Hint: $P(A)=E[\mathbb{I}_A]$

\vspace{5mm}
$K_1(A,B)=P(A \cap B)=P(B \cap A)=K_1(B,A)$ symmetric

$P(A)=E[\mathbb{I}_A]$; $P(B)=E[\mathbb{I}_B]$; $P(A\cap B)=E[\mathbb{I}_A\mathbb{I}_B]$

$k_1(x,y)=\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jE[\mathbb{I}_A\mathbb{I}_A]=\|\sum_{i=1}^n\alpha_iE[\mathbb{I}_A]\|^2\ge0$
\vspace{5mm}

\item Define for any two events $A$ and $B$, 
$K_2(A,B)=P(A \cap B)-P(A)P(B)$
Verify that $K_2$ is positive definite. 
\end{enumerate}

\vspace{5mm}
$K_2(A,B)=P(A \cap B)-P(A)P(B)=E[\mathbb{I}_A\mathbb{I}_B]-E[\mathbb{I}_A]E[\mathbb{I}_B]=Cov[\mathbb{I}_A,\mathbb{I}_B]$

$K_2(x,y)=\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jCov[\mathbb{I}_A,\mathbb{I}_A]=\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jVar[\mathbb{I}_A]\ge 0$
\vspace{5mm}



\end{enumerate}


\section{Kernels and RKHS}
\begin{enumerate}
\item Define the RKHS  over $\mathbb{R}^d$ $K(x,y)=x^Ty+c$ where $c>0$. 

\begin{enumerate}
\item What is the RKHS associated with the kernel $K$? no proof is required. 

$$\mathcal{H} = \{f:\ \mathbb{R}^d\mapsto\mathbb{R};\ f_{w,w_0}(x)=w^Tx+w_0;\quad w\in\mathbb{R}^d,w_0\in\mathbb{R}\}$$





\item What is the inner product in this RKHS? no proof required.  

$$\langle f_{v,v_0},f_{w,w_0}\rangle_{\mathcal{H}}=v^Tw+\frac1cv_0w_0\Rightarrow\langle f_{v,v_0},f_{v,v_0}\rangle=\|f_{v,v_0}\|^2_{\mathcal{H}}=\|v\|^2+\frac{v_0^2}c$$


\item Verify the reproducing property

$\mathcal{H}$ contains all the functions $k(\cdot,x_i): t\mapsto k(t,x)=t^Tx+c=f_t(x)$

$$\langle f_{w,w_0},k(\cdot,x)\rangle=\langle f_{w,w_0},f_{x,c}\rangle=x^Tw+\frac1ccw_0=w^Tx+w_0=f_w(x)$$

$\therefore\langle f,k(\cdot,x)\rangle_{\mathcal{H}}=f(x)$ for each $f\in\mathcal{H}$, $x\in\mathcal{X}$

\end{enumerate}


\item Define the RKHS  over $\mathbb{R}^d$
$K(x,y)=(x^Ty)^2$
The RKHS associated with the kernel $K$ is $\{f_S;f_S(x)=x^T S x\}$ where $S$ is a symmetric $(d,d)$ matrix. The inner product is
$<f_{S_1},f_{S_2}>=<S_1,S_2>_F$



\begin{enumerate}
\item Verify the reproducing property. 

$$\mathcal{H} = \{f_S: f_S(x)=x^TS x;\}$$

$\mathcal{H}$ contains all the functions $k(\cdot,x_i): t\mapsto k(x,t)=\langle xx^T,tt^T\rangle$



$$ \langle f_{S_1},k(\cdot,x_i)\rangle_{\mathcal{H}}=\langle f_{S_1},f_{xx^T}\rangle_{\mathcal{H}}=\langle S_1,xx^T\rangle_{\mathcal{F}}=x^TS_1 x= f_{S_1}(x)$$


\item Why do we require that $S$ is symmetric?

$$\langle f_{S_1},f_{S_2}\rangle_{\mathcal{H}}=\langle S_1,S_2\rangle_{\mathcal{F}}=\sum\limits_{i,j=1}^n[S_1]_{ij}[S_2]_{ij}$$


$$[S_1]_{ij}[S_2]_{ij}=\trace[(x_i^Tx_j)(y_j^Ty_i)]=\trace[(y_ix_i^T)(x_jy_j^T)]=\langle x_iy_i^T,x_jy_j^T\rangle_{\mathcal{F}}=\langle z_i,z_j\rangle_{\mathbb{R}^{n^2}}$$


$\underset{(d,d)}{S}$ is a symmetric Matrix,
$y^Tx=x^Ty$

$$k(y,x)=(y^Tx)(y^Tx)=y^T\cdot xx^T\cdot y$$

\end{enumerate}

\item Define the RKHS  over $\mathbb{R}^d$ $K(x,y)=(x^Ty+c)^2$ where $c>0$. 

\begin{enumerate}
\item What is the RKHS associated with the kernel $K$? no proof is required. 

$$\{f_{S,s_0}; f_S(x)=x^T S x+s_0\}$$

where $S$ is a symmetric $(d,d)$ matrix

\item What is the inner product in this RKHS? no proof required.  


$$\langle f_{S_1},f_{S_2}\rangle_{\mathcal{H}}=\langle S_1,S_2\rangle_{\mathcal{F}}+\frac{s_0^2}c$$

\item Verify the reproducing property

$\mathcal{H}$ contains all the functions $k(\cdot,x_i): t\mapsto k(x,t)=\langle xx^T,tc\rangle$

$$ \langle f_{S_1},k(\cdot,x_i)\rangle_{\mathcal{H}}=\langle f_{S_1},f_{xx^T}\rangle_{\mathcal{H}}=\langle S_1,xx^T\rangle_{\mathcal{F}}=x^T S_1x+s_0= f_{S_1}(x)$$

\end{enumerate}

\end{enumerate}




\section{Fisher kernel} 
Let $\theta \in \mathbb{R}$ be a parameter and let $p_\theta$ be a probabilistic model (i.e a point mass function or a density) over a set $\mathcal{X}$ indexed by $\theta$. Let $\theta_0 \in \mathbb{R}$ be a specific value for $\theta$.

Let us define the Fisher score at $x \in \mathcal{X}$ as
$\phi(x,\theta_0) = \frac{\delta}{\delta \theta} \ln p_\theta(x) \mbox{ evaluated at } \theta=\theta_0$
assuming that this quantity exists. 

Define $I(\theta)$, the Fisher information associated with the parameter $\theta$, i.e., 
$I(\theta)=E[\phi^2(X,\theta)]$
where $E$ stands for expectation and $X$ is a random variable with distribution $p_\theta$. 

The Fisher kernel is then 
$k(x,x')=\frac{\phi(x,\theta_0)\phi(x',\theta_0)}{I(\theta_0)}$
where 
\begin{enumerate}
\item Verify that $k(.,.)$ is a positive definite kernel over $\mathcal{X}$

$k(x,x')=\frac{\phi(x,\theta_0)\phi(x',\theta_0)}{I(\theta_0)}=\frac{\phi(x',\theta_0)\phi(x,\theta_0)}{I(\theta_0)}=k(x',x)$ symmetric

\begin{align*}
  p_\theta(x)&=\theta^x(1-\theta)^{(1-x)} \\
  \ln p_\theta(x)&=x\ln\theta+(1-x)\ln(1-\theta)\\
  \phi(x,\theta_0)= \frac{d}{d \theta} \ln p_\theta(x)&=\frac{x}{\theta}+\frac{1-x}{1-\theta}=\frac{x-\theta}{\theta(1-\theta)}
\end{align*}

$k(x,x')=\frac{1}{I(\theta_0)}\sum_{i=1}^n\alpha_i\phi(x_i)\sum_{j=1}^n\alpha_j\phi(x_j)=\frac{1}{I(\theta_0)}\|\sum_{i=1}^n\alpha_i\phi(x_i)\|^2\ge0$



\item Consider the following model: $x \in \{0,1\}$, $X \sim Bernoulli(\theta)$, $0 < \theta < 1$, that is
$p_\theta(x)=\theta^x(1-\theta)^{(1-x)}$
We recall that in this case $E[X]=\theta$ and $Var[X]=E[(X-\theta)^2]=\theta(1-\theta)$
Compute $k(x,x')$

\begin{align*}
I(\theta)=E[\phi^2(X,\theta)]&=E[(\frac{x-\theta}{\theta(1-\theta))^2}]\\
=\frac{E[(x-\theta)^2]}{\theta^2(1-\theta)^2}&=\frac{V[X]}{\theta^2(1-\theta)^2}\\
=\frac{\theta(1-\theta)}{\theta^2(1-\theta)^2}&=\frac{1}{\theta(1-\theta)}
\end{align*}

$$k(x,x')=\frac{\phi(x,\theta_0)\phi(x',\theta_0)}{I(\theta_0)}=\frac{(x-\theta_0)(x'-\theta_0)}{\theta_0^2(1-\theta_0)^2}\theta_0(1-\theta_0)=\frac{(x-\theta_0)(x'-\theta_0)}{\theta_0(1-\theta_0)}$$

\item Assume now $x=(x_1,x_2)$ with $x_1 \in \{0,1\}$ and $x_2 \in \{0,1\}$. 
We consider the following model where $X=(X_1,X_2)$, $X_1$ and $X_2$ are independent with the same $Bernoulli(\theta)$ distribution. 
Compute $k(x,x')$. 

$$k(x,x')=\frac{(x-\theta_0)(x'-\theta_0)}{\theta_0(1-\theta_0)}=\frac{xx'-(x+x')\theta_0+\theta_0^2}{\theta_0(1-\theta_0)}=\frac{x_1x'_1+x_2x'_2-(x_1+x_2,x'_1+x'_2)^T\theta_0+\theta_0^2}{\theta_0(1-\theta_0)}$$



\end{enumerate}
 


 


\end{document}
