
\documentclass[11pt]{article}
\setlength{\textwidth}{7in} \setlength{\textheight}{9.8in}
\setlength{\topmargin}{-0.5in} \setlength{\oddsidemargin}{-0.25in}
\setlength{\evensidemargin}{-0.25in}


\usepackage{graphicx}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{algpseudocode}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{Shen Qu}
\lhead{Homework2}
\chead{STAT 671}
\rfoot{Page \thepage}

\DeclareMathOperator{\trace}{trace}

\begin{document}
%\includegraphics{../../../psulogo_horiz_bw.eps}\hfill\includegraphics{../../../deptlogo}



\section{Kernels}

\begin{enumerate}
\item let $(x,y) \in \mathbb{R}^+ \times \mathbb{R}^+$, where $\mathbb{R}^+=\{x \in \mathbb{R};x \geq 0\}$, the ``french positive\rq\rq{} real numbers. 

\begin{enumerate}
\item  Verify that $\min(x,y) = \int_0^\infty \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt$
where  $\mathbb{I}_A =  \left\{
\begin{tabular}{ll}
1 & \mbox{ if A is true}\\
0 & \mbox{otherwise}
\end{tabular}\right.$

When $x\le y$,
\begin{align*}
\int_0^\infty \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt&=\int_0^x \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt+\int_x^y \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt+\int_y^\infty \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt\\
&=\int_0^x 1\cdot 1 dt+\int_x^y 0\cdot 1 dt+\int_y^\infty 0\cdot 0 dt=x\\
\end{align*}

By the same way, when $y\le x$, $\int_0^\infty \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt=y$.

Therefore, $\min(x,y) = \int_0^\infty \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt$
\vspace{2mm}
\item Use the previous question to show that $K(x,y)=\min(x,y)$ is a pd kernel over $\mathbb{R}^+$


\vspace{2mm}
$K(x,y)=\min(x,y)=\int_0^\infty \mathbb{I}_{t\leq x} \mathbb{I}_{t\leq y} dt=\min(y,x)=K(y,x)$ symmetric

$\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\min(x,y)=\int_0^\infty \sum_{i=1}^n\alpha_i\mathbb{I}_{t\leq x} \sum_{j=1}^n\alpha_j\mathbb{I}_{t\leq y} dt=\int_0^\infty (\sum_{i=1}^n\alpha_i\mathbb{I}_{t\leq x})^2dt\ge0$
\vspace{3mm}


\item Show that $\max(x,y)$ is not a pd kernel over  $\mathbb{R}^+$. 
\end{enumerate}

\vspace{2mm}

When $x\le y$, $\int_0^\infty \mathbb{I}_{t\geq x} \mathbb{I}_{t\geq y} dt=\int_y^\infty 1\cdot 1 dt=\left.t\right|_y^\infty\neq\max(x,y)$

The Gram Matrix
$$M(x,y)=\begin{bmatrix}\max(x, x) & \max(x, y)\\\max(x,y) & \max(y,y)\end{bmatrix} =
       \begin{bmatrix}x &y \\y & y\end{bmatrix}=xy-y^2\le 0$$

When $y\le x$ is the same. $M(x,y)=xy-x^2\le 0$

Therefore, $\max(x,y)$ can not be a pd kernel over  $\mathbb{R}^+$


\item Consider a probability space $(\Omega,\mathcal{A},P)$



\begin{enumerate}
\item Define for any two events $A$ and $B$, $K_1(A,B)=P(A \cap B)$
where $A \cap B$ is the intersection between the events A and B 
Verify that $K_1$ is positive definite. Hint: $P(A)=E[\mathbb{I}_A]$

\vspace{2mm}
$K_1(A,B)=P(A \cap B)=P(B \cap A)=K_1(B,A)$ symmetric

$P(A)=E[\mathbb{I}_A]$; $P(B)=E[\mathbb{I}_B]$; $P(A\cap B)=E[\mathbb{I}_A\mathbb{I}_B]$

$\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jE[\mathbb{I}_{A_i}\mathbb{I}_{A_j}]=E[\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\mathbb{I}_{A_i}\mathbb{I}_{A_j}]=E[\|\sum_{i=1}^n\alpha_i\mathbb{I}_{A_i}\|^2]\ge0$
\vspace{3mm}

\item Define for any two events $A$ and $B$, 
$K_2(A,B)=P(A \cap B)-P(A)P(B)$
Verify that $K_2$ is positive definite. 
\end{enumerate}

\vspace{2mm}
$K_2(A,B)=P(A \cap B)-P(A)P(B)=E[\mathbb{I}_A\mathbb{I}_B]-E[\mathbb{I}_A]E[\mathbb{I}_B]=Cov[\mathbb{I}_A,\mathbb{I}_B]$

$\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jCov[\mathbb{I}_{A_i},\mathbb{I}_{A_j}]=Cov[\sum_{i=1}^n\alpha_i\mathbb{I}_{A_i},\sum_{j=1}^n\alpha_j\mathbb{I}_{A_j}]=Var[\sum_{i=1}^n\alpha_i\mathbb{I}_{A_i}]\ge 0$



\end{enumerate}


\section{Kernels and RKHS}
\begin{enumerate}
\item Define the RKHS  over $\mathbb{R}^d$ $K(x,y)=x^Ty+c$ where $c>0$. 

\begin{enumerate}
\item What is the RKHS associated with the kernel $K$? no proof is required. 

$$\mathcal{H} = \{f:\ \mathbb{R}^d\mapsto\mathbb{R};\ f_{w,w_0}(x)=w^Tx+w_0;\quad w\in\mathbb{R}^d,w_0\in\mathbb{R}\}$$





\item What is the inner product in this RKHS? no proof required.  

$$\langle f_{v,v_0},f_{w,w_0}\rangle_{\mathcal{H}}=v^Tw+\frac1cv_0w_0\Rightarrow\langle f_{v,v_0},f_{v,v_0}\rangle=\|f_{v,v_0}\|^2_{\mathcal{H}}=\|v\|^2+\frac{v_0^2}c$$


\item Verify the reproducing property

$\mathcal{H}$ contains all the functions $k(\cdot,x_i): t\mapsto k(t,x)=t^Tx+c=f_t(x)$

$$\langle f_{w,w_0},k(\cdot,x)\rangle=\langle f_{w,w_0},f_{x,c}\rangle=x^Tw+\frac1ccw_0=w^Tx+w_0=f_w(x)$$

$\therefore\langle f,k(\cdot,x)\rangle_{\mathcal{H}}=f(x)$ for each $f\in\mathcal{H}$, $x\in\mathcal{X}$

\end{enumerate}


\item Define the RKHS  over $\mathbb{R}^d$
$K(x,y)=(x^Ty)^2$
The RKHS associated with the kernel $K$ is $\{f_S;f_S(x)=x^T S x\}$ where $S$ is a symmetric $(d,d)$ matrix. The inner product is
$<f_{S_1},f_{S_2}>=<S_1,S_2>_F$



\begin{enumerate}
\item Verify the reproducing property. 

$$\mathcal{H} = \{f_S: f_S(x)=x^TS x;\}$$

$\mathcal{H}$ contains all the functions 

$k(\cdot,x_i): t\mapsto k(t,x)=(t^Tx)(t^Tx)=x^T\cdot (tt^T)\cdot x=f_t(x)$

$$ \langle f_{S_1},k(\cdot,x_i)\rangle_{\mathcal{H}}=\langle f_{S_1},f_{xx^T}\rangle_{\mathcal{H}}=\langle S_1,xx^T\rangle_{\mathcal{F}}=\trace[S_1xx^T]=\trace[x^TS_1x]=x^TS_1 x= f_{S_1}(x)$$

Frobenius Norm

$\therefore\langle f_{S_1},k(\cdot,x)\rangle_{\mathcal{H}}=f_{S_1}(x)$ for each $f\in\mathcal{H}$, $x\in\mathcal{X}$


\item Why do we require that $S$ is symmetric?

$\underset{(d,d)}{S}$ is a symmetric Matrix,

%$$[S_1]_{ij}[S_2]_{ij}=\trace[(x_i^Tx_j)(y_j^Ty_i)]=\trace[(y_ix_i^T)(x_jy_j^T)]=\langle x_iy_i^T,x_jy_j^T\rangle_{\mathcal{F}}=\langle z_i,z_j\rangle_{\mathbb{R}^{n^2}}$$

If not, we can not complete the step of $(t^Tx)(t^Tx)=x^T\cdot (tt^T)\cdot x$


\end{enumerate}

\item Define the RKHS  over $\mathbb{R}^d$ $K(x,y)=(x^Ty+c)^2$ where $c>0$. 

\begin{enumerate}
\item What is the RKHS associated with the kernel $K$? no proof is required. 

$$\{f_{S,s_0}; f_S(x)=x^T S x+2s^Tx+s_0^2\}$$

where $S$ is a symmetric $(d,d)$ matrix

\item What is the inner product in this RKHS? no proof required.  


$$\langle f_{S_1},f_{S_2}\rangle_{\mathcal{H}}=\langle S_1,S_2\rangle_{\mathcal{F}}+\frac{s_0^2}c$$

\item Verify the reproducing property

$\mathcal{H}$ contains all the functions 

$k(\cdot,x_i): t\mapsto k(t,x)=(t^Tx+c)(t^Tx+c)=x^T\cdot (tt^T)\cdot x+2ct^Tx+c^2=f_t(x)$


$$ \langle f_{S},k(\cdot,x_i)\rangle_{\mathcal{H}}=\langle f_{S},f_{xx^T}\rangle_{\mathcal{H}}=\langle S,xx^T\rangle_{\mathcal{F}}=x^T Sx+2s^Tx+s_0^2= f_{S}(x)$$

\end{enumerate}

\end{enumerate}




\section{Fisher kernel} 
Let $\theta \in \mathbb{R}$ be a parameter and let $p_\theta$ be a probabilistic model (i.e a point mass function or a density) over a set $\mathcal{X}$ indexed by $\theta$. Let $\theta_0 \in \mathbb{R}$ be a specific value for $\theta$.

Let us define the Fisher score at $x \in \mathcal{X}$ as
$\phi(x,\theta_0) = \frac{\delta}{\delta \theta} \ln p_\theta(x) \mbox{ evaluated at } \theta=\theta_0$
assuming that this quantity exists. 

Define $I(\theta)$, the Fisher information associated with the parameter $\theta$, i.e., 
$I(\theta)=E[\phi^2(X,\theta)]$
where $E$ stands for expectation and $X$ is a random variable with distribution $p_\theta$. 

The Fisher kernel is then 
$k(x,x')=\frac{\phi(x,\theta_0)\phi(x',\theta_0)}{I(\theta_0)}$
where 
\begin{enumerate}
\item Verify that $k(.,.)$ is a positive definite kernel over $\mathcal{X}$

$k(x,x')=\frac{\phi(x,\theta_0)\phi(x',\theta_0)}{I(\theta_0)}=\frac{\phi(x',\theta_0)\phi(x,\theta_0)}{I(\theta_0)}=k(x',x)$ symmetric


$k(x,x')=\frac{1}{I(\theta_0)}\sum_{i=1}^n\alpha_i\phi(x_i,\theta_0)\sum_{j=1}^n\alpha_j\phi(x_j,\theta_0)=\frac{1}{I(\theta_0)}\|\sum_{i=1}^n\alpha_i\phi(x_i,\theta_0)\|^2\ge0$


\item Consider the following model: $x \in \{0,1\}$, $X \sim Bernoulli(\theta)$, $0 < \theta < 1$, that is
$p_\theta(x)=\theta^x(1-\theta)^{(1-x)}$
We recall that in this case $E[X]=\theta$ and $Var[X]=E[(X-\theta)^2]=\theta(1-\theta)$
Compute $k(x,x')$

\begin{align*}
p_\theta(x)&=\theta^x(1-\theta)^{(1-x)} \\
  \ln p_\theta(x)&=x\ln\theta+(1-x)\ln(1-\theta)\\
  \frac{d}{d \theta} \ln p_\theta(x)&=\frac{x}{\theta}+\frac{1-x}{1-\theta}=\frac{x-\theta}{\theta(1-\theta)}
\end{align*}

$$I(\theta)=E[\phi^2(X,\theta)]=E[(\frac{X-\theta}{\theta(1-\theta)})^2]
=\frac{E[(X-\theta)^2]}{\theta^2(1-\theta)^2}=\frac{V[X]}{\theta^2(1-\theta)^2}
=\frac{\theta(1-\theta)}{\theta^2(1-\theta)^2}=\frac{1}{\theta(1-\theta)}$$


$$k(x,x')=\frac{\phi(x,\theta_0)\phi(x',\theta_0)}{I(\theta_0)}=\frac{(x-\theta_0)(x'-\theta_0)}{\theta_0^2(1-\theta_0)^2}\theta_0(1-\theta_0)=\frac{(x-\theta_0)(x'-\theta_0)}{\theta_0(1-\theta_0)}$$

\item Assume now $x=(x_1,x_2)$ with $x_1 \in \{0,1\}$ and $x_2 \in \{0,1\}$. 
We consider the following model where $X=(X_1,X_2)$, $X_1$ and $X_2$ are independent with the same $Bernoulli(\theta)$ distribution. 
Compute $k(x,x')$. 

\begin{align*}
  p_\theta(\vec x)&=p_\theta(x_1)p_\theta(x_2)=\theta^{x_1+x_2}(1-\theta)^{2-x_1-x_2} && why?\\
  \ln p_\theta(x)&=(x_1+x_2)\ln\theta+(2-x_1-x_2)\ln(1-\theta)\\
  \phi(\vec x,\theta)&= \frac{d}{d \theta} \ln p_\theta(x)=\frac{x_1+x_2}{\theta}+\frac{2-x_1-x_2}{1-\theta}=\frac{x_1+x_2-2\theta}{\theta(1-\theta)}
\end{align*}

\begin{align*}
I(\theta)&=E[\phi^2(\vec X,\theta)]=\frac{E[(X_1+X_2-2\theta)^2]}{\theta^2(1-\theta)^2}\\
&\underset{x_1\perp x_2}{=}\frac{E[(X_1-\theta)^2]+E[(X_2-\theta)^2]+2(E[X_1]-\theta)(E[X_2]-\theta)}{\theta^2(1-\theta)^2}\\
&=\frac{V[X_1]+V[X_2]-0}{\theta^2(1-\theta)^2}=\frac{2\theta(1-\theta)}{\theta^2(1-\theta)^2}=\frac{2}{\theta(1-\theta)}
\end{align*}

\begin{align*}
k(x,x')&=\frac{\phi(\vec x,\theta_0)\phi(\vec x',\theta_0)}{I(\theta_0)}=\frac{(x_1+x_2-2\theta_0)(x'_1+x'_2-2\theta_0)}{\theta_0^2(1-\theta_0)^2}\frac{\theta_0(1-\theta_0)}2\\
&=\frac{(x_1+x_2-2\theta_0)(x'_1+x'_2-2\theta_0)}{2\theta_0(1-\theta_0)}
\end{align*}

\end{enumerate}
 


 


\end{document}
