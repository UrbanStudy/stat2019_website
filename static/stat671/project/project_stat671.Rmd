---
title: 'Support Vector Machines'
author: "Di & Shen"
subtitle: 'STAT 671: Statistical Learning'
output:
  ioslides_presentation: default
  beamer_presentation:
    includes:
      in_header: "preamble.tex"
  slidy_presentation: default
---


# 

- Kernel Methods: 
  * Supervised Learning 
    + Kernel ridge regression 
    + Kernel logistic regression 
    + Large-margin classiﬁers 
    + Interlude: convex optimization and duality 
    + Support vector machines


## Introduction

Historically the ﬁrst “kernel method” for pattern recognition, still the most popular. 

Often state-of-the-art in performance. 

One particular choice of loss function (hinge loss). \textcolor{orange}{}

Leads to a sparse solution, i.e., not all points are involved in the decomposition (compression). 

Particular algorithm for fast optimization (decomposition by chunking methods).


## Deﬁnition 

The hinge loss is the function $\mathbb{R}\to\mathbb{R}_+$: 

$$\varphi_{hinge}(u) = \max(1−u,0) =\begin{cases}0 &\text{if  } u\ge 1,\\ 1−u& \text{otherwise}. \end{cases}$$

SVM is the corresponding large-margin classiﬁer, which solves:
$$\underset{f\in\mathcal{H}}{\min}\left\{\frac1n\sum_{i=1}^n\varphi_{hinge}(y_if(x_i)) + \lambda\|f\|^2_\mathcal{H}\right\}$$

## Problem reformulation 

By the representer theorem, the solution satisfies
$$\hat f (x) =\sum_{i=1}^n\hat\alpha_i K (x_i,x) $$,
where $\hat\alpha$ solves

$$\underset{\alpha\in\mathbb{R^n}}{\min}\left\{\frac1n\sum_{i=1}^n\varphi_{hinge}(y_i[K\alpha]_i) + \lambda\alpha^TK\alpha\right\}$$
This is a convex optimization problem But the objective function is not smooth (because of the hinge loss)

## Problem reformulation 

Let us introduce additional slack variables $\xi_1,...,\xi_n\in\mathbb{R}$. The problem is equivalent to:

$$\underset{\alpha\in\mathbb{R^n},\xi\in\mathbb{R^n}}{\min}\left\{\frac1n\sum_{i=1}^n\xi_i + \lambda\alpha^TK\alpha\right\}$$
subject to:
$$\xi_i\ge \varphi_{hinge}(y_i[K\alpha]_i)$$ . 
The objective function is now smooth, but not the constraints 
However it is easy to replace the non-smooth constraint by a cunjunction of two smooth constraints, because: 

$$u \ge\varphi_{hinge}(v)\iff \begin{cases}u\ge 1−v \\u\ge 0\end{cases}$$
## SVM (primal formulation)

In summary, the SVM solution is
$$\hat f (x) =\sum_{i=1}^n\hat\alpha_i K (x_i,x) $$,
where $\hat\alpha$ solves: 

$$\underset{\alpha\in\mathbb{R^n},\xi\in\mathbb{R^n}}{\min}\left\{\frac1n\sum_{i=1}^n\xi_i + \lambda\alpha^TK\alpha\right\}$$
subject to:
$$\begin{cases}(y_i[K\alpha]_i +\xi_i −1\ge 0,& \text{for} i = 1,...,n,\\\xi_i\ge 0,& \text{for} i = 1,...,n\end{cases}$$.

## Solving the SVM problem

This is a classical quadratic program (minimization of a convex quadratic function with linear constraints) for which any out-of-the-box optimization package can be used. 

The dimension of the problem and the number of constraints, however, are 2n where n is the number of points. General-purpose QP solvers will have diﬃculties when n exceeds a few thousands. 

Solving the dual of this problem (also a QP) will be more convenient and lead to faster algorithms (due to the sparsity of the ﬁnal solution).

## SVM (dual formulation)

$$\underset{\alpha\in\mathbb{R^n}}{\max}\left\{2\sum_{i=1}^n\alpha_i y_i -\sum_{i,j=1}^n\alpha_i\alpha_j K(x_i,x_j)\right\}=2\alpha^Ty-\alpha^TK\alpha$$
subject to:
$$0\le y_i\alpha_i\le\frac1{2\lambda n}, \text{for } i = 1,...,n$$

## Complimentary slackness conditions

## Analysis of KKT conditions 

## Geometric interpretation

## Support vectors

- Consequence of KKT conditions 

The training points with $\alpha_i\neq0$ are called support vectors. 

Only support vectors are important for the classiﬁcation of new points:

$$\forall x\in\mathcal{X},\ f (x) =\sum_{i=1}^n\alpha_i K (x_i,x)=\sum_{i\in SV}\alpha_i K (x_i,x) $$
where SV is the set of support vectors.

- Consequences 

The solution is sparse in $\alpha$, leading to fast algorithms for training (use of decomposition methods). 

The classiﬁcation of a new point only involves kernel evaluations with support vectors (fast).

## Remark: C-SVM

Often the SVM optimization problem is written in terms of a regularization parameter $C$ instead of $\lambda$ as follows:

$$\underset{f\in\mathcal{H}}{\arg\min}\left\{\frac12\|f\|^2_\mathcal{H}+C\sum_{i=1}^nL_{hinge}(f(x_i),y_i)\right\}$$

This is equivalent to our formulation with $C=\frac1{2\lambda n}$. 

The SVM optimization problem is then:

$$\underset{\alpha\in\mathbb{R^d}}{\max}\left\{2\sum_{i=1}^n\alpha_i y_i -\sum_{i,j=1}^n\alpha_i\alpha_j K(x_i,x_j)\right\}$$

subject to:
$$0\le y_i\alpha_i\le C, \text{for } i = 1,...,n$$
This formulation is often called C-SVM.

## Remark: 2-SVM

A variant of the SVM, sometimes called 2-SVM, is obtained by replacing the hinge loss by the square hinge loss:

$$\underset{f\in\mathcal{H}}{\min}\left\{\frac1n\sum_{i=1}^n\varphi_{hinge}(y_if(x_i))^2 + \lambda\|f\|^2_\mathcal{H}\right\}$$

The dual problem of the 2-SVM is:

$$\underset{\alpha\in\mathbb{R^d}}{\max}\left\{2\alpha^Ty-\alpha^T(K+n\lambda I)\alpha\right\}$$
$$0\le y_i\alpha_i,\ \text{for } i = 1,...,n$$

This is therefore equivalent to the previous SVM with the kernel $K + n\lambda I$ and $C=+\infty$




# Introduction

Some basic concepts of support vector machines.


## Example 1: A 1-dimension dataset

25 soft drink sugar content measurements. 
 
The distinct clusters for identifying candidate decision boundaries.

```{r, echo=F}
# Create the data frame.
sugar <- data.frame(
   sugar_sample = as.factor(c (1:25)),
   sugar_content=c(10.9,10.9,10.6,10.0,8.0,8.2,8.6,10.9,10.7,8.0,7.7,7.8,8.4,11.5,11.2,8.9,8.7,7.4,10.9,10.0,11.4,10.8,8.5,8.2,10.6)
)
library(ggplot2)
plot_1 <- ggplot(data =sugar, aes(x = sugar_content, y =0)) + 
    geom_point() + theme_light()+
    geom_text(aes(label = sugar_content), size = 2.5, vjust = 2, hjust = 0.5)
plot_1
```

## Find the maximal margin separator

Identified two distinct clusters (classes), the regular and the reduced sugar.

A dataset in which the classes do not overlap is called separable, the classes being separated by a decision boundary. 

The maximal margin separator is the decision boundary that is furthest from both classes. 

It is located at the mean of the relevant extreme points from each class. 

The relevant points are the highest valued point in the low sugar content class and the lowest valued point in the high sugar content class. 

find the maximal margin separator for the sugar content dataset.

```{r,eval=T}
#The maximal margin separator is at the midpoint of the two extreme points in each cluster.
mm_separator <- (8.9 + 10)/2
```

## Visualize the maximal margin separator

Add the maximal margin separator to the scatter plot.

```{r,echo=F}
#create data frame containing the maximum margin separator
separator <- data.frame(sep = mm_separator)

#add ggplot layer 
plot_sep <- plot_1 + geom_point(data = separator, aes(x = sep, y = 0), color = "blue", size = 4)

#display plot
plot_sep
```

## Example 2: A 2d dataset.
A 2 dimensional uniformly distributed dataset containing 600 datapoints.

```{r}
#set seed
set.seed(42)

#set number of data points. 
n <- 600

#Generate data frame with two uniformly distributed predictors.
df <- data.frame(x1 = runif(n,0,1), 
                 x2 = runif(n,0,1))
```

## Create a decision boundary

Add a class variable to that dataset by creating a variable y whose value is -1 or +1 depending on whether the point (x1, x2) lies below or above the straight line that passes through the origin and has slope 1.4.

```{r}
#classify data points depending on location
df$y <- factor(ifelse(df$x2 - 1.4*df$x1 < 0, -1, 1), 
    levels = c(-1, 1))
```

## Introduce a margin in the dataset

Create a margin in the dataset and then display the margin in a plot. The slope of the linear decision boundary is 1.4.


```{r,echo=F}
#set margin
delta <- 0.07

# retain only those points that lie outside the margin
df1 <- df[abs(1.4*df$x1 - df$x2) > delta, ]

#build plot
plot_margins <- ggplot(data = df1, aes(x = x1, y = x2, color = y)) + geom_point() + 
    scale_color_manual(values = c("red", "blue")) + 
    geom_abline(slope = 1.4, intercept = 0)+
    geom_abline(slope = 1.4, intercept = delta, linetype = "dashed") +
    geom_abline(slope = 1.4, intercept = -delta, linetype = "dashed")
 
#display plot 
plot_margins
```


# Support Vector Classifiers - Linear Kernels



# Polynomial Kernels


# Radial Basis Function Kernels


https://youtu.be/QRjllL-MP0U

Clair de lune v.s. Mondscheinsonate

https://terrytao.wordpress.com/2019/08/13/eigenvectors-from-eigenvalues/

```{r, eval=F}
install.packages("e1071")
library(e1071)
methods(svm)
getAnywhere(svm.default)
getAnywhere(svm.formula)
```

