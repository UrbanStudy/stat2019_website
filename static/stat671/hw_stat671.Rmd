---
title: ''
# fontfamily: mathpazo
fontsize: 10pt
geometry: margin=8mm
output:
  pdf_document:
    toc: no
    number_sections: yes
  html_document:
    toc: no
    df_print: paged
always_allow_html: true
pagenumbering: yes
classoption: portrait
header-includes:
- \usepackage{multicol}
- \usepackage{multirow}
- \usepackage{caption}
- \setlength\tabcolsep{0.1pt}
- \setlength\lineskip{0pt}
- \setlength\parskip{0pt}
---


\begin{flushright}STAT 671 HW1\\
Shen Qu
\end{flushright}

\section{ Classifier }
\subsection{proof}

$g(x)=<C_+-C_-,X-C>=<C_+,X>-<C_-,X>-<C_+,C>+<C_-,C>$;

$<C_+,X>=<\frac1{n_{+}}\sum\limits_{i\in I_+}^nx_i,x>$;

$<C_-,X>=<\frac1{n_{-}}\sum\limits_{i\in I_-}^nx_i,x>$;

$<C_+,C>=<C_+,\frac12C_+>+<C_+,\frac12C_->=\frac1{2n_{+}^2}\sum\limits_{(i,j)\in I_{+}}<x_i,x_j>+\frac12<C_+,C_->$

$<C_-,C>=<C_-,\frac12C_+>+<C_-,\frac12C_->=\frac12<C_+,C_->+\frac1{2n_{-}^2}\sum\limits_{(i,j)\in I_{-}}<x_i,x_j>$

$g(x)=<\frac1{n_{+}}\sum\limits_{i\in I_+}^nx_i,x>-<\frac1{n_{-}}\sum\limits_{i\in I_-}^nx_i,x>-\frac1{2n_{+}^2}\sum\limits_{(i,j)\in I_{+}}<x_i,x_j>-\frac12<C_+,C_->+\frac12<C_+,C_->+\frac1{2n_{-}^2}\sum\limits_{(i,j)\in I_{-}}<x_i,x_j>$

$$=\sum_{i=1}^n\alpha_i<x_i,x>+b$$

 where $\alpha_i=\begin{cases}\frac1{n_{+}}&y_i=+1\\-\frac1{n_{-}}&y_i=-1\end{cases}$; $b=\frac1{2n_{-}^2}\sum\limits_{(i,j)\in I_{-}}<x_i,x_j>-\frac1{2n_{+}^2}\sum\limits_{(i,j)\in I_{+}}<x_i,x_j>$

\subsection{iris data clasification}


- Import the iris data

\fontsize{8pt}{0pt}

```{r,echo=F}
rm(list=ls())
library(datasets)
data(iris)
```

```{r,echo=F,message=F,warning=F,out.width='50%',fig.align='center'}
library(ggplot2)
GGally::ggpairs(iris,mapping=aes(color =Species,shape=Species,alpha=0.3),
        columns=c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"))+theme_light()
```

```{r}
# Define the train and test sets
iris$class <- NA
iris_setosa <- iris[iris$Species=="setosa",]
iris_versicolor <- iris[iris$Species=="versicolor",]
iris_virginica <- iris[iris$Species=="virginica",]
iris_train_se<- iris_setosa[1:40,]
iris_train_ve<- iris_versicolor[1:40,]
iris_train_vi<- iris_virginica[1:40,]
iris_test_se<- iris_setosa[41:50,]
iris_test_ve<- iris_versicolor[41:50,]
iris_test_vi<- iris_virginica[41:50,]
iris_train_se.ve<- rbind(iris_train_se,iris_train_ve)
iris_train_ve.vi<- rbind(iris_train_ve,iris_train_vi)
iris_test_se.ve<- rbind(iris_test_se,iris_test_ve)
iris_test_ve.vi<- rbind(iris_test_ve,iris_test_vi)
```


```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
# Define a kernel function and classifier
k <- function(x,y){return(x %*% t(y))}
classifier <- function(test,train_po,train_ne){
  for(i in 1:nrow(train_po)){
    a_po = 1/nrow(train_po)*sum(k(as.matrix(test),train_po[i,]))
  }
  for(i in 1:nrow(train_ne)){
    a_ne = 1/nrow(train_ne)*sum(k(as.matrix(test),train_ne[i,]))
  }
  for(i in 1:(nrow(train_po)-1)){
    for (j in (i+1):nrow(train_po)){
      b_po = -0.5*(1/nrow(train_po)^2)*sum(k(as.matrix(train_po[i,]),train_po[j,]))
    }
  }
  for(i in 1:(nrow(train_ne)-1)){
    for (j in (i+1):nrow(train_ne)){
      b_ne = 0.5*(1/nrow(train_ne)^2)*sum(k(as.matrix(train_ne[i,]),train_ne[j,]))
    }
  }
  class = a_po+a_ne+b_po+b_ne
  return(class)
}
for (i in 1:20){
iris_test_se.ve[i,6] <- classifier(iris_test_se.ve[i,1:4],iris_train_se[,1:4],iris_train_ve[,1:4])
}
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
# define a 3D grid
kd <- with(iris_test_se.ve, MASS::kde2d(iris_train_se.ve$Petal.Length,iris_train_se.ve$Petal.Width, n = 20))
kd <- MASS::kde2d(iris_train_se.ve$Petal.Length,iris_train_se.ve$Petal.Width, n = 20)
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
# Plot the test and 3D grid
library(plotly)
plot_ly (type='surface',x=kd$x,y=kd$y,z =kd$z, opacity=0.95)%>%add_markers( x = ~iris_test_se.ve$Petal.Length, y = ~iris_test_se.ve$Petal.Width, z = ~iris_test_se.ve$class-2, color = ~iris_test_se.ve$Species,size =10, symbol=iris_test_se.ve$Species, symbols = c('circle-open','cross'))
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
for (i in 1:100){
iris[i,6] <- classifier(iris[i,1:4],iris_train_se.ve_setosa[,1:4],iris_train_se.ve_versicolor[,1:4])
}
plot_ly (type='surface',x=kd$x,y=kd$y,z =kd$z)%>%add_markers( x = ~iris$Petal.Length, y = ~iris$Petal.Width, z = ~iris$class-2, color = ~iris$Species,size =1 )
```


```{r,echo=T,message=F,warning=F,out.width='100%'}
# Define the kernel function and Computing the classifier
k = function(x,y)  return(sum(x*y))
# setosa v.s.versicolor
k.pp=outer(1:40,1:40,Vectorize(function(i,j) k(iris_train_se[i,1:4],iris_train_se[j,1:4])))
k.mm=outer(1:40,1:40,Vectorize(function(i,j) k(iris_train_ve[i,1:4],iris_train_ve[j,1:4])))
b=(sum(k.mm)/(40^2)-sum(k.pp)/(40^2))/2
alpha=ifelse(iris_train_se.ve$Species=="setosa",1/40,-1/40)
k.x=outer(1:80,1:20,Vectorize(function(i,j) k(iris_train_se.ve[i,1:4],iris_test_se.ve[j,1:4])))
iris_test_se.ve[,6]=(t(k.x)%*%alpha+b)
# virginica v.s.versicolor
k.pp=outer(1:40,1:40,Vectorize(function(i,j) k(iris_train_vi[i,1:4],iris_train_vi[j,1:4])))
k.mm=outer(1:40,1:40,Vectorize(function(i,j) k(iris_train_ve[i,1:4],iris_train_ve[j,1:4])))
b=(sum(k.mm)/(40^2)-sum(k.pp)/(40^2))/2
alpha=ifelse(iris_train_ve.vi$Species=="virginica",1/40,-1/40)
k.x=outer(1:80,1:20,Vectorize(function(i,j) k(iris_train_ve.vi[i,1:4],iris_test_ve.vi[j,1:4])))
iris_test_ve.vi[,6]=(t(k.x)%*%alpha+b)
# Evaluate the classifier
iris_test_se.ve$evaluate=ifelse(iris_test_se.ve$class>0,"setosa","versicolor")
iris_test_se.ve$evaluate=ifelse(iris_test_se.ve$Species==iris_test_se.ve$evaluate,"Rigt","Wrong")
error_rate1 <- length(which(iris_test_se.ve$evaluate=="Wrong"))/20
iris_test_ve.vi$evaluate=ifelse(iris_test_ve.vi$class>0,"virginica","versicolor")
iris_test_ve.vi$evaluate=ifelse(iris_test_ve.vi$Species==iris_test_ve.vi$evaluate,"Rigt","Wrong")
error_rate2 <- length(which(iris_test_ve.vi$evaluate=="Wrong"))/20
```

\normalsize

Error rate = 0% and 5% in two tests respectively.

```{r,echo=F, out.width='40%', fig.align='center',fig.show='hold'}
knitr::include_graphics(c('hw_stat671_files/se.ve.png','hw_stat671_files/ve.vi.png'))
```

The left figure of setosa v.s. versicolor shows that the two specises are separated by zero plane, while virginica v.s. versicolor shows that a few points are close to zero. The tables also show a misclassified point in virginica v.s. versicolor.

\begin{center}
    \captionof{table}{Confusion matrix}
    \label{tab:confusion-matrix}
\begin{tabular}{|c|c|c|c||c|c|c|}
\hline
\multirow{2}{*}{}& \multicolumn{6}{c|}{Actural Species} \\
\cline{2-7}
                             & test 1 & Setosa & Versicolor & test 2 & Virginica & Versicolor \\\hline

\multirow{2}{*}{Test Species}& Setosa & 10 & 0 & Virginica & 9 & 0\\
\cline{2-7}
                             & Versicolor & 0 & 10 & Versicolor & 1 & 10\\
\hline
\end{tabular}
  \end{center}

\fontsize{8pt}{0pt}

```{r,echo=F,message=F,warning=F}
library(pander)
library(dplyr)
pander(bind_cols(iris_test_se.ve[,5:7],iris_test_ve.vi[,5:7]),caption="setosa v.s.versicolor/  /virginica v.s.versicolor")
```

\normalsize

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
# Define a grid 
g.n=20
x.min=min(iris_test_se.ve[,3:4])
x.max=max(iris_test_se.ve[,3:4])
z.hat=matrix(NA,nrow=g.n,ncol=g.n)
g=seq(from=x.min,to=x.max,length.out=g.n)
for (i in (1:g.n)){
  for (j in (1:g.n)){
    u=c(0,0,g[i],g[j])
    k.x=outer(1:80,1,Vectorize(function(i,j) k(as.matrix(iris_train_se.ve[i,1:4]),u)))
    z.hat[i,j]=sum(k.x*alpha)+b
  }
}
```

```{r,eval=F,include=F,message=F,warning=F,out.width='50%'}
# Plot the test and grid
z.zero=matrix(0,nrow=20,ncol=20)
library(plotly)
plot_ly (iris_test_se.ve,x =~Petal.Length, y =~Petal.Width, z =~class,type = 'scatter3d', color=~Species,symbol=~Species, symbols = c('circle-open','cross'),size=3)
plot_ly (iris_test_ve.vi,x =~Petal.Length, y =~Petal.Width, z =~class,type = 'scatter3d', color=~Species,symbol=~Species, symbols = c('circle-open','cross'),size=3)
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
library(kableExtra)
kable(list(iris_test_se.ve[,c(5:7)],iris_test_ve.vi[,c(5:7)]),format="latex",booktabs=F)%>%
     add_header_above(c("setosa v.s.versicolor" = 3,"virginica v.s.versicolor" = 3))
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
library(kableExtra)
kable(iris_test_se.ve[,c(5:7)],format="latex",booktabs=F)%>%
     kable_styling(c("striped", "bordered"), full_width = F,font_size = 8)%>%
     add_header_above(c("setosa v.s.versicolor" = 4))
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
kable(iris_test_ve.vi[,c(5:7)],format="latex",booktabs=F)%>%
     kable_styling(c("striped", "bordered"), full_width = F,font_size = 8)%>%
     row_spec(13,background ="yellow")%>%
     add_header_above(c("virginica v.s.versicolor" = 4))
```

\normalsize

\section{ Perceptron }

\subsection{ pseduo code }

Init: 

-- define the classifier $f(x)=w^Tx$

-- define perceptron algorithm
      
-- set the sample size =n
      
-- set initial weight with $w \leftarrow y_1x_1$
      
-- set initial misclassified argument with TRUE


While misclassfied=TURE    

-- change argument with FALSE
  
-- For $i=2 \ldots n$ do

--- If $y_i w^Tx_i < 0$ then $w \leftarrow w + y_ix_i$

--- change argument with TRUE
     
--- EndIf

-- EndFor
      
EndWhile

```{r,echo=F,message=F,warning=F}
rm(list=ls())
```


\subsection{Using 3 Kernels}

\fontsize{8pt}{0pt}

```{r,echo=F,message=F,warning=F}
rm(list=ls())
```


```{r, echo=T,message=F,warning=F,collapse=T}
n <- 30; dim <- 2; threshold <-1           ## generate 2d data
x <- runif(n * dim)
x <- matrix(x, ncol = dim)
y <- ifelse(apply(x, 1, sum) < threshold, -1, 1)
data <- cbind(x, y, alpha = rep(1, n))

k<- function(x,w){return(sum(x*w))}        ## define the kernel function
k_perceptron <- function(data,dim) {       ## difine perceptron algorithm
n <- nrow(data)                            ## get the sample size
y <- data[ ,dim+1]                         ## select y
w <- c(data[1,1:dim]*data[1,dim+2],0)      ## initialize weight with y_1*x_1
misclassfied <- TRUE                       ## initialize the argument
  while (misclassfied) {                   ## start to loop
    misclassfied <- FALSE
    for (i in 2:n) {                       ## loop times = sample size-1
      if (y[i]*k(data[i,-dim-1],w) < 0) { ## if expectation does not match y
         w <- w + y[i] * data[i,-dim-1]    ## update the weights
         misclassfied <- TRUE              ## reset argument and check next one
  }}} 
  return(w)
}
k <- function(x,y) {return(sum(x*y))} ## Use k1
(w1 <- k_perceptron(data,2))
k <- function(x,y) {return(sum(x*y+1))} ## Use k2
(w2 <- k_perceptron(data,2))
k <- function(x,y) {return(sign(x%*%y))} ## Use k3
(w3 <- k_perceptron(data,2))
```

The output shows that the perceptrons with $k=x^Ty$ and $k=\text{sign}(x^Ty)$ can classify the data. The $k=x^Ty+1$ is biased.

```{r, echo=F,message=F,warning=F,out.width='45%',collapse=T,fig.align='center',fig.show='hold'}
plot(data[,1:2],xlab="x1\n Before perceptron",ylab="x2",pch=ifelse(data[,3]==1,1,3),col=ifelse(data[,3]==1,"blue","red"))
plot(data[,1:2],xlab="x1\n Use k1 perceptron",ylab="x2",pch=ifelse(data[,3]==1,1,3),col=ifelse(data[,3]==1,"blue","red"))
  abline(-w1[3]/w1[2], -w1[1]/ w1[2],col="orange")
plot(data[,1:2],xlab="x1\n Use k2 perceptron",ylab="x2",pch=ifelse(data[,3]==1,1,3),col=ifelse(data[,3]==1,"blue","red"))
  abline(-w2[3]/w2[2], -w2[1]/ w2[2],col="green")
plot(data[,1:2],xlab="x1\n Use k3 perceptron",ylab="x2",pch=ifelse(data[,3]==1,1,3),col=ifelse(data[,3]==1,"blue","red"))
  abline(-w3[3]/w3[2], -w3[1]/ w3[2],col="yellow")
#  legend("bottomleft",legend=c("w1","w2","w3"),col=c("orange","green","yellow"),lty=1,ncol=3)
```
\normalsize

\section{ Kernels over $\mathcal{X}=\mathbb{R}^2$ }

$x=(x_1,x_2) \in \mathbb{R}^2$ and $y=(y_1,y_2) \in \mathbb{R}^2$,


\subsection{Let $\phi(x)=(x_1^2,\sqrt{2}x_1x_2,x_2^2)$,verify that $\phi(x)^T\phi(y) = (x^Ty)^2$}

$$(x^Ty)^2=(\begin{bmatrix} x_1\\x_2 \end{bmatrix}_{2\times1}\begin{bmatrix} y_1&y_2 \end{bmatrix}_{1\times2})^2=(x_1y_1+x_2y_2)^2=x_1^2y_1^2+2x_1x_2y_1y_2+x_2^2y_2^2$$

$$=\begin{bmatrix} x_1^2\\\sqrt{2}x_1x_2\\x_2^2 \end{bmatrix}_{3\times1}\begin{bmatrix} y_1^2&\sqrt{2}y_1y_2&y_2^2 \end{bmatrix}_{1\times3}=\phi(x)^T\phi(y) \qquad\blacksquare$$


\subsection{Find a function $\phi(x)$:$\mathbb{R}^2 \mapsto \mathbb{R}^6$ such that for any $(x,y)$, $\phi(x)^T\phi(y) = (x^Ty+1)^2$}

$$(x^Ty+1)^2=(x_1y_1+x_2y_2+1)^2=x_1^2y_1^2+2x_1y_1+2x_1x_2y_1y_2+2x_2y_2+x_2^2y_2^2+1$$
$$=\begin{bmatrix} x_1^2\\\sqrt{2}x_1\\\sqrt{2}x_1x_2\\\sqrt{2}x_2\\x_2^2\\1 \end{bmatrix}_{6\times1}\begin{bmatrix} y_1^2&\sqrt{2}y_1&\sqrt{2}y_1y_2&\sqrt{2}y_2&y_2^2&1 \end{bmatrix}_{1\times6}$$
Thus, $$\phi(x)=(x_1^2,\sqrt{2}x_1,\sqrt{2}x_1x_2,\sqrt{2}x_2,x_2^2,1) \qquad \blacksquare$$

\subsection{Find a function $\phi(x)$:$\mathbb{R}^2 \mapsto \mathbb{R}^9$ such that for any $(x,y)$, $\phi(x)^T\phi(y) = (x^Ty+1)^2$}

$$(x^Ty+1)^2=x_1^2y_1^2+2x_1y_1+\frac12x_1x_2y_1y_2+\frac12x_2x_1y_2y_1+\frac12x_1^{0.5}x_2x_1^{0.5}y_1^{0.5}y_2y_1^{0.5}+\frac12x_2^{0.5}x_1x_2^{0.5}y_2^{0.5}y_1y_2^{0.5}+2x_2y_2+x_2^2y_2^2+1$$
$$=\begin{bmatrix} x_1^2\\\sqrt{2}x_1\\\frac1{\sqrt{2}}x_1x_2\\\frac1{\sqrt{2}}x_2x_1\\\frac1{\sqrt{2}}x_1^{0.5}x_2x_1^{0.5} \\\frac1{\sqrt{2}}x_2^{0.5}x_1x_2^{0.5} \\\sqrt{2}x_2\\x_2^2\\1 \end{bmatrix}_{9\times1}\begin{bmatrix} y_1^2&\sqrt{2}y_1&\frac1{\sqrt{2}}y_1y_2&\frac1{\sqrt{2}}y_2y_1&\frac1{\sqrt{2}}y_1^{0.5}y_2y_1^{0.5} &\frac1{\sqrt{2}}y_2^{0.5}y_1y_2^{0.5} &\sqrt{2}y_2&y_2^2&1 \end{bmatrix}_{1\times9}$$

Thus, $$\phi(x)=(x_1^2,\sqrt{2}x_1,\frac1{\sqrt{2}}x_1x_2,\frac1{\sqrt{2}}x_2x_1,\frac1{\sqrt{2}}x_1^{0.5}x_2x_1^{0.5} ,\frac1{\sqrt{2}}x_2^{0.5}x_1x_2^{0.5},\sqrt{2}x_2,x_2^2,1) \qquad\blacksquare$$



\subsection{Verify that $K(x,y)=(1+x^Ty)^d$ for $d=1,2\ldots$ is a positive definite kernel}

We know that $x^Ty=\begin{bmatrix} x_1\\x_2 \end{bmatrix}_{2\times1}\begin{bmatrix} y_1&y_2 \end{bmatrix}_{1\times2})=x_1y_1+x_2y_2=y^Tx$, then

$$k(x,y)=(1+x^Ty)^d=(1+y^Tx)^d=k(y,x)$$
Thus, $k(x,y)$ is symmetric.

$$\exists \phi(x)=\left(\sqrt{\binom{k}{0}}x_1^{k}x_2^{0},\sqrt{\binom{k}{1}}x_1^{k-1}x_2^{1},\cdots,\sqrt{\binom{k}{k-1}}x_1^{1}x_2^{k-1},\sqrt{\binom{k}{k}}x_1^{0}x_2^{k}\right)$$

Make

$$k(\phi(x),\phi(y))=\phi(x)^T\phi(y)=\sum_{l=0}^k\binom{k}l(x_1y_1)^{k-l}(x_2y_2)^{l}=(x_1y_1+x_2y_2)^{k}=(x^Ty)^{k}$$

$k(\phi(x),\phi(y))$ is a p.d. kernel and $\|\sum_{i=1}^2\alpha_i\phi_i(x)\|^2\ge0$.
Therefore,

$$k(x,y)=(1+x^Ty)^d=\sum_{k=0}^d\binom{d}{k}(x^Ty)^k1^{d-k}=\sum_{k=0}^d\binom{d}k\phi(x)^T\phi(y)$$

For $d,k\in\mathbb{N^+}$,

$$\sum_{i=1}^2\sum_{j=1}^2\alpha_i\alpha_j<x_i,x_j>_{R^d}=\sum_{i=1}^2\sum_{j=1}^2\alpha_i\alpha_j\left[\sum_{k=0}^d\binom{d}k\phi_i(x)^T\phi_j(x)\right]=\sum_{k=0}^d\binom{d}k\left[\sum_{i=1}^2\sum_{j=1}^2\alpha_i\alpha_j<\phi_i(x)^T\phi_j(x)>_{R}\right]$$
$$=\sum_{k=0}^d\binom{d}k\|\sum_{i=1}^2\alpha_i\phi_i(x)\|^2\ge0$$

Therefore, for $x_1,x_2$, $x_i\in\mathbb{R^d}$; $\alpha_1,..\alpha_2$, $\alpha_i\in\mathbb{R}$
$k(x,y)=(1+x^Ty)^d$ is a positive definite kernel. $\qquad\blacksquare$


\subsection{find a function $\phi$:$\mathbb{R}^2 \mapsto H$, where $H$ is an inner product space such that for any $(x,y)$, $<\phi(x),\phi(y)>_H = x^Ty -1$}

$$k(x,y)=<\phi(x),\phi(y)>_H = x^Ty -1=\begin{bmatrix} x_1\\x_2 \end{bmatrix}_{2\times1}\begin{bmatrix} y_1&y_2 \end{bmatrix}_{1\times2}-1=x_1y_1+x_2y_2-1$$

For all $|x|<\frac{\sqrt{2}}{2}$, when $\alpha_1=\alpha_2=1$, $<\phi(x),\phi(x)>_H=2x^2-1<0$.

By the property of Kernel, $k(x,y)$ should be positive. 

Therefore, there isn't a function $\phi$:$\mathbb{R}^2 \mapsto H$ that can make $<\phi(x),\phi(y)>_H = x^Ty -1$. $\qquad\blacksquare$










