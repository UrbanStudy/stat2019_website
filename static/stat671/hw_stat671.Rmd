---
title: ''
# fontfamily: mathpazo
fontsize: 10pt
geometry: margin=3mm
#linestretch: 0.1
classoption:
- portrait
pagenumbering: TRUE
#whitespace: none
output:
  pdf_document:
    toc: FALSE
    number_sections: TRUE
header-includes:
    - \usepackage{multicol}
    - \usepackage{multirow}
    - \usepackage{caption}
    - \setlength\tabcolsep{0.1pt}
    - \setlength\lineskip{0pt}
    - \setlength\parskip{0pt}
---


671 HW1

\section{ Classifier }
\subsection{proof}
Dot product $\vec a\vec b=a_xb_x+a_yb_y=|\vec a||\vec b|\cos(\theta)$

Exercise: 


$g(x)=<C_+-C_-,X-C>=<C_+,X>-<C_-,X>-<C_+,C>+<C_-,C>$;

$<C_+,X>=<\frac1{n_{+}}\sum\limits_{l\in I_+}^nx_i,x>$;

$<C_-,X>=<\frac1{n_{-}}\sum\limits_{l\in I_-}^nx_i,x>$;

$<C_+,C>=<C_+,\frac12C_+>+<C_+,\frac12C_->=\frac1{2n_{+}^2}\sum\limits_{(i,j)\in I_{+}}<x_i,x_j>+\frac12<C_+,C_->$

$<C_-,C>=<C_-,\frac12C_+>+<C_-,\frac12C_->=\frac12<C_+,C_->+\frac1{2n_{-}^2}\sum\limits_{(i,j)\in I_{-}}<x_i,x_j>$


$g(x)=\sum_{l=1}^n\alpha_i<x_i,x>+b$, 

$b=\frac12\left[\frac1{n_{-}^2}\sum\limits_{(i,j)\in I_{-}}<x_i,x_j>-\frac1{n_{+}^2}\sum\limits_{(i,j)\in I_{+}}<x_i,x_j>\right]$

$\alpha_i=\begin{cases}\frac1{n_{+}}&y_i=+1\\-\frac1{n_{-}}&y_i=-1\end{cases}$


\section{ Classifier }
\subsection{iris data clasification}


\fontsize{10pt}{0pt}
\footnotesize

- Import the iris data

```{r,echo=T,message=F,warning=F,out.width='100%'}
rm(list=ls())
library(datasets)
data(iris)
```

```{r,echo=T,message=F,warning=F,out.width='100%',fig.align='center'}
library(ggplot2)
GGally::ggpairs(iris,mapping=aes(color =Species,shape=Species,alpha=0.3),
        columns=c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"))+theme_light()
```

- Define the train and test sets

```{r,echo=T,message=F,warning=F,out.width='100%'}
iris$class <- NA
iris_setosa <- iris[iris$Species=="setosa",]
iris_versicolor <- iris[iris$Species=="versicolor",]
iris_virginica <- iris[iris$Species=="virginica",]

iris_train_se<- iris_setosa[1:40,]
iris_train_ve<- iris_versicolor[1:40,]
iris_train_vi<- iris_virginica[1:40,]

iris_test_se<- iris_setosa[41:50,]
iris_test_ve<- iris_versicolor[41:50,]
iris_test_vi<- iris_virginica[41:50,]

iris_train_se.ve<- rbind(iris_train_se,iris_train_ve)
iris_train_ve.vi<- rbind(iris_train_ve,iris_train_vi)
iris_test_se.ve<- rbind(iris_test_se,iris_test_ve)
iris_test_ve.vi<- rbind(iris_test_ve,iris_test_vi)
```


```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
# Define a kernel function and classifier
k <- function(x,y){return(x %*% t(y))}
classifier <- function(test,train_po,train_ne){
  
  for(i in 1:nrow(train_po)){
    a_po = 1/nrow(train_po)*sum(k(as.matrix(test),train_po[i,]))
  }
  for(i in 1:nrow(train_ne)){
    a_ne = 1/nrow(train_ne)*sum(k(as.matrix(test),train_ne[i,]))
  }
  
  for(i in 1:(nrow(train_po)-1)){
    for (j in (i+1):nrow(train_po)){
      b_po = -0.5*(1/nrow(train_po)^2)*sum(k(as.matrix(train_po[i,]),train_po[j,]))
    }
  }
  for(i in 1:(nrow(train_ne)-1)){
    for (j in (i+1):nrow(train_ne)){
      b_ne = 0.5*(1/nrow(train_ne)^2)*sum(k(as.matrix(train_ne[i,]),train_ne[j,]))
    }
  }
  
  class = a_po+a_ne+b_po+b_ne
  return(class)
}
for (i in 1:20){
iris_test_se.ve[i,6] <- classifier(iris_test_se.ve[i,1:4],iris_train_se[,1:4],iris_train_ve[,1:4])
}
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
# define a 3D grid
kd <- with(iris_test_se.ve, MASS::kde2d(iris_train_se.ve$Petal.Length,iris_train_se.ve$Petal.Width, n = 20))
kd <- MASS::kde2d(iris_train_se.ve$Petal.Length,iris_train_se.ve$Petal.Width, n = 20)
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
# Plot the test and 3D grid
library(plotly)
plot_ly (type='surface',x=kd$x,y=kd$y,z =kd$z, opacity=0.95)%>%add_markers( x = ~iris_test_se.ve$Petal.Length, y = ~iris_test_se.ve$Petal.Width, z = ~iris_test_se.ve$class-2, color = ~iris_test_se.ve$Species,size =10, symbol=iris_test_se.ve$Species, symbols = c('circle-open','cross'))
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
for (i in 1:100){
iris[i,6] <- classifier(iris[i,1:4],iris_train_se.ve_setosa[,1:4],iris_train_se.ve_versicolor[,1:4])
}
plot_ly (type='surface',x=kd$x,y=kd$y,z =kd$z)%>%add_markers( x = ~iris$Petal.Length, y = ~iris$Petal.Width, z = ~iris$class-2, color = ~iris$Species,size =1 )
```

- Define the kernel function and Computing the classifier

```{r,echo=T,message=F,warning=F,out.width='100%'}
k = function(x,y)  return(sum(x*y))
k.pp=outer(1:40,1:40,Vectorize(function(i,j) k(iris_train_se[i,1:4],iris_train_se[j,1:4])))
k.mm=outer(1:40,1:40,Vectorize(function(i,j) k(iris_train_ve[i,1:4],iris_train_ve[j,1:4])))
b=(sum(k.mm)/(40^2)-sum(k.pp)/(40^2))/2
alpha=ifelse(iris_train_se.ve$Species=="setosa",1/40,-1/40)

k.x=outer(1:80,1:20,Vectorize(function(i,j) k(iris_train_se.ve[i,1:4],iris_test_se.ve[j,1:4])))
iris_test_se.ve[,6]=(t(k.x)%*%alpha+b)

```

```{r,echo=T,message=F,warning=F,out.width='100%'}
k = function(x,y)  return(sum(x*y))
k.pp=outer(1:40,1:40,Vectorize(function(i,j) k(iris_train_vi[i,1:4],iris_train_vi[j,1:4])))
k.mm=outer(1:40,1:40,Vectorize(function(i,j) k(iris_train_ve[i,1:4],iris_train_ve[j,1:4])))
b=(sum(k.mm)/(40^2)-sum(k.pp)/(40^2))/2
alpha=ifelse(iris_train_ve.vi$Species=="virginica",1/40,-1/40)

k.x=outer(1:80,1:20,Vectorize(function(i,j) k(iris_train_ve.vi[i,1:4],iris_test_ve.vi[j,1:4])))
iris_test_ve.vi[,6]=(t(k.x)%*%alpha+b)

```


```{r,echo=F,message=F,warning=F,out.width='100%'}
# Define a grid 
g.n=20
x.min=min(iris_test_se.ve[,3:4])
x.max=max(iris_test_se.ve[,3:4])
z.hat=matrix(NA,nrow=g.n,ncol=g.n)
g=seq(from=x.min,to=x.max,length.out=g.n)
for (i in (1:g.n)){
  for (j in (1:g.n)){
    u=c(0,0,g[i],g[j])
    k.x=outer(1:80,1,Vectorize(function(i,j) k(as.matrix(iris_train_se.ve[i,1:4]),u)))
    z.hat[i,j]=sum(k.x*alpha)+b
  }
}
```



```{r,echo=F,message=F,warning=F,out.width='100%'}
# Plot the test and grid
z.zero=matrix(0,nrow=20,ncol=20)
library(plotly)
p <- plot_ly (type = 'surface' , x=g,y=g,z=(z.hat), opacity = 0.9 ,showscale =T )%>%
  add_markers( x = iris_test_se.ve$Petal.Length, y = iris_test_se.ve$Petal.Width, z = iris_test_se.ve$class, 
               color =iris_test_se.ve$Species, size=1,
               symbol=iris_test_se.ve$Species, symbols = c('circle-open','cross'),
               opacity = 1)%>%
  layout(scene = list(xaxis = list(title = 'Petal.Length'),
                     yaxis = list(title = 'Petal.Width'),
                     zaxis = list(title = 'Class')))

```

- Evaluate the classifier

```{r,echo=T,message=F,warning=F,out.width='100%',collapse=T}
iris_test_se.ve$evaluate=ifelse(iris_test_se.ve$class>0,"setosa","versicolor")
iris_test_se.ve$evaluate=ifelse(iris_test_se.ve$Species==iris_test_se.ve$evaluate,"Rigt","Wrong")
length(which(iris_test_se.ve$evaluate=="Wrong"))/20

iris_test_ve.vi$evaluate=ifelse(iris_test_ve.vi$class>0,"virginica","versicolor")
iris_test_ve.vi$evaluate=ifelse(iris_test_ve.vi$Species==iris_test_ve.vi$evaluate,"Rigt","Wrong")
length(which(iris_test_ve.vi$evaluate=="Wrong"))/20
```

\begin{center}
    \captionof{table}{Confusion matrix}
    \label{tab:confusion-matrix}
\begin{tabular}{|c|c|c|c||c|c|c|}
\hline
\multirow{2}{*}{}& \multicolumn{6}{c|}{Actural Species} \\
\cline{2-7}
                             & test 1 & Setosa & Versicolor & test 2 & Virginica & Versicolor \\\hline

\multirow{2}{*}{Test Species}& Setosa & 10 & 0 & Virginica & 9 & 0\\
\cline{2-7}
                             & Versicolor & 0 & 10 & Versicolor & 1 & 10\\
\hline
\end{tabular}
  \end{center}

Error rate = 0% and 5% in two tests respectively.

```{r,echo=F,message=F,warning=F,out.width='100%'}
library(pander)
library(dplyr)
pander(bind_cols(iris_test_se.ve[,c(5:7)],iris_test_ve.vi[,c(5:7)]),caption="setosa v.s.versicolor/  /virginica v.s.versicolor")
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
library(kableExtra)
kable(list(iris_test_se.ve[,c(5:7)],iris_test_ve.vi[,c(5:7)]),format="latex",booktabs=F)%>%
     add_header_above(c("setosa v.s.versicolor" = 4,"virginica v.s.versicolor" = 4))
```


```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
library(kableExtra)
kable(iris_test_se.ve[,c(5:7)],format="latex",booktabs=F)%>%
     kable_styling(c("striped", "bordered"), full_width = F,font_size = 8)%>%
     add_header_above(c("setosa v.s.versicolor" = 4))
```

```{r,eval=F,include=F,message=F,warning=F,out.width='100%'}
kable(iris_test_ve.vi[,c(5:7)],format="latex",booktabs=F)%>%
     kable_styling(c("striped", "bordered"), full_width = F,font_size = 8)%>%
     row_spec(13,background ="yellow")%>%
     add_header_above(c("virginica v.s.versicolor" = 4))
```

\section{ Perceptron }
\subsection{ pseduo code }


```{r, eval=F}
k_percetron <- function(data,T,n){
  alpha=rep(0,n)
  for (i in 1:n){
    for (j in 1:n){
      K[i,j]=alpha %*% k(dataX[i,],dataX[j,])
    }
  }
  for (t in 1:T){
    for (i in 1:n){
      ifelse (dataX$y[i]%*%alpha[t]%*%data$x[i,]<0, alpha[t]+data$y[i]%*%data$x[i,], alpha[t])
    }
  }
  return(sum(K[i,j]))
}
```



```{r, eval=F}
data <- data.frame(matrix(runif(200,min = -10, max=10),nrow = 100, ncol = 2))
data$y <- ifelse(data$X1<=data$X2,1,-1)

k_percetron <- function(data,T){
  alpha=rep(0,100)
  for (i in 1:100) {
    for (j in 1:100){
      Kij=alpha[i] %*% k(as.matrix(data[i,-3]),as.matrix(data[,-3]))
    }
  }
  for (t in 1:T){
    for (i in 1:100){
      alpha[t] =ifelse (as.matrix(data$y[i])%*%alpha[t]%*%as.matrix(data[i,-3])<0, alpha[t]+as.matrix(data$y[i])%*%as.matrix(data[i,-3]), alpha[t])
    }
  }
}
# use kernel 1
k <- function(a,b){return(a %*% t(b))}
k_percetron(data,10)
# use kernel 2
k <- function(a,b){return((a %*% t(b))^2)}
k_percetron(data,10)
# use kernel 3
k <- function(a,b){return((a %*% t(b) +1)^2)}
k_percetron(data,10)
```

```{r,echo=T,message=F,warning=F,out.width='50%'}
rm(list=ls())
## generate 2d data
n.p=50
n.m=50
n=n.p+n.m
library(mvtnorm)
data.p=rmvnorm(n=n.p,mean=c(1,1)+c(2,2),sigma=diag(rep(1,2)))
data.m=rmvnorm(n=n.m,mean=c(-1,-1)+c(2,2),sigma=diag(rep(2,2)))
class = c(rep(1,n.p),rep(-1,n.m))
data=rbind(data.p,data.m)
id_train=sort(sample(1:100,size=80))
id_test=sort(c(1:100)[-id_train])
data_train=data[id_train,]
class_train=class[id_train]
n_train=dim(data_train)[1]
data_train.m=data_train[class_train==-1,]
data_train.p=data_train[class_train==1,]
n_train.m=dim(data_train.m)[1]
n_train.p=dim(data_train.p)[1]
data_test=data[id_test,]
class_test=class[id_test]
n_test=dim(data_test)[1]
plot(data_train,col=class_train+rep(2,n_train),pch=1);points(data_test,col=class_test+rep(2,n_test),pch=3)
```


```{r, eval=F}
## compute the classifier by k1
k = function(x,y)  return(sum(x*y))
k_percetron=function(u){
  k.x=outer(1:n_train,1,Vectorize(function(i,j) k(data_train[i,],data_train[j,])))
  return(sum(alpha*k.x))
}  
alpha=matrix(NA,nrow=n_train,ncol=2)
alpha[1,]=class_train[1]*data_train[1,]
for (i in (2:n_train)){
  if ((k_percetron(data_train[i,])*class_train[i])<0) alpha[i,]=alpha[i,]+class_train[i]*data_train[i,]
}
alpha
z.hat=matrix(NA,nrow=20,ncol=1)
k.x=outer(1:80,1:20,Vectorize(function(i,j) k(data_train[i,],data_train[j,])))
for (i in (1:20)){
z.hat[i]=sum(t(k.x[,i])%*%alpha)
}
z.hat
```



```{r}
## compute the classifier by k1
k = function(x,y)  return(sum(x*y))

k_percetron=function(n.test){
  
alpha=rep(0,n_train)
alpha[1]=class_train[1]  
  
for (j in (2:n_train)){  
  k.x=outer(1:n_train,1,Vectorize(function(i,j) k(data_train[i,],data_train[j,])))
  
  if ((sum(alpha*k.x)*class_train[j])<0) alpha[j]=alpha[j]+class_train[j]*data_train[j,]
}

z.hat=matrix(NA,nrow=n.test,ncol=1)
k.x=outer(1:80,1:20,Vectorize(function(i,j) k(data_train[i,],data_train[j,])))
for (t in (1:n.test)){
z.hat[t]=sum(t(k.x[,t])%*%alpha)
}
return(z.hat)
}

```


\subsection{Using 3 Kernels}

```{r,echo=F,message=F,warning=F,out.width='100%'}
# Plot the test and grid
k1_test <- cbind(data.frame(x1=data_test[,1]),data.frame(x2=data_test[,2]),data.frame(class=class_test),data.frame(predict=c(k_percetron(20))))

g.n=20
x.min=min(data_test)
x.max=max(data_test)
y.hat=matrix(NA,nrow=g.n,ncol=g.n)
g=seq(from=x.min,to=x.max,length.out=g.n)
for (i in (1:g.n)){
  for (j in (1:g.n)){
    u=c(g[i],g[j])
    k.x=outer(1:n_train,1,Vectorize(function(i,j) k(data_train[i,],u)))
    y.hat[i,j]=sum(k.x*alpha)
  }
}

library(plotly)
plot_ly (type = 'surface' , x=g,y=g,z=(y.hat), opacity = 0.9 ,showscale =T )%>%
  add_markers( x = k1_test$x1, y = k1_test$x2, z = k1_test$predict, 
               color =k1_test$class, size=1,
               symbol=k1_test$class, symbols = c('circle-open','cross'),
               opacity = 1)%>%
  layout(scene = list(xaxis = list(title = 'x1'),
                     yaxis = list(title = 'x2'),
                     zaxis = list(title = 'predict')))
plot_ly ( k1_test,x = k1_test$x1, y = k1_test$x2, z =k1_test$predict,type = 'scatter3d', color=k1_test$class,symbol=k1_test$class, symbols = c('circle-open','cross'))%>%
  add_surface(x=g,y=g,z=(y.hat), opacity = 0.9 ,showscale =T)
```


```{r eval=F}
## evaluate the classifier

    for (j in 1:100){
    for (i in 1:100){
      alpha[j] =ifelse (as.matrix(data$y[i])%*%alpha[j]%*%as.matrix(data[i,-3])<0, alpha[j]+as.matrix(data$y[i])%*%as.matrix(data[i,-3]), alpha[j])
    }
  }
```


```{r}

## compute the classifier by k1
k1_test <- k_percetron(20)
## compute the classifier by k2
k = function(x,y)
  return(sum(x*y)+1)
k2_test <- k_percetron(20)
## compute the classifier by k3
k = function(x,y)
  return((1+sum(x*y))^2)
k3_test <- k_percetron(20)

pander(cbind(k1_test,k2_test,k3_test),caption="k1/ k2 /k3")
```

\section{ Kernels over $\mathcal{X}=\mathbb{R}^2$ }

\subsection{Verify that $\phi(x)^T\phi(y) = (x^Ty)^2$}
\begin{enumerate}
\item Find a function $\phi(x)$:$\mathbb{R}^2 \mapsto \mathbb{R}^6$ such that for any $(x,y)$, $\phi(x)^T\phi(y) = (x^Ty+1)^2$
\item Find a function $\phi(x)$:$\mathbb{R}^2 \mapsto \mathbb{R}^9$ such that for any $(x,y)$, $\phi(x)^T\phi(y) = (x^Ty+1)^2$
\item Verify that $$K(x,y)=(1+x^Ty)^d$$ for $d=1,2\ldots$ is a positive definite kernel
\item find a function $\phi$:$\mathbb{R}^2 \mapsto H$, where $H$ is an inner product space such that for any $(x,y)$, $<\phi(x),\phi(y)>_H = x^Ty -1$
\end{enumerate}





