%&hylatex
%File: c:\paul\550.436s\hwkassign.tex Wed Feb 07 11:38:34 2001

\documentclass{article}[12pt]

\usepackage{graphicx}   %       Graphics bundle
\usepackage{pspicture}  %       PSPicture
\usepackage{amsmath}
\usepackage{amssymb}
\setlength{\topmargin}{-0.4in}
\setlength{\topskip}{0.3in}    % between header and text
\setlength{\textheight}{9in} % height of main text
\setlength{\textwidth}{6.5in}    % width of text
\setlength{\oddsidemargin}{0in} % odd page left margin
\setlength{\evensidemargin}{0in} % even page left margin
\linespread{1.3}
\setlength{\parindent}{0pt}

\pagestyle{myheadings}
\usepackage{graphicx}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{color}
\usepackage{hyperref}



%\input{commands}

\begin{document}
\thispagestyle{empty}
\includegraphics{../../../psulogo_horiz_bw.eps}\hfill\includegraphics{../../../deptlogo}
\vspace{10pt}
\begin{center}
{\large\bf STAT 671}    \\*[5pt] {\Large Statistical Learning I}
\\*[12pt] {\large Fall 2019}
\\ {\large Homework 4}
\\ {\large Due November 13$^{th}$ at the beginning of class}
\end{center}
\vspace{1cm}
\noindent

This homework is an exan that was given during a previous iteration of this course. I have voluntarily kept the presentation as it was for the exam. 
\noindent
Do good work.

\noindent 
Best regards, 
\section{Multivariate Fisher Kernel}
The Fisher kernel is a kernel for data in $\mathbb{R}^d$ which is based on a probabilistic model. Specifically, let $x \in \mathbb{R}^d$, and let $p_{\theta}$ be a probability density function indexed by a parameter vector $\theta \in \mathbb{R}^k$.  First, fix some $\theta_0 \in \mathbb{R}^k$ and define the Fisher score of a data point $x$, by $\phi(x)$, the gradient of the logarithm of $p_{\theta}$ evaluated at $\theta=\theta_0$. That is, 
\begin{equation}
\phi(x)=\nabla_\theta \ln p_{\theta}(x) |_{\theta=\theta_0}
\end{equation}
Thus $\phi: \mathbb{R}^d \to \mathbb{R}^k$. 

Next, define the Fisher information matrix as the following expected value:
\begin{equation}
I = E_{p_{\theta_0}}[\phi(X)\phi^T(X)]
\end{equation}
Note that $I$ is a $(k,k)$ matrix. You can use the fact that this matrix is symmetric and positive definite. 
Finally, define the Fisher kernel as 
\begin{equation}
K(x,y)=\phi(x)^T I^{-1} \phi(y)
\end{equation}
for any couple of points $(x,y)$, both in $\mathbb{R}^d$
\begin{enumerate}  
\item Verify that $K$ is symmetric
\newpage
\item Verify that $K$ is positive definite
\newpage
\item
Consider the following multivariate Normal model with given invertible covariance matrix $\Lambda^{-1}$: 
\begin{equation}
p_\theta(x)=(2\pi)^{-d/2}(\det{\Lambda})^{1/2}\exp\left(-\frac{1}{2}(x-\theta)^T\Lambda (x-\theta)\right)
\end{equation}
Show that 
\begin{equation}
\phi(x)=\Lambda (x - \theta_0)
\end{equation}
\newpage
\item Compute the Fisher information matrix $I$ for this model
\newpage
\item Compute the Fisher kernel for this model
\newpage
\end{enumerate}
\section{Optimal ordering}
We consider a binary classification problem where the training set is $\mathcal{D}=\{(x_i,y_i), 1\leq i \leq n\}$, with $x_i \in \mathbb{R}^d$ and $y_i \in \{-1,+1\}$ is the class. For convenience, we consider the following subset of indices 
\begin{eqnarray}
I_-&=&\{i, 1\leq i \leq n, y_i=-1\}\\
I_+&=& \{i, 1\leq i \leq n, y_i=+1\}
\end{eqnarray}
Moreover, notate $n_-$ the number of elements in $I_-$, and $n_+$ the number of elements in $I_+$. Thus 
\begin{equation}
n_-+n_+=n
\end{equation}
 Our objective is to use $\mathcal{D}$ to construct a function $f:\mathbb{R}^d \to \mathbb{R}$ that assign larger values to the data points in the positive class than to the data points in the negative class while being smooth in some sense. 

Thus, we propose to choose $f$ in a RKHS with a kernel K that minimizes the following functional:
\begin{equation}
J_0(f) = \frac{1}{n_- n_+}\sum_{i \in I_-}\sum_{j \in I_+} \mathbb{I}_{f(x_i)>f(x_j)} + \lambda ||f||_H^2
\end{equation}
 where $\mathbb{I}_u$ is the indicator function of the event $u$. It takes the value 1 if the event u occurs and 0 otherwise and $\lambda>0$. 

Since $J_0$ is not convex, we propose instead to minimize
\begin{equation}
J(f) = \frac{1}{n_- n_+}\sum_{i \in I_-}\sum_{j \in I_+}\left(1-\left(f(x_j)-f(x_i)\right)\right) + \lambda ||f||_H^2
\end{equation}
\newpage
\begin{enumerate}
\item Show that the minimum is achieved for a function $f$ index by a parameter $\alpha \in \mathbb{R}^d$ and of the form
\begin{equation}
f(x)=\sum_{i=1}^n \alpha_i K(x_i,x)
\end{equation}
Note: do not invoke the representer theorem. Instead, use the key elements in the proof of this theorem applied to this particular case. 
\newpage
\item Rewrite then $J(f)$ as a functional $J(\alpha)$ using the notation  $K$ for the $(n,n)$ matrix $K_{ij}=K(x_i,x_j)$ and $K_i$ for the $i^{th}$ column of $K$.
\newpage 
\item Simplify further the expression $J(\alpha)$ using the notations:
\begin{eqnarray}
K_-&=&\frac{1}{n_-}\sum_{i \in I_-}K_i\\
 K_+&=&\frac{1}{n_+}\sum_{i \in I_+}K_i
\end{eqnarray}

\newpage
\item Compute $\nabla_\alpha J(\alpha)$, the gradient in $\alpha$ of $J(\alpha)$. Solve for $\alpha$, assuming that $K$ 
is invertible. 
\newpage
\item Bonus question: Compute $f^*(x)$, where $f^*$ is the minimizer of $J(f)$ for the linear kernel $K(x,y)=x^Ty$. Use the notations
 \begin{eqnarray}
{x}_-&=&\frac{1}{n_-}\sum_{i \in I_-}x_i\\
{x}_+&=&\frac{1}{n_+}\sum_{i \in I_+}x_i
\end{eqnarray}
\end{enumerate}
\end{document}
