---
title: "Time series"
author: "shen"
date: "2020/9/7"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Time series

- periodic

```{r,eval=F}
diff(x,s=4,lag=4)
ts.plot(linear_growth)
```

- White noise

White Noise (WN) is the simplest example of a stationary process.

A weak white noise process has:

A fixed, constant mean.

A fixed, constant variance.

```{r,eval=F}
arima(y,order=c(0,0,0))
```


- Random walk

Random Walk (RW) is a simple example of a non-stationary process.

A random walk has:

No specified mean or variance.

Strong dependence over time.

Its changes or increments are white noise (WN).

Random walk with drift - I
The random walk with a drift:


$Y_t = c + Y_{t-1} + \epsilon_t$

where $\epsilon_t$ is mean zero white noise (WN).

Two parameters, the constant c , and the WN variance $\sigma_{\epsilon}^2$

```{r}
random_walk <- arima.sim(model = list(order = c(0, 1, 0)), n = 100)
ts.plot(random_walk)
ts.plot(diff(random_walk))
```

- Stationary processes

Weak stationarity - I: mean, variance, covariance constant over time.

Weak stationarity - II: Covariance of Y_tY and Y_sY is same (constant) for all $|t-s|= h$ for all h.

$Cov(Y_2, Y_5) = Cov(Y_7, Y_{10})$

ime trends, periodicity, and a lack of mean reversion

```{r,eval=F}
random_walk <- cumsum(white_noise)
```

The white noise (WN) and random walk (RW) models are very closely related. However, only the RW is always non-stationary, both with and without a drift term.

```{r}
white_noise <- arima.sim(model=list(order=c(0,0,0)),n=100)
random_walk <- cumsum(white_noise)
wn_drift <- arima.sim(model=list(order=c(0,0,0)),n=100,mean=0.4)
rw_drift <- cumsum(wn_drift)
plot.ts(cbind(white_noise, random_walk, wn_drift, rw_drift))
```

```{r,eval=F}
acf(x, lag.max =1, plot =F)
cor(x[-1], x[-n]) * (n-1)/n
```

The time series x shows strong persistence, meaning the current value is closely relatively to those that proceed it. 

The time series y shows a periodic pattern with a cycle length of approximately four observations, meaning the current value is relatively close to the observation four before it. 

The time series z does not exhibit any clear pattern.

- Autocorrelation

$y_t-\mu=\phi(y_{t-1}-\mu)+\varepsilon_t$; $\phi$ slope; 

$\phi=0$ white noise

$\phi<0$ oscillatory time series

$\phi=1$ Random walk

```{r}
x <- arima.sim(model =list(ar=0.5), n =100)
```


Persistence is defined by a high correlation between an observation and its lag, while anti-persistence is defined by a large amount of variation between an observation and its lag.

```{r,eval=F}
arima(x, order = c(1,0,0))
AR <-arima(AirPassengers,order=c(1,0,0))
AR_fitted <- AirPassengers - residuals(AR)
ts.plot(AirPassengers)
points(AR_fitted, type = "l", col = 2, lty = 2)

AR_forecast <- predict(AR_fit, n.ahead = 10)$pred
AR_forecast_se <- predict(AR_fit, n.ahead = 10)$se
points(AR_forecast, type = "l", col = 2)
points(AR_forecast - 2*AR_forecast_se, type = "l", col = 2, lty = 2)
points(AR_forecast + 2*AR_forecast_se, type = "l", col = 2, lty = 2)
```

- A simple moving average

$y_t=\mu+\varepsilon_t+\theta\varepsilon_{t-1}$; 

```{r}
data(Mishkin, package = "Ecdat")
inflation <- as.ts(Mishkin[, 1])
inflation_changes <- diff(inflation)
ts.plot(inflation_changes)
acf(inflation_changes,lag.max=24)
MA_inflation_changes <-arima(inflation_changes,order=c(0,0,1)) # MA model
AR_inflation_changes <-arima(inflation_changes,order=c(1,0,0)) # AR model

MA_inflation_changes; AR_inflation_changes

AIC(MA_inflation_changes);BIC(MA_inflation_changes);
AIC(AR_inflation_changes);BIC(AR_inflation_changes);
```


```{r}
MA_inflation_changes_fitted <- inflation_changes - residuals(MA_inflation_changes)
AR_inflation_changes_fitted <- inflation_changes - residuals(AR_inflation_changes)
cor(MA_inflation_changes_fitted ,AR_inflation_changes_fitted )

ts.plot(inflation_changes)
points(MA_inflation_changes_fitted, type = "l", col = 2, lty = 2)

MA_inflatio_forecast <- predict(MA_inflation_changes, n.ahead = 10)$pred
MA_inflatio_forecast_se <- predict(MA_inflation_changes, n.ahead = 10)$se

points(MA_inflatio_forecast, type = "l", col = 2)
points(MA_inflatio_forecast - 2*MA_inflatio_forecast_se, type = "l", col = 3, lty = 2)
points(MA_inflatio_forecast + 2*MA_inflatio_forecast_se, type = "l", col = 3, lty = 2)
```


```{r}
x <- arima.sim(model =list(ma=0.5), n =100)
```


MA model should show short-run dependence but reverts quickly to the mean, so it must be the MA model. 

AR and RW show strong persistence

WN does not show any clear patterns.

Plot A shows autocorrelation for the first lag only, which is consistent with the expectations of the MA model. 

Plot B shows dissipating autocorrelation across several lags, consistent with the AR model. 

Plot C is consistent with a RW model with considerable autocorrelation for many lags. 

Finally. Plot D shows virtually no autocorrelation with any lags.

```{r}
library(astsa)
```


## Example of 2015f1

The first-order autoregression model of GDP growth can be estimated by computing OLS estimates in the regression of $Y_t$ on $Y_{t-1}$

$$y_t = \beta_0 + \beta_1  y_{t-1}+\mu_t$$

- Check Autocorrelation


```{r,echo=F}
ts_2015f1 <- ts(table_2015f1[,3])
dif_2015f1 <- diff(ts_2015f1)
ts.plot(ts_2015f1)
ts.plot(dif_2015f1)
acf(ts_2015f1)
acf(dif_2015f1)
```

The time series plot of Y shows a strong autocorrelations.

The difference of $Y_t$ and $Y_{t-1}$ still shows some autocorrelations with lag=1.

```{r}
dif2_2015f1 <- diff(ts_2015f1,lag=2)
acf(dif2_2015f1)
```



```{r}
log.ts_2015f1 <- log(ts_2015f1)
ts.plot(log.ts_2015f1)
log.dif_2015f1 <- diff(log.ts_2015f1)
ts.plot(log.dif_2015f1)
acf(log.dif_2015f1)
#log.dif_2015f1 <- log(abs(dif_2015f1))
#ts.plot(log.dif_2015f1)
#acf(log.dif_2015f1)
```


Simple Moving Average is a method of time series smoothing and is forecasting technique.

```{r}
sma_2015f1 <- smooth::sma(ts_2015f1, silent=FALSE,interval=TRUE)# , h=1, require(Mcomp)
summary(sma_2015f1)
(mu <- sma_2015f1$fitted)
sd <- sma_2015f1$residuals
c(mu[20]-qt(.025,23,lower.tail = F)*sd[20],
  mu[20]+qt(.025,23,lower.tail = F)*sd[20])
```



```{r,echo=F, out.width='100%'}
table_2015f1$Y.dif <- c(0,diff(table_2015f1$Y))
table_2015f1$X2.dif <- c(0,diff(table_2015f1$X2))
#table_2015f1$X2.cum <- cumsum(table_2015f1$X2)
table_2015f1$Y.log.dif <- c(0,diff(log(table_2015f1$Y)))
table_2015f1$X2.log.dif <- c(0,diff(log(table_2015f1$X2)))
ggpairs(table_2015f1[,c(1:7)])
```

$Y_{dif}^2$ is not usefull


```{r}
(y.ar_2015f1 <-arima(ts_2015f1[,2],order=c(1,0,0)))
(y.ma_2015f1 <-arima(ts_2015f1[,2],order=c(0,0,1)))
(x2.ar_2015f1 <-arima(ts_2015f1[,1],order=c(1,0,0)))
(x2.ma_2015f1 <-arima(ts_2015f1[,1],order=c(0,0,1)))
```

```{r,out.width='25%'}
model_2015f1_3 <- lm(Y.dif ~X2.dif, table_2015f1[-1,])
summary(model_2015f1_3)
plot(model_2015f1_3 )

```

```{r,out.width='25%'}
model_2015f1_4 <- lm(Y.log.dif ~X2.log.dif, table_2015f1)
summary(model_2015f1_4)
plot(model_2015f1_4)
```


```{r,out.width='25%'}
model_2015f1_5 <- lm(Y ~X2.cum, table_2015f1)
summary(model_2015f1_5)
plot(model_2015f1_5,which=1:2 )
```



- Durbin-Watson test

```{r}
lmtest::dwtest(model_2015f1_3)
lmtest::dwtest(model_2015f1_4)
lmtest::dwtest(model_2015f1_5)
```

D-W is significant in model4, It's a violation of the independence assumption.

The serial autocorrelations should all be 0, the model3 is recommended.



### prediction

```{r}
ma_2015f1 <- ma(dif_2015f1, order = 4)# , centre = TRUE
ts.plot(ma_2015f1)
Acf(ma_2015f1)# ,na.action = na.pass
```


The prediction interval is

```{r}
pi <- predict(model_2015f1_3,data.frame(X2.dif=table_2015f1[20,5]),interval = "prediction",level=0.95)
c(pi[1]+table_2015f1[19,3],pi[2]+table_2015f1[19,3],pi[3]+table_2015f1[19,3])
```

When X1=20, the difference of X2 equals -728

```{r eval=T, echo=T,fig.show='hold'}
plot(table_2015f1[,1:2])
abline(h=1500)
segments(c(5,6,12,18,25),0,c(5,6,12,18,25),1500,col=8,lty = 2)
plot(table_2015f1[,c(1,5)])
abline(h=-728)
```

```{r}
(y.hat <- (ts_2015f1[,2]-residuals(y.ar_2015f1))[20])
y.hat-sqrt(y.ar_2015f1$sigma2)
y.hat+sqrt(y.ar_2015f1$sigma2)
```