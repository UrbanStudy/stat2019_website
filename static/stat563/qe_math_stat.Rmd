---
title: ""
subtitle: ""
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true

bibliography: bib_math_stat.bib
---


```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```

# 

## 2000

## 2004S

## 2004F

## 2007S

## 2007F

### 2007F2A

[2007F2A][] [2015S2A][] [2019SA2][] [@Basu] 

Suppose $Y_1$ and $Y_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:

$f(x)=\begin{cases}10e^{-10x}& x>0\\0& o.w.\end{cases}$

Find the p.d.f. of $X=Y_1-Y_2$.

### 2007F3A
[2015S1Ab][] [2008S2A][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Suppose $Y_1$ and $Y_2$ have the joint pdf
$f(y_1,y_2)=\begin{cases}2&0\le y1\le y2\le 1\\0& o.w.\end{cases}$

(a) Find the marginal density functions of $Y_1$ and $Y_2$ and check whether they are independent.

(b) Find $E[Y_1+Y_2]$

(c) Find $P(Y_1\le3/4|Y_2>1/3)$

### 2007F5A
[2013FB2][] [2014F1B][] [2015S1B][] [@CDF] [@MLE]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find $\hat\theta$, the mle of $\theta$.

(b) Find $E[\hat\theta]$.

(c) Prove that $\hat\theta$ is consistent for $\theta$.

### 2007F4B
[2015S3B][] [2018S1B][] [2019SB2][] [@Laplace] [@MLE]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

## 2008S

### 2008S2A
[2015S1Ab][] [2007F3A][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}c(1-y_2)&0\le y1\le y2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c.

(b) Find the marginal density functions of $Y_1$ and $Y_2$.

(c) Find $P(Y_2\le1/2|Y_1\le3/4)$

## 2008F

## 2009S

## 2009F

## 2010S

## 2010F

## 2011S

## 2011F

## 2012S

## 2012F

## 2013S

## 2013F




## 2014S

Crain, Kochar

### 2014S1A

## 2014F

### 2014F1A
[@Bern] [@MGF]

Repeat a sequence of i.i.d. Bernoulli trials until you observe the frst success, where p = the probability of a success and $q=1-p=$ the probability of a failure on any one trial. Let the random variable Y count the number of failures before the frst success.

(a) State the name of this statistical experiment.

(b) Provide a mathematical formula for the probability mass function, $P(Y=y)$ where $y=$?.

(c) Give in closed form the $P(Y\ge y)$.

(d) Determine the $E(Y)$.

(e) Derive the moment generating function (M.G.F.) of Y . Remember to state the interval over which this M.G.F. exists.

### 2014F2A

One percent of all individuals in a certain population are carriers of a particular disease. A diagnostic test for this disease has a 90% detection rate for carriers and a 5% detection rates for noncarriers. Suppose the test is applied independently to two different blood samples from the same randomly selected individual.

(a) What is the probability that both tests yield the same result?

(b) If both tests are positive, what is the probability that the selected individual is a carrier?


### 2014F1B
[2007F5A][] [2013FB2][] [2015S1B][] [@CDF] [@MLE] [@CI]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find the MLE $\hat\theta$ of $\theta$.

(b) Prove that $\hat\theta$ is consistent for $\theta$.

(c) Find a 95% confidence interval for $\theta$ when $n=6$.

### 2014F2B
[@Pois] [@FishI]

Let $X_1,X_2,..,X_{n}$ denote a random sample from a Poisson distribution with mean $\theta,\theta>0$.

(a) Find the Fisher information about $\theta$ in the sample.

(b) Suppose we want to estimate $m(\theta)=P(X_1=0)=e^{-\theta}$. Find a lower bound on the variance of any unbiased estimator of the parametric function $m(\theta)$.

### 2014F3B
[@UMP] [@HypoT] [@power]

Let $\theta$ be a parameter with space $\Omega=\{0; 1\}$. Let $X$ be a discrete random variable taking on values 1,2,3,or 4. Let the probability funtion of $X$ be given by the following table:

 ----------------------------------
            $X_1$ $X_2$ $X_3$ $X_4$
 ---------- ----- ----- ----- ----- 
 $\theta_0$ $1/2$ $1/4$ $1/8$ $1/8$
 
 $\theta_1$ $2/9$ $2/9$ $2/9$ $1/3$
 ----------------------------------

Find the UMP size 1/8 and 1/4 tests to test $H_0:\theta=0$ against $H_A:\theta=1$. Also find the powers of these two tests.

### 2014F4B
[@Norm] [@MLE]

Let $X_1,X_2,..,X_{n}$ be a random sample from a normal distribution, $N(\mu,\sigma^2)$, where $-\infty<\mu<+\infty$ and $\sigma>0$. Find the MLE of $\mu/\sigma$ and find itsexpected value.

### 2014F5B
[2017FB2][] [@Expo] [@suff]

Let $X_1,X_2,..,X_{n}$ denote a random sample from exponential distribution
with pdf,
$f(x,\mu)=\begin{cases}e^{-(x-\mu)}& \mu<x<\infty\\0& e.w.\end{cases}$

(a) Show that $X_{(1)}=\min\{X_i\}$ is a complete sufficient statistic.

(b) Are $X_{(1)}$ and the sample variance independent statistics? Justify your answer.

## 2015S

Tableman, Kochar

### 2015S1Aa

Let X be a random variable with finite mean $\mu$, finite variance $\sigma^2$,
and assume $E(X^8)<\infty$. Prove or disprove:

i. $E[(X-\mu/\sigma)^2]\ge1$.

ii. $E[(X-\mu/\sigma)^4]\ge1$.

### 2015S1Ab
[2007F3A][] [2008S2A][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Suppose a box contains a large number of tacks, and the probability $X$ that a particular tack will land with its point up when it is tossed varies from tack to tack in accordance with the following pdf:

$f(x)=\begin{cases}2(1-x)&0<x<1\\0& o.w.\end{cases}$

Suppose a tack is selected at random from this box and this tack is then tossed three times independently. Determine the probability the tack will land with its point up on all three tosses.

### 2015S2A
[2007F2A][] [2019SA2][] [@Basu] 

Suppose $Y_1$ and $Y_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:

$f(x)=\begin{cases}10e^{-10x}& x>0\\0& o.w.\end{cases}$

Find the p.d.f. of $X=Y_1-Y_2$.

### 2015S3A
[2016S3][] [@SNorm] [@mean]

First,let $\Phi(.)$ and $\phi(.)$ denote the standard normal cdf and pdf respectively. Then, let $X_1,..,X_n$ denotes a random sample from a normal distribution with means $\theta$ and variance $\sigma^2$, and let $F(.)$ and $f(.)$ denote the common cdf and pdf of the r.s. respectively. Assume the sample size $n$ is odd; that is, $n=2k-1$; $k=1,2,3,..$ In this situation, the sample
median is the $k^{th}$ order statistic, denoted by $Y_k$.

(a) (5) Let $g(y)$ denote the pdf of the sample median $Y_k$. Derive $g(y)$. You may use the symbols $F(.)$ and $f(.)$.

(b) (5) Determine the $E(Y_k|\bar X)$, where $\bar X$ is the sample mean of the above random sample. Justify your answer.

### 2015S4A
[@CDF]

(a) Let X be a continuous type random variable with cumulative distribution function F(x). Find the distribution of the random variable
$Y=\ln(1-F(X))$:

(b) Prove that for any $y\ge c$, the function
$G_c(y)=P[X\le y|X\ge c]$


### 2015S5A
[@Pois]

Suppose X and Y are independent Poisson random variables with parameters $\lambda$ and $2\lambda$, respectively.

(a) Find the distribution of $X+Y$.

(b) Find $E[X|X+Y=5]$.

### 2015S1B
[2007F5A][] [2013FB2][] [2014F1B][] [@CDF] [@MLE]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find $\hat\theta$, the mle of $\theta$.

(b) Find $E[\hat\theta]$.

(c) Prove that $\hat\theta$ is consistent for $\theta$.

### 2015S2B
[2018S2B][] [@FishI]

Let $X_1,X_2,..,X_n$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 90th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound @CRLB on the variance of any unbiased estimator of $g(\theta)$.

### 2015S3B
[2007F4B][] [2018S1B][] [2019SB2][] [@Laplace] [@MLE] [@MLE]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2015S4B
[2018S3B][] [2019SB3][] [@SPower] [@power] [@UMP]

Suppose $Y$ is a random variable (sample size = 1) from a population with density function
$f(y|\theta)=\begin{cases}\theta y^{\theta-1}& 0<x<1,\theta>0\\0& o.w.\end{cases}$

(a) Sketch the power function of the test of the rejection: $Y>0.5$.

(b) Based on the single observation $Y$ , find the uniformly most powerful test of size $\alpha$ for testing $H_0:\theta=1$ against $H_A :\theta>1$.

### 2015S5B
[2019SA4][] [@FishI] [@CI]

Let $X_1,X_2,..,X_{5}$ denote a random sample size n = 5 from a continuous distribution with cdf $F(.)$ with median $\theta$. Let $S(\theta)=$ the number of $fX0is > \theta$. We can express this as 
$S(\theta)=\sum_{i=1}^5I(X_i>\theta)$;
where I(.) is an indicator variable.

(a) State explicitly the distribution of $S(\theta)$.

(b) Sketch the graph of $S(\theta)$ as a function of $\theta$. Then describe the graph in words.

(c) Find a confidence interval for $\theta$ with confidence coefficient close to 0.95.


## 2015F 

### 2015F1
[@Unif] [@suff] [@UMVUE]

Let $X_1,X_2,..,X_{n}$ be iid continuous uniform r.v.'s over $[0,\theta]$, where $0 <\theta<\infty$. Let $Y_{n}=\max\{X_1,X_2,..,X_{n}\}$

(a) (5 pts) Find $E[Y_n]$.

(b) (5 pts) Find $Var[Y_n]$.

(c) (5 pts) Show that $Y_n$ is a sufficient statistic for $\theta$.

(d) (5 pts) Assuming $Y_n$ is a complete sufficient statistic for $\theta$, find the UMVUE of $\theta$.

(e) (5 pts) Assuming $Y_n$ is a complete sufficient statistic for $\theta$, find the UMVUE of $\theta^2$.

### 2015F2
[@Expo]

Suppose $X_1$ and $X_2$ are iid exponential with parameter = 1.

(a) (5 pts) Find the pdf of $Y_1=X_1/(X_1+X_2)$.

(b) (5 pts) Find the pdf of $Y_2=X_1+X_2$.

(c) (5 pts) Are $Y_1$ and $Y_2$ independent?

### 2015F3
[@Unif] [@LimD]

Let $X_1,X_2,..,X_{n}$ be iid $uniform[0,1]$ rv's. Let $0\le Y_1\le Y_2\le..\le Y_n\le1$ be the
corresponding order statistics.

(a) (5 pts) Find the pdf $g_k(y_k)$ of $Y_k$.

(b) (5 pts) Find $E[Y_k]$.

(c) (5 pts) Find $Var[Y_k]$.

(d) (5 pts) What is the limiting distribution of $W_1=nY_1$?

(e) (5 pts) What is the limiting distribution of $W_n=n(1-Y_n)$?

### 2015F4
[@Beta] [@HypoT]

Let X1 be a random sample of size n = 1 from the Beta distribution with pdf
$f(x|\theta)=\begin{cases}\frac{\Gamma(2\theta)}{\Gamma(\theta)\Gamma(\theta)}x^{\theta-1}(1-x)^{\theta-1}&0<x<1\\0&o.w.\end{cases}$

Suppose a researcher is interested in testing $H_0:\theta= 1$ against $H_1:\theta=2$. The researcher decides to reject $H_0$ in favor of $H_1$ if $X_1<2/3$.

(a) (5 pts) Find the size of the test

(b) (5 pts) Compute the power of the test at $\theta=2$.

### 2015F5
[@Expo] [@LRT] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample from the exponential distribution with
mean $\theta_1$ and let $Y_1,Y_2,..,Y_{n}$ be an independent random sample from another exponential distribution with mean $\theta_2$. Find the likelihood ratio test for testing $H0:\theta_1=\theta_2$ vs $H_a:\theta_1\neq\theta_2$

## 2016S

Crain, Kim

### 2016S1
[@Pois] [@MVUE]

Let $X_1,X_2,..,X_n$ be iid (independent, identically distributed) Poisson random variables with parameter $\lambda>0$.

(a) Find a complete sufficient statistic for $\lambda$.

(b) Find the MVUE (Minimum Variance Unbiased Estimator) of $\lambda$.

(c) Find the MVUE of $\lambda^2$:

(d) Find the MVUE of $e^{-\lambda}$.

(e) Find the MVUE of $P(X_i=1)=\lambda^1e^{-\lambda}/1!$:

(f) Find the MVUE of $P(X_i=k)=\lambda^ke^{-\lambda}/k!$:

### 2016S2
[2017FB1][] [@Bino] [@MVUE]

Let $Y$ be $Binomial(n,p)$, with $n$ known and $p$ unknown. Among functions $u(Y)$ of $Y$,

(a) What is the MVUE of $p$?

(b) What is the MVUE of $p^2$?

(c) What is the MVUE of $pq=p(1-p)$?

(d) What is the MVUE of $P(Y=k)=\binom{n}{k}p^k(1-p)^{n-k}$?


### 2016S3
[2015S3A][] [@SNorm] [@mean]

Let $Z$ be $N(0,1)$. Let $\Phi(z)=\int_{-\infty}^z=\phi(x)dx$, where $\phi(x) = \frac1{\sqrt{2\pi}}e^{−x^2/2},-\infty<x<\infty$, where $\phi(x)$ is the standard normal pdf, and $\Phi(z)$ is the standard normal cdf.

(a) Find $E[\Phi(Z)]$.

(b) Find $E[\Phi^{2}(Z)]$.

(c) Find $E[n\Phi^{n-1}(Z)]$.

(d) Find $E[Z^4]$.

(e) Find $E[Z^5]$

### 2016S4
[2016F7][] [2017FB4][] [2018FB2][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$.

(c) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

(d) (2 pts) What is the asymptotic distribution of $\hat\theta$?

### 2016S5
[2015S1Ab][] [2007F3A][] [2008S2A][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $X$ and $Y$ have the following joint pdf:
$f(x,y) =\begin{cases}6(y-x)& 0<x<y<1\\0& o.w.\end{cases}$
Define $Z=(X+Y)=2$ and $W=Y$, respectively.

(a) Find the joint pdf of Z and W.

(b) Find the marginal pdf of Z.

## 2016F

Fountain, Ian Dinwoodie

### 2016F1
[@Norm]

Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$.
Let $S_k^2=\frac1k\sum_{i=1}^n(X_i-\bar X)^2$ be an estimator of $\sigma^2$. Find the value of $k$ that minimizes the mean squared error of the estimator.

### 2016F2
[@MGF]

The moment generating function of a particular random variable is $M_X(t)=\frac{e^t}{4-3e^t}$. Find the coefficient of variation ($CV=\sigma/\mu$) of this distribution.

### 2016F3

If X is a random variable such that $E[X]=2$ and $E[X^2]=13$, determine a lower bound for the probability $P(-4<X<8)$. (Hint: Use a famous inequality.)

### 2016F4
[@Unif] [@CDF] [@PDF]

Let $Y_1$ and $Y_2$ be a random sample of size 2 from $Uniform(0,1)$. Find the cumulative distribution and probability density functions of $U=Y_1+Y_2$.

### 2016F5

Let $Y_n$ be the $n^{th}$ order statistic of a random sample of size n from the normal distribution $N(\theta,\sigma^2)$. Prove that $Y_n-\bar Y$ and $\bar Y$ are independent.

562-2

### 2016F6
[@Expo] [@BayesE]

Suppose that $X_1,X_2,..,X_n$ i.i.d. $Exponential(\theta)$, i.e. $f(x;\theta)=\theta e^{-\theta x},x>0$. Also assume that the
prior distribution of is $h(\theta)=\lambda e^{-\lambda\theta},\theta>0$. Find the Bayes estimator of $\theta$, assuming squared error loss.

### 2016F7
[2016S4][] [2017FB4][] [2018FB2][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from the following distribution:

$f(x;\theta)=(\theta+1)x^\theta,\ 0\le x\le 1$

where $\theta>-1$ is an unknown parameter.

(a) Find the method of moments estimator for $\theta$.


(b) Find the maximum likelihood estimator for $\theta$.

(c) Determine if your MLE is unbiased.

(d) Find the asmptotic variance of your MLE in part (b), as $n\to\infty$.

(e) Find the Cramer-Rao lower bound on the variance of an unbiased estimator of $\theta$.

(f) Identify the sufficient statistic for $\theta$.

(g) Suppose you've taken a sample of size $n=10$. Determine the @UMP test of the null hypothesis $\theta=.5$ vs. the alternative $\theta>0.5$.


### 2016F8
[2015S1Ab][] [2007F3A][] [2008S2A][] [2016S5][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}c(1-y_2)&0\le y1\le y2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c.

(b) Find the marginal density functions of $Y_1$ and $Y_2$.

(c) Find $P(Y_2\le1/2|Y_1\le3/4)$



## 2017S

Ian Dinwoodie, Robert Fountain

### 2017S1

Let $\Theta$ be a real-valued random variable with density $f\Theta(\theta) = \frac1{\sqrt{2\pi}}e^{−\theta^2/2},\theta\in\mathbb R$, and let Y have
conditional density $f(y|\theta) = \frac1{\sqrt{2\pi}}e^{−(y-\theta)^2/2},y\in\mathbb R$. Determine

(a) the conditional density of $\Theta$ given $Y = y$, (2 pts)

(b) the marginal density of $Y$. (3 pts)

### 2017S2

If $Z_1,Z_2$ are independent standard normal random variables, find the density of $Z_1/Z_2$.


### 2017S3

Suppose $X_1,X_2,..,X_n$ is a random sample of $Exp(1)$ @Expo random variables. Find the moment generating function $M(t)=E(e^{tX_{(1)}}), t\in\mathbb R$, where $X_{(1)}$ is the minimum. (5 pts)

### 2017S4

Let $X=e^Z$ be a lognormal random variable, $Z\sim N(0,1)$. Find its skewness $E(X − μ)^3/\sigma^3$.

### 2017S5

Suppose $X_1,X_2,..,X_n$ is a random sample of $N(\mu,\sigma^2)$ random variables.

(a) Find the expectation of the MLE for $\sigma^2$. (3 pts)

(b) Compute the correlation of $\bar X$ and $X_n$. (2 pts)

### 2017S6
[@UMP]

Let $X_1,X_2,X_3,...$ be i.i.d. $Poisson(\lambda)$ random variables. Find a UMP (uniformly most powerful) test of $H_0:\lambda\le1$ versus $H_1:\lambda>1$ at a level $\alpha$ near .05. (5 pts)

### 2017S7

Suppose that $(P,X)$ is a pair of random variables with $P\sim Beta(1/2, 1/2)$ and then $X_{|P=p}\sim bin(n,p)$. Find the variance of X.

### 2017S8
[@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample of $N(\mu,\sigma^2)$ random variables. Find the Cramer-Rao
lower bound  on the variance of an unbiased estimator for $\sigma^2$. (Assume $\mu$ is known.)


## 2017F

Kim, Kochar

### 2017FA1
[2018S3A][]

Suppose Xenophon and Yves meet for lunch, and Xenophon arrives at time X
uniformly from 1 to 2 P.M., and Yves arrives independently at time Y with the same distribution. Find the distribution of $|Y-X|$ and its expectation, that is, the expected waiting time of either party.



### 2017FA2
[@Expo]

Let $X_1,X_2,..,X_n$ be i.i.d. $Exp(\lambda)$ random variables with rate parameter $\lambda$ and density
$f(x)=\lambda e^{-\lambda x}, x>0$, with $\sigma^2=1/\lambda^2$. We are thinking about using the estimator $\bar X^2$ for the variance. Find the limiting distribution of
$\sqrt{n}(\bar X_n^2-1/\lambda^2)$

Let  be a random sample from $Poisson(\mu)$ @Pois. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

### 2017FA3
[@Expo]

If X and Y are independent $Exp(1)$ random variables, and the density of the ratio $X/(X+Y)$.

### 2017FA4

Let $F$ be the cdf of an exponential random variable with median 10 and let $G$ be
that of an independent exponential random variable Y with median 5. Find the distribution
of $V=F(X)+G(Y)$.

### 2017FB1
[2016S2][] @Bino

Let $Y$ be $Binomial(n,p)$, with $n$ known and $p$ unknown. Among functions $u(Y)$ of $Y$,

(a) What is the MVUE of $p$?

(b) What is the MVUE of $p^2$?

(c) What is the MVUE of $pq=p(1-p)$?

(d) What is the MVUE of $P(Y=k)=\binom{n}{k}p^k(1-p)^{n-k}$?

### 2017FB2
[2014F5B][] [@Expo]

Let $X_1,X_2,..,X_{10}$ be a random sample from an exponential distribution with location
parameter $\theta$ with pdf
$f(x;\theta)=\begin{cases}e^{-(x-\theta)}& \theta<x<\infty\\0& e.w.\end{cases}$

where $-\infty<\theta<\infty$is an unknown parameter. For testing the null hypothesis $H_0:\theta=0$
vs the alterative $H_1:\theta>0$, a reasonable test is to reject the null hypothesis if $X_{(1)}=\min\{X_1,X_2,..,X_{10}\}\ge C$. Find $C$ so that the size of the test is 0.05. Also find the power of this test at $\theta=1$. Is this test unbiased?

### 2017FB3

Let $X_1,X_2,..,X_{m}$ be a random sample of size m from $N(\theta,1)$ distribution and
let $Y_1,..,Y_m$ be an independent random sample of size $m$ from $N(3\theta,1)$.

(a) Show that the joint distribution of X's and Y's has @MLR (monotone likelihood ratio) property.

(b) Find the @UMP test of size $\alpha$ for testing $H_0:\theta\le0$ vs $H_1:\theta>0$.

(c) Find an expression of the power function of the @UMP test.

### 2017FB4
[2016S4][] [2016F7][] [2018FB2][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) What is the asymptotic distribution of $\hat\theta$?

(c) (2 pts) Using $\hat\theta$, find an unbiased estimator of $\theta$.

(d) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.



## 2018S

Kochar, Kim

### 2018S1A

An electric device has lifetime denoted by $T$. The device has value $V=5$ if it fails before time t = 3; otherwise, it has value $V = 2T$. Find the cdf of $V$ , if $T$ has pdf

$f_T(t)=\frac1{1.5}e^{-\frac1{1.5}t}, t>0$

### 2018S2A

Let $X$ have pdf

$fX(x)=\frac29(x+1), -1\le x\le2$

Find the pdf of $Y=X^2$.

### 2018S3A
[2017FA1][]

Suppose Xenophon and Yves meet for lunch, and Xenophon arrives at time X
uniformly from 1 to 2 P.M., and Yves arrives independently at time Y with the same distribution. Find the distribution of $|Y-X|$ and its expectation, that is, the expected waiting time of either party.

### 2018S4A

Let $X\sim N(\mu,\sigma^2)$ and $Y\sim N(\gamma,\sigma^2)$. Suppose that X and Y are independent. Deine $U=X+Y$ and $V=X-Y$.

(a) Show that U and V are independent.

(b) Find the distribution of each of them.

### 2018S1B
[2007F4B][] [2019SB2][] [@Laplace]

Let $X_1,X_2,..,X_1$ be a random sample of size 11 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2018S2B
[2015S2B][] [@FishI]

Let $X_1,X_2,..,X_n$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 90th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound @CRLB on the variance of any unbiased estimator of $g(\theta)$.

### 2018S3B
[2015S4B][] [2019SB3][] [@SPower] [@power] [@UMP]

Let $X_1,X_2,..,X_n$ be a random sample from a a distribution with pdf

$f(x;\theta)=\begin{cases}\theta x^{\theta-1}& 0<x<1\\0& o.w.\end{cases}$

$H_0:\theta=1\quad H_1 :\theta>1$

Derive the UMP test of size $\alpha$ and obtain the null distribution of your test statistic.

### 2018S4B

The life time of an electronic component has exponential distribution @Expo with mean $\mu$.
10 such components are put on test at the same time and the experiment is terminated when all of them fail and the times of their failure, $X_1,X_2,..,X_n$ are noted. Based on this information,
derive the likelihood ratio test @LRT at the level $\alpha= 05$ of the null hypothesis $H_0:\mu=5$ against the alternative $H_1:\mu\neq5$. Also find an expression for the @power function of this test.

$T=\sim x_i$, $\frac{2T}{\theta}\sim\chi^2_{2n}$

reject $T>t_0$ two-side rejection region.

## 2018F

Kochar, Bruno

### 2018FA1
[2015S1Ab][] [2007F3A][] [2008S2A][] [2016S5][] [2016F8][] [2019SA1 ][] [@joint] [@marg]

Suppose $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}C&0\le y1\le y2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c.

(b) Find the marginal density functions of $Y_1$ and $Y_2$ and check whether they are independent.

(c) Find $E[Y_1+Y_2]$

(d) Find $P(Y_1\le1/3|Y_1\le3/4)$

### 2018FA2

Let $X_1,X_2,..,X_n$ be a random sample from an  exponential distribution [@Expo] with mean 5.

$X_{(r)}=(n-j):(n-1)$

(a) Find the @CDF of the sample range.

pdf $X_{(n)}-X_{(1)}$

CDF $=\sum pdf$

(b) Find the expected value [@mean]of the sample range. @5.4.5

$E[X_{(r)}]=\sum_{j=1}^{n-1}\frac1{(n-1)-j+1}$

### 2018FA3

(a) (5 pts) In the daily production of a certain type of rope, the number of defects per foot, $X$ is assumed to have a Poisson distribution [@Pois] with mean $\lambda=3$. The profit per foot of the rope sold is given by
$P=30-3X-X^2$
Find the expected profit per foot.

$E[P]=30-3EX-EX^2$


(b) (5 pts) Suppose that $X$ is distributed as $U(0,1)$ and that $Y$ is a random variable with
$E(Y|X=x)=\alpha+\beta x^2$
Find $E[Y]$.

$E[Y]=E(E[Y|X])$

### 2018FA4

Let $X_1,X_2,..,X_n$ be a random sample from $Poisson(\mu)$ @Pois. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

$\sqrt{n}(\bar X_n-\mu)\to n(0,\frac1{I(\mu)})$

Delta Method

$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})\to n(0,\frac{[(e^{-\mu})']^2}{I(\mu)})$

### 2018FB1
@LRT

(4+6 pts) Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution with mean 120 and unknown variance $\sigma^2$. Derive the likelihood ratio test for testing the null hypothesis $H_0:\sigma^2=4$ against the alternative $H_1:\sigma^2\neq4$. Also fnd the exact as well as the asymptotic null distributions of your test statistic.


### 2018FB2
[2016S4][] [2016F7][] [2017FB4][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) What is the asymptotic distribution of $\hat\theta$?

(c) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$ of $\theta$.

(d) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

### 2018FB3
[@UMVUE]

Let $X_1,X_2,..,X_n$ be a random sample from $N(\mu,\sigma^2)$ distribution. Find a lower bound on the variance of any unbiased estimator of the 95th percentile of this distribution based on the Information Inequality. Also compare this bound to the variance of the uniformly minimum variance unbiased estimator .

$\hat\tau=\bar x+1.645s$

### 2018FB4
[@Unif] [@HypoT]
Let $X_1$ and $X_2$ be two independent random variables each having uniform distribution
on the interval $(\theta,\theta+1)$. For testing $H_0:\theta=0$ against $H_a:\theta> 0$, we have two competing tests :

1. Test 1 : Reject $H_0 if X_1>0.95$

2. Test 2 : Reject $H_0 if X_1+X_2>c$.

Find the value of c so that the Test 2 has the same value of Type I error probability as Test 1.

$\alpha=P(x_1>0.95)=P(x_1+x_2>c)$

find pdf of $x_1+x_2$ (example of textbook)


## 2019S

Kochar, Bruno

### 2019SA1 
[2015S1Ab][] [2007F3A][] [2008S2A][] [2016S5][] [2016F8][] [2018FA1][] [@joint] [@marg]

Suppose X and Y have the joint pdf

$f(x;y) =\begin{cases}Cxy& 0\le x\le 2, 0\le y\le 2, x + y\le 2\\0& o.w.\end{cases}$

(a) (4 pts) Find the value of C;

(b) (4 pts) Find the marginal densities of X and Y and check whether they are independent or not;

(c) (2 pts) Compute $P(X<Y)$;

### 2019SA2

[2007F2A][] [2015S2A][] [@Basu] 

Suppose $Y_1$ and $Y_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:

$f(y)=\begin{cases}\theta e^{-\theta y}& y\ge0\\0& o.w.\end{cases}$

with $\theta>0$.

Find the p.d.f. of $X=Y_1-Y_2$.

### 2019SA3 

Let $\theta$ be Beta distributed, $\theta\sim Beta(1,1)$. Let $N_1$ be Binomial given $\theta$, that is $N_1\sim Bin(n,\theta)$ given $\theta$.

(a) (4 pts) Compute $p(\theta|N_1=n_1)$ and $E[\theta|N_1=n_1]$

(b) (6 pts) Compute $p(N_1=n_1)$ for $n_1=0...n$.

### 2019SA4 
[2015S5B][]

Let $X_1,X_2,..,X_{10}$ be a random sample from a Poisson distribution with mean .

(a) (4 Ppts) Use the method of moment generating functions to nd the distribution of $S_{10}=\sum^{10}_{i=1} X_i$.

(b) (6 pts) Let $S_4=\sum_{i=1}^4X_i$ Find the conditional distribution of $S_4$ given $S_{10}=s$, for $s>0$. This distribution belongs to a family of distributions that you know. Which family? which parameters?

### 2019SB1

Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution $N(\mu,\sigma^2=25)$. Reject $H_0:\mu=50$ and accept $H_1:\mu=55$ if $\bar X_n\ge c$. Find the two equations in n and c that you would solve to get $P(\bar X_n\ge c|\mu)=K(\mu)$ to be equal to $K(50) = 0:05$ and $K(55) = 0.90$.
Solve these two equations. Round up if n is not an integer. Hint: $z_{.05}=1.645$ and $z_{.1}=1.28$

### 2019SB2 
[2007F4B][] [2015S3B][] [2018S1B][] [@Laplace] [@MLE]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2019SB3 
[2015S4B][] [2018S3B][] [@SPower] [@power] [@UMP]

Suppose $X_1,X_2,..,X_n$ is a random sample from a distribution with pdf

$f(x,\theta)=\begin{cases}\theta x^{\theta-1}& 0<x<1\\0& o.w.\end{cases}$

Suppose that the value of  is unknown and it is desired to test the following hypotheses :

$H_0:\theta=1\quad H_1 :\theta>1$

Derive the UMP test of size $\alpha$ and obtain the null distribution of your test statistic.

### 2019SB4 
[2016S4][] [2016F7][] [2017FB4][] [2018FB2][]


Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$ of $\theta$.

(c) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

(d) (2 pts) What is the asymptotic distribution of $\hat\theta$?



# References