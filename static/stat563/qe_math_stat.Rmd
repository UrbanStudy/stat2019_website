---
title: ""
subtitle: ""
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true

bibliography: bib_math_stat.bib
---


```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```

# 

## 2000

## 2003S

## 2003F

## 2004F

## 2005F
Kochar

## 2007F

### 2007F1A
[2008S4A] [2013FB1] [@Norm] [@MLE] [@suff] [@consi] [@unbias]

Let $Y_1,Y_2,..,Y_{n}$ be a random sample from $N(\mu,\sigma^2)$ distribution and let $X_1,X_2,..,X_{m}$ be an independent random sample from $N(2\mu,\sigma^2)$ distribution.

(a) Find minimal sufficient statistics for $(\mu,\sigma^2)$

(b) Find maximum likelihood estimators of $\mu$ and $\sigma^2$

(c) Show that
$\hat\sigma^2=\frac{\sum_{i=1}^m(X_i-\bar X)^2+\sum_{j=1}^n(Y_j-\bar Y)^2}{m+n-2}$
is unbiased and consistent for estimating $\sigma^2$

### 2007F2A
[2008S5A][] [2009FA1][] [2014F4A][] [2015S2A][] [2019SA2][]

Suppose $Y_1$ and $Y_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:

$f(x)=\begin{cases}10e^{-10x}& x>0\\0& o.w.\end{cases}$

Find the p.d.f. of $X=Y_1-Y_2$.

### 2007F3A
[2008S2A][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Suppose $Y_1$ and $Y_2$ have the joint pdf
$f(y_1,y_2)=\begin{cases}2&0\le y1\le y2\le 1\\0& o.w.\end{cases}$

(a) Find the marginal density functions of $Y_1$ and $Y_2$ and check whether they are independent.

(b) Find $E[Y_1+Y_2]$

(c) Find $P(Y_1\le3/4|Y_2>1/3)$

### 2007F4A
[2015S4A][] [@CDF]

(a) Let $X$ be a continuous type random variable with cumulative distribution function $F(x)$. Find the distribution of the random variable $Y=\ln(1-F(X))$:

(b) Prove that for any $y\ge c$, the function $G_c(y)=P[X\le y|X\ge c]$ has the properties of a distribution function.

### 2007F5A
[2013FB2][] [2014F1B][] [2015S1B][] [@CDF] [@MLE]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find $\hat\theta$, the mle of $\theta$.

(b) Find $E[\hat\theta]$.

(c) Prove that $\hat\theta$ is consistent for $\theta$.


### 2007F4B
[2013FB4][] [2015S3B][] [2018S1B][] [2019SB2][] [@Laplace] [@MLE]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2007F5B
[2015S4B][] [2018S3B][] [2019SB3][] [@SPower] [@power] [@UMP]

Suppose $X_1,X_2,..,X_n$ is a random sample from a distribution with pdf

$f(x,\theta)=\begin{cases}\theta x^{\theta-1}& 0<x<1\\0& o.w.\end{cases}$

Suppose that the value of $\theta$ is unknown and it is desired to test the following hypotheses :

$H_0:\theta=1\quad H_1 :\theta>1$

Derive the UMP test of size $\alpha$ and obtain the null distribution of your test statistic.

## 2008S

### 2008S1A

A box contains 2 red balls, 2 white balls, and 3 blue balls. If 5 balls are selected at random without replacement, what is the probability that only one color is missing from the selection?

### 2008S2A
[2007F3A][] [2009FA2][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}c(1-y_2)&0\le y_1\le y_2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c.

(b) Find the marginal density functions of $Y_1$ and $Y_2$.

(c) Find $P(Y_2\le1/2|Y_1\le3/4)$

### 2008S3A
[2008F1][] [2016F4][] [@Unif] [@CDF] [@PDF]

Let $(Y_1,Y_2)$ denote a random sample of size $n=2$ from the uniform distribution on the interval $(0, 1)$. Find the probability density and cumulative distribution functions of $U=Y_1+Y_2$..

### 2008S4A
[2007F1A][] [2013FB1][] [@Norm] [@unbias] [@consi]

Let $Y_1,Y_2,..,Y_{n}$ be a random sample of size $n$ from a normal population with mean $\mu$ and variance $\sigma^2$. Assuming $n=2k$ for some integer $k$, one possible estimator for $\sigma^2$ is given by:
$\hat\sigma^2=\frac1{2k}\sum_{i=1}^k(Y_{2i}-Y_{2i-1})^2$

(a) Show that $\hat\sigma^2$ is an unbiased estimator for $\sigma^2$

(b) Show that $\hat\sigma^2$ is a consistent estimator for $\sigma^2$

### 2008S5A
[2007F2A][] [2009FA1][] [2014F4A][] [2015S2A][] [2019SA2][]

The lifetime (in hours) Y of an electronic component is a random variable with density function
$f(y)=\begin{cases}\frac1{300}e^{-\frac1{300}y}& y>0\\0& o.w.\end{cases}$

(a) What is the probability that a randomly selected component will operate for at least 300 hours?

(b) Five of these components operate independently in a piece of equipment. The equipment fails if at least three of the components fail.

Find the probability that the equipment will operate for at least 300 hours without failure?

### 2008S1B
[2009FA4][] [2015F1][] [@Unif] [@mean] [@Var] [@suff] [@UMVUE] 
Let $X_1,X_2,..,X_{n}$ be a random sample of size $n$ from a uniform distribution over the interval $[-\theta/2,\theta/2], \theta>0$ being unknown.

(a) Prove tbat $T=\max_{1\le i\le n}|X_{i}|$ is complete and sufficient for $\theta$.

(b) Find the UMVU estimator of $\theta$.

### 2008S2B
[2014F2B][] [@Pois] [@FishI]

Let $X_1,X_2,..,X_{n}$ be a random sample from Poisson distribution with parameter $\lambda(>0)$.

(a) Find the Fisher's information in the sample about the parameter $\lambda$.

(b) Suppose we want to estimate $P[X_1=0]=e^{-\lambda}$. Find a lower bound on the variance of any unbiased estimator of this parametric function.

### 2008S3B
[@consi]

Let $X_1,X_2,..,X_{n}$ be a random sample from a distribution with probability density function
$f_{\theta_1}(x)=\begin{cases}\frac1{\theta_1} e^{-\frac{x}{\theta_1}}& x>0\\0& o.w.\end{cases}$
and $Y_1,Y_2,..,Y_{n}$ an independent random sample from
$f_{\theta_2}(x)=\begin{cases}\frac1{\theta_2} e^{-\frac{x}{\theta_2}}& x>0\\0& o.w.\end{cases}$

(a) Find $p_{\theta_1,\theta_2}= P[X_1\le Y_1]$.

(b) Find the MLE, $\hat p_n$, of $p_{\theta_1,\theta_2}= P[X_1\le Y_1]$.

(c) Show that $\hat p_n$ is a consistent estimator of $p_{\theta_1,\theta_2}$.

### 2008S4B
[2014SB2][] [@Unif]

Let $X_1,X_2,..,X_{10}$ be independent random variables such that $X$, has $U(0,i\theta)$ distribution for $i= 1,2,..,10$. Based on these 10 observations, find the maximum likelihood estimator of $\theta$ and find its bias.

### 2008S5B
[2017FB3][] [@Norm] [@MLR] [@UMP] [@power] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample of size m from $N(\theta,1)$ distribution and
let $Y_1,..,Y_m$ be an independent random sample of size $m$ from $N(3\theta,1)$.

(a) Show that the joint distribution of X's and Y's has @MLR (monotone likelihood ratio) property.

(b) Find the UMP test of size $\alpha$ for testing $H_0:\theta\le0$ vs $H_1:\theta>0$.

(c) Find an expression of the power function of the UMP test.


## 2008F
Fountain

### 2008F1
[2008S3A][] [2016F4][] [@Unif] [@CDF] [@PDF]

Let $Y_1$ and $Y_2$ be a random sample of size 2 from $Uniform(0,1)$. Find the cumulative distribution and probability density functions of $U=Y_1+Y_2$.

### 2008F2
[2010SA1][] [2014F2A][]

Only 5 in 1000 adults are afflicted with a rare disease for which a diagnostic test has been developed. The test is such that when an individual actually has the disease, a positive result will occur 99% of the time, whereas an individual without the disease will show a positive result only 2% of the time. If a randomly selected individual is tested and the result is positive, what is the probability that the individual has the disease? A man committed a suicide in a week after learning from his doctor that he has a terminal cancer. What do you think of his reaction based on your answer to this problem?

### 2008F3
[2016F3][] [@Cheb]

If X is a random variable such that $E[X]=3$ and $E[X^2]=13$, determine a lower bound for
the probability $P(-2<X<8)$. (Hint: Use a famous inequality.)

### 2008F4
[@limD]

Let $Y_1$ be the minimum of a random sample of size n from a distribution that has p.d.f.
$f(x)=e^{-(x-\theta)},\theta<x<\infty$, zero elsewhere. Let $Z_n= n(Y_1-\theta)$. Determine the limiting distribution of $Z_n$. (Hint: Determine the p.d.f. of Y, and then apply the change of variable technique.)

### 2008F5
[2009SB1][] [2009FB4][] [2016S4][] [2016F7][] [2017FB4][] [2018FB2][] [2019SB4][] [@MOM] [@MLE] [@MSE] [@CRLB]

Let $X_1,X_2,..,X_n$$\sim$i.i.d.$f(x;\theta)=\theta(x+1)^{-\theta-1},\ x>0,\theta>2$

a. Find $\hat\theta_{MOM}$, the method of moments estimator of $\theta$.

b. Find $\hat\theta_{MLE}$, the maximum likelihood estimator of $\theta$.

c. Find the MSE (mean squared error) of $\hat\theta_{MLE}$.

d. Using $\hat\theta_{MLE}$, create an unbiased estimator $\hat\theta_{U}$.

e. Find the efficiency of $\hat\theta_{U}$.

f. Construct the most powerful test of $H_0:\theta=3$ vs. $H_1:\theta=4$.

### 2008F6
[2016F5][] [@Basu]

Let $Y_n$ be the $n^{th}$ order statistic of a random sample of size n from the normal distribution $N(\theta,\sigma^2)$. Prove that $Y_n-\bar Y$ and $\bar Y$ are independent.

562-2

### 2008F7
[2016F6][] [@Expo] [@BayesE]

Suppose that $X_1,X_2,..,X_n$ i.i.d. $Exponential(\theta)$, i.e. $f(x;\theta)=\theta e^{-\theta x},x>0$. Also assume that the prior distribution of $\theta$ is $h(\theta)=\lambda e^{-\lambda\theta},\theta>0$. Find the Bayes estimator of $\theta$, assuming squared error loss.

## 2009S
unkown, Fountain

### 2009SA1
[@joint] [@marg]

Suppose random variables X and Y have a joint probability mass function
$p(x,y)=\begin{cases}\frac{x+y+1}{30}& x,y=0,1,2,..,x+y\le3\\0& o.w.\end{cases}$
Determine the marginal probability mass function of Y.

### 2009SA2
[@Pois]

Suppose a random variable $X$ has a probability mass function $p(x)=\frac{e^{-\mu}\mu^x}{x!},x=0,1,2,..$,zero, elsewhere. Find the values of $\mu$, so that $x=1$ is the unique mode.

### 2009SA3
[@Pois]

Let $X_1,X_2,..,X_n$ be the independent $Poisson(m_i)$ random variables. Show that $Y=\sum_{i=1}^n X_i$ has $Poisson(\sum_{i=1}^n m_i)$.

### 2009SA4
[@Cor] [@Cheb]

Let $\sigma_1^2=\sigma_2^2=\sigma^2$ be the common variance, $\rho$ the correlation coefficient, $\mu_1$ and $\mu_2$ the means of $X_1$ and $X_2$ , respectively. Show that

$P[|(X_1-\mu_1)+(X_2-\mu_2)|\ge k\sigma]\le\frac{2(1+p)}{k^2}$

### 2009SA5
[@Expo]

Let Xn have a probability density function
$f(x;n)=\begin{cases}ne^{-nx}& 0<x<\infty\\0& o.w.\end{cases}$
Find the limiting distribution of $Y_n=X_n/n$.

### 2009SB1
[2008F5][] [2009FB4][] [2016S4][] [2016F7][] [2017FB4][] [2018FB2][] [2019SB4][] [@MOM] [@MLE] [@MSE] [@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample of size n from the following distribution:

$f(x;\theta)=(\theta+1)x^\theta,\ 0\le x\le 1,\theta>-1$

(a) Find $\hat\theta_{MOM}$, the method of moments estimator for $\theta$.

(b) Find $\hat\theta_{MLE}$, the maximum likelihood estimator for $\theta$.

(c) Using $\hat\theta_{MLE}$, create an unbiased estimator $\hat\theta_{U}$.

(d) Find the Cramer-Rao lower bound on the variance of an unbiased estimator of $\theta$.

(e) Construct the most powerful test of $H_0:\theta=0$ vs. $H_1:\theta=1$, showing as much detail as possible.

### 2009SB2
[@Norm] [@indep]
 
Let $X_1,X_2,..,X_5$ be a random sample of size 5 from the normal distribution $N(0,\sigma^2)$. Prove
that $R=(X_1^2+ X_2^2)/(X_1^2+ X_2^2+X_3^2+ X_4^2+X_5^2)$ and $D=X_1^2+ X_2^2+X_3^2+ X_4^2+X_5^2$ are independent.

### 2009SB3
[@Pois] [@Gamma] [@BayesE]

Suppose thfrt $X_1,X_2,..,X_{n}$ have i.i.d. $Poisson(\theta)$. Also assume that the prior distribution
of is e is $Gamma(\alpha,\beta)$. Find the Bayes estimator of $\theta$, assuming squared-error loss.

## 2009F

### 2009FA1
[2007F2A][] [2008S5A][] [2014F4A][] [2015S2A][] [2019SA2][]

The lifetime (in hours) Y of an electronic component is a random variable with density function
$f(y)=\begin{cases}\frac1{200}e^{-\frac1{200}y}& y>0\\0& o.w.\end{cases}$

(a) What is the probability that a randomly selected component will operate for at least 400 hours?

(b) What is the probability that the lifetime of a randomly selected component will exceed its mean lifetime by more than two standard deviations?

(c) Four of these components operate independently in a piece of equipment. The equipment fails if at least three of the components fail. Find the probability that the equipment will operate for at least 400 hours without failure?

### 2009FA2
[2007F3A][] [2008S2A][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Suppose $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}c&0\le y_1\le y_2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c.

(b) Find the marginal density functions of $Y_1$ and $Y_2$ and check whether they are independent.

(c) Find $P(Y_1\le1|Y_2>1)$

### 2009FA3
[2015S5B][] [2019SA4][] [@MOM] [@Pois]

Let $Y_1,Y_2,..,Y_{12}$ be a random sample from a Poisson distribution with mean $\lambda$.

(a) (4 Ppts) Use the method of moment generating functions to find the distribution of $S_{12}=\sum^{12}_{i=1} Y_i$.

(b) (6 pts) Let $S_4=\sum_{i=1}^4Y_i$ Find the conditional distribution of $S_4$ given $S_{12}=s$.


### 2009FA4
[2008S1B][] [2015F1][] [@Unif] [@PDF] [@mean] [@Var] [@consi]

Suppose $X_1,X_2,..,X_{n}$ is a random sample from a unform distribution over $[1,\theta]$, where $\theta>1$. Let $Y_{n}=\max\{X_1,X_2,..,X_{n}\}$

(a) (3 pts) Find the probability density-function of $Y_{n}$.

(b) (4 pts) Find the mean and the variance of $Y_{n}$.

(c) (3 pts) Examine whether $Y_{n}$ is a consistent estimator of $\theta$.


### 2009FB1
[2019SB1][]

Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution $N(\mu,\sigma^2=25)$. Reject $H_0:\mu=50$ and accept $H_1:\mu=55$ if $\bar X_n\ge c$. Find the two equations in n and c that you would solve to get $P(\bar X_n\ge c|\mu)=K(\mu)$ to be equal to $K(50)=0.05$ and $K(55)=0.90$.
Solve these two equations. Round up if n is not an integer. Hint: $z_{.05}=1.645$ and $z_{.1}=1.28$

### 2009FB2
[@MLE] [@LRT]

The Pareto distribution is a frequently used model in study of incomes and has the distribution
function
$F(x;\theta_1,\theta_2)=\begin{cases}1-(\theta_1/x)^{\theta_2}& \theta_1<x\\0& o.w.\end{cases}$
where $\theta_1>0$ and $\theta_2>0$.

(a) (4 pts) Let $X_1,X_2,..,X_n$ be a random sample from this distribution. Find the MLEs of $\theta_1$ and $\theta_2$.

(b) (3 pts) Find the likelihood ratio test for testing $H_0:\theta_1=1$ against $H_1:\theta_1\neq1$.

(c) (3 pts) Using $\alpha=.05$, find out the critical value for your test. Hint: $\chi^2_{1,.025}= 5.024$;$\chi^2_{1,.05}= 3.841$; $\chi^2_{1,.975}=.001$; $\chi^2_{1,.95}=.004$; $\chi^2_{2,.025}=7.378$; $\chi^2_{2,.05}= 5.991$; $\chi^2_{2,.975}=.051$;$\chi^2_{2,.95}=.103$

### 2009FB3
[@UMVUE] [@Expo] [@indep]

Let $X_1,X_2$ denote a random sample of size $n=2$ from a distribution with pdf
$f(x;\theta)=\begin{cases}\frac1\theta e^{-\frac{x}\theta}& 0<x<\infty\\0& o.w.\end{cases}$
where $0<\theta<\infty$ is an unknown parameter.

(a) (5 pts) Show that $Y_1=X_1+X_2$ is independent of $X_1/X_2$.

(b) (5 pts) Find the UMVUE of $\theta^2$

### 2009FB4
[2008F5][] [2009SB1][] [2016S4][] [2016F7][] [2017FB4][] [2018FB2][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$ of $\theta$.

(c) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

(d) (2 pts) What is the asymptotic distribution of $\hat\theta$?


## 2010S

### 2010SA1
[2008F2][] [2014F2A][]

Only 1 in 1000 adults is afflicted with a rare disease for which a diagnostic test has been developed. The test is such that when an individual actually has the disease, a positive result will occur 99% of the time, whereas an individual without the disease will show a positive result only 2% of the time (false positive). If a randomly selected individual is tested and the result is positive, what is the probability that the individual has the disease?

### 2010SA2

Let $X_1$ and $X_2$ be a random sample of size 2 from the following pdf

$f(x,\beta)=\begin{cases} \frac1{2\beta^3}x^2e^{-x/\beta}& x\ge0\\0& o.w.\end{cases}$

(a) Compute the expected value of $X_1/X_2$

(b) Compute the variance of $X_1/X_2$

### 2010SA3
[2011S4][] [2018FA4][] [@Pois] [@LimD]

Let $X_1,X_2,..,X_n$ be a random sample from $Poisson(\mu)$. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

### 2010SA4
[2007F3A][] [2008S2A][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $X$ and $Y$ have the following joint pdf:
$f(x,y) =\begin{cases}6(y-x)& 0<x<y<1\\0& o.w.\end{cases}$
Define $Z=(X+Y)=2$ and $W=Y$, respectively.

(a) Find the joint pdf of Z and W.

(b) Find the marginal pdf of Z.

### 2010SB1
[2010FB1][] [2011S5][] [2013FB3][] [2015S2B][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_{20}$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 75th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.


### 2010SB2
[@power]

Let $f_1(x)=\begin{cases}1&0<x\le1\\0& o.w.\end{cases}$
and
$f_2(x)=\begin{cases}4x&0<x\le1/2\\4(1-x)& 1/2<x\le1\\0& o.w.\end{cases}$

Based on a single observation $X$, derive the most powerful level $\alpha=0.1$ test for testing $H_0:X\sim f_1$ against the alternative $H_2:X\sim f_2$. Also find the power of your test.

### 2010SB3
[2010FB3][] [@Norm] [@MLE]  [@UMVUE]

Let $X_1,X_2,..,X_n$ be a random sample from $N(1,\sigma^2)$ distribution.

(a) Find the MLE of $\sigma^2$

(b) Is it an unbiased estimator of $\sigma^2$? Justify your answer.

(c) Is it a UMVUE of $\sigma^2$? Justify your answer.

### 2010SB4
[2010FB4][] [2011S6][] [2015F5][] [@Expo] [@LRT] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample from the exponential distribution with mean $\theta_1$ and let $Y_1,Y_2,..,Y_{n}$ be an independent random sample from another exponential distribution with mean $\theta_2$. 

(a) Find the likelihood ratio test for testing $H0:\theta_1=\theta_2$ vs $H_a:\theta_1\neq\theta_2$

(b) Show that the test in (a) is equivalent to to an exact F test. (Hint : Transform $\sum X_i$; and
$\sum Y_i$ to $\chi^2$ random variables).

## 2010F

### 2010FA1
[2013FA2] [@Unif] [@mean] [@Cov] [@Cor]

Suppose $X$ is $uniform[0,1]$. Assume Y, given $X=x$, is $uniform[0,x]$ Find the joint pdf of $X$ and $Y$. Find the mean and variance of $X$ and $Y$. Find the covariance and correlation of $X$ and Y.

### 2010FA2 
[2013FA3] [@Unif] [@mean] [@LimD]

Let $X_1,X_2,..,X_{n}$ be iid uniform[0,1] random variables, and define $Y_1=\min{X_1,X_2,..,X_{n}}$. Find the cdf of $Y_1$. Suppose $W_1=nY_1$. Note that $0<Y_1<1$, but $0<W_1<n$. Find the limiting distribution of $W_1$ as $n\to\infty$. 


### 2010FA3
[@Norm] [@mean] [@Var]

Suppose $X$ is $N(\mu,\sigma^2)$. Define $Y=e^X$. Find the mean and variance of $Y$.

### 2010FA4
[@Pois] [@MGF]

Assume that $X_i$ is $Poisson(\mu_i),i=1,..,n$. If the $X_i$'s are independent, use moment generating
functions to show that $\sum_{i=1}^n X_i$ is also Poisson. Do you think $\sum_{i=1}^n iX_i$ is Poisson?

### 2010FB1
[2010SB1][] [2011S5][] [2013FB3][] [2015S2B][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_{20}$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 75th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2010FB2
[2015F4] [@Beta] [@HypoT] [@power]

Let X1 be a random sample of size $n=1$ from the Beta distribution with pdf
$f(x|\theta)=\begin{cases}\frac{\Gamma(2\theta)}{\Gamma(\theta)\Gamma(\theta)}x^{\theta-1}(1-x)^{\theta-1}&0<x<1\\0&o.w.\end{cases}$

Suppose a researcher is interested in testing $H_0:\theta=1$ against $H_1:\theta=2$. The researcher decides to reject $H_0$ in favor of $H_1$ if $X_1<2/3$.

(a) Find the size of the test

(b) Compute the power of the test at $\theta=2$.

### 2010FB3
[2010SB3][] [@Norm] [@MLE]  [@UMVUE]

Let $X_1,X_2,..,X_n$ be a random sample from $N(1,\sigma^2)$ distribution.

(a) Find the MLE of $\sigma^2$

(b) Is it an unbiased estimator of $\sigma^2$? Justify your answer.

(c) Is it a UMVUE of $\sigma^2$? Justify your answer.

### 2010FB4
[2010SB4][] [2011S6][] [2015F5][] [@Expo] [@LRT] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample from the exponential distribution with mean $\theta_1$ and let $Y_1,Y_2,..,Y_{n}$ be an independent random sample from another exponential distribution with mean $\theta_2$. 

(a) Find the likelihood ratio test for testing $H0:\theta_1=\theta_2$ vs $H_a:\theta_1\neq\theta_2$

(b) Show that the test in (a) is equivalent to to an exact F test. (Hint : Transform $\sum X_i$; and
$\sum Y_i$ to $\chi^2$ random variables).

## 2011S

### 2011S1
[@Cor] [@Cov] [@Var]

Let $U$ and $V$ be r.v.'s such that $Var(U+V)=30$ and $Var(U-V)=10$.

(a) Find $Cov(U,V)$.

(b) If additionally, we know $Var(U)=Var(V)$, find the correlation of $U$ and $V$.

### 2011S2
[@Unif]

Let $X_1,X_2,..,X_{n}$, be iid $Uniform[0,\theta]$ r.v. 's.

(a) Find an unbiased estimator of $\theta$.

(b) Finri the minimum variance unbiased estimator of $\theta$.

(c) Find an unbiased estimator of $\theta^2$

(d) Find the minimum variance unbiased estimator of $\theta^2$

### 2011S3 
[@Unif] [@Weib] [@trans] 

Let $f(x)=2xe^{-x^2},0<x<\infty$, and zero elsewhere.

(a) Show $f(x)$ is a probability density function.

(b) If $X$ has pdf $f(x)$, find $E(X)$.

(c) If $X$ has pdx $f(x)$, find $E(X^2)$.

### 2011S4
[2010SA3][] [2018FA4][] [@Pois] [@LimD]

Let $X_1,X_2,..,X_n$ be a random sample from $Poisson(\mu)$. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

### 2011S5
[2010SB1][] [2010FB1][] [2013FB3][] [2015S2B][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_{20}$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 75th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2011S6
[2010SB4][] [2010FB4][] [2015F5][] [@Expo] [@LRT] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample from the exponential distribution with mean $\theta_1$ and let $Y_1,Y_2,..,Y_{n}$ be an independent random sample from another exponential distribution with mean $\theta_2$. 

(a) Find the likelihood ratio test for testing $H0:\theta_1=\theta_2$ vs $H_a:\theta_1\neq\theta_2$

(b) Show that the test in (a) is equivalent to to an exact F test. (Hint : Transform $\sum X_i$; and
$\sum Y_i$ to $\chi^2$ random variables).

## 2011F

### 2011F1
[@Norm] [@mean]

Let $X$ be a $N(0,\sigma^2)$ random variable. Find $E(X^4)$.

### 2011F2
[@Gamma]

Let $(X,Y)$ have bivariate density $f(x,y)=\frac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}x^{\alpha-1}y^{\beta-1}(1-x-y)^{\gamma-1},0<x<1,0<yx<1,0<x+y<1$ for parameters $\alpha>0,\beta>0,\gamma>0$. Determine

(a) the conditional density of $Y$ given $X=.5$,

(b) the density of $Y/.5$ given $X=.5$,

(c) the marginal density of $X$.

### 2011F3
[@Unif] [@Weib] [@trans] 

Determine the transformation $g$ that will make $X=g(U)$ have the Weibull density $f(x)=2xe^{-x^2},x>0$, where $U$ is a $unifonn(0,1)$ random variable.

### 2011F4
[@Norm] [@t] [@MGF]

Suppose $X_1,X_2,..,X_{n}$ is a random sample of $N(\mu,\sigma^2)$ random variables. Find the moment generating function $M(t)=E(e^{tT}),t\in\mathbb R$, where $T=\frac{\bar X-\mu}{S/\sqrt{n}}$ is the usual t-statistic.

### 2011F5
[@Norm] [@Cor] [@Cov] [@Var]

Suppose $X_1,X_2,..,X_{n}$ is a random sample of $N(\mu,\sigma^2)$ random variables.

(a) Find the correlation of $\bar X$ and $S^2$, the sample mean and sample variance.

(b) Find the variance of $S^2$.

(c) Compute the covariance of $X_1$ and $\bar X$.

### 2011F6
[@Pois] [@UMP] [@HypoT]

Let $X_1,X_2,X_3$ be iid $Poisson(\lambda)$ random variables. Find a UMP (uniformly most powerful)
test of $H_0:\lambda\ge1$ versus $H_1:\lambda<1$ at a level $\alpha$ near .05.

### 2011F7
[@Pois] [@MLE] [@MSE]

Let $X_1,..,X_{n}$ be lid $Poisson(\lambda)$ random variables.

(a) Find the best unbiased estimator of $e^{-\lambda}$, the probability that $X=0$.

(b) Find the MLE (maximum likelihood estimator) for $e^{-\lambda}$.

(c) Compute the MSE (mean-squared error) for the MLE as a function of $\lambda$,

### 2011F8
[@Bino] [@CRLB] [@MLE]

Let $X$ have tbe binomial distribution $bin(n,p)$. Find the Cramer-Rao lower bound on the variance of an unbiased estimator for $p$, and compare it to the variance of the MLE for $p$.

## 2013S

### 2013S1
[@Norm]

Let $X_1,X_2,..,X_{n}$ be iid $N(\mu,\sigma^2)$ random variables. If we have convergence in distribution $\sqrt{n}(S^2-\sigma^2)\to N(0,2\sigma^4)$ for the sample variance $S^2$, use it to get a normal approximation for the distribution of $S$.

### 2013S2
[2014F3A][]

Let $(X,Y)$ have bivariate density $f(x,y)=e^{-x},0<y<x$. Determine

(a) the marginal density of $X$,

(b) the conditional density of $Y$ given $X=x$.

### 2013S3
[@Pois] [@MLE] [@MOM]

Let X have the $Poisson(\lambda)$ distribution. Find the Cramer-Rao lower bound on the variance of an unbiased estimator for $\lambda$ and compare it to the variance of the Method of Moments estimator for $\lambda$. 

### 2013S4

Find a transformation $g$ that will make $X=g(U)$ have the $\chi^2(2)$ density where $U$ is a $uniform(0,1)$ random variable (useful for the Box-Mueller method of simulating standard normal random variables). 

### 2013S5
[@Norm] [@MLE]

Suppose $X_1,X_2,..,X_{n}$ is a random sample of $N(\mu,\sigma^2)$ random variables. Find the
mean-squared error of the MLE for $\sigma^2$ and the mean-squared error of its best unbiased estimator. 

### 2013S6
[@Norm]

Suppose $X_1,X_2,..,X_{n}$ is a random sample of $N(\mu,\sigma^2)$ random variables.

(a) Find the exact distribution of $\bar X$.

(b) Compute the covariance of $X_1-\bar X$ and $X$.

### 2013S7
[@Bino] [@UMP] 

Let $X_1,X_2$ be two iid $bin(5,p)$ random variables. Find a UMP (uniformly most powerful) test of $H_0: p\le 0.5$ versus $H_1: p>.5$ at a level $\alpha$ near .01.

### 2013S8
[@Gamma] [@MLE]

Let $X_1,X_2,..,X_{n}$ be iid $Gamma(\alpha=1,\beta)$ random variables. Find the expectation of
the MLE $1/\bar X$ for the rate $\lambda=1/\beta$ and say whether it is greater than or less than $\lambda$


## 2013F

### 2013FA1

Assume an urn contains R red and B blue marbles. Marbles are drawn from the urn, one at a time and without replacement, until all the marbles have been drawn.

(a) What is the probability that the first marble drawn is red?

(b) What is the probability that the second marble is red?

(c) What is the probability that the last marble is red?

(d) What is the probability that the first ai:td last marbles are red?

### 2013FA2
[2010FA1] [@Unif] [@mean] 

Let $X$ be a $uniform[0,1]$ random variable. Let $Y$, given $X$, be $uniform[0,X]$.

(a) What are the mean and variance of $X$?

(b) What are the mean and variance of $Y$?

(c) What is the joint pdf $f(x,y)$ of $X$ and $Y$?


### 2013FA3
[2010FA2] [@Unif] [@mean] [@asym]

Suppose $U_1,U_2,..,U_n$ are iid $uniform[0,\theta]$ random variables, where $0<\theta<\infty$. Let $W_n=n\times\min\{U_1,U_2,..,U_n\}$, so that $0\le W_n\le n\times\theta$.
Let $H_n(w)=P(W_n\le w)$ be the cdf of $W_n$, and let $h_n(w)$ be the pdf of $W_n$

(a) Find the limit $H(w)$ of $H_n(w)$ as $n\to\infty$. Is it a cdf of a random variable?

(b) Find the limit $h(w)$ of $h_n(w)$ as $n\to\infty$.

(c) What is the asymptotic distribution of $W_n$?

(d) What is the mean, $E(W_n)$, of $W_n$?

### 2013FA4
[@Bino]

Suppose $X$ has a negative binomial distribution, with pdf.
$P(X=x)=\binom{x-1}{r-1})p^rq^{x-r},x=r,r+1,r+2,...$,
where $p+ q=1$, and $r$ is a fixed positive integer, namely the required number of successes to stop.

(a) Find the mean $E[X]$ of $X$.

(b) Find the variance $Var[X]$ of $X$.

### 2013FA5

Let X and Y be two continuous type independent random variables with distribution functions $F$ and $G$, respectively. Find

(a) the pdf of $V=F(X)+G(Y)$,

(b) the pdf of $W=\min\{F(X),G(Y)\}$.

### 2013FB1
[2007F1A] [2008S4A] [@Norm] [@MLE] [@suff] [@consi] [@unbias]

Let $Y_1,Y_2,..,Y_{n}$ be a random sample from $N(\mu,\sigma^2)$ distribution and let $X_1,X_2,..,X_{m}$ be an independent random sample from $N(2\mu,\sigma^2)$ distribution.

(a) Find minimal sufficient statistics for $(\mu,\sigma^2)$

(b) Find maximum likelihood estimators of $\mu$ and $\sigma^2$

(c) Show that
$\hat\sigma^2=\frac{\sum_{i=1}^m(X_i-\bar X)^2+\sum_{j=1}^n(Y_j-\bar Y)^2}{m+n-2}$
is unbiased and consistent for estimating $\sigma^2$

### 2013FB2
[2007F5A][] [2014F1B][] [2015S1B][] [@CDF] [@MLE]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find $\hat\theta$, the mle of $\theta$.

(b) Find $E[\hat\theta]$.

(c) Prove that $\hat\theta$ is consistent for $\theta$.

### 2013FB3
[2010SB1][] [2010FB1][] [2011S5][] [2015S2B][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 90th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2013FB4
[2007F4B][] [2015S3B][] [2018S1B][] [2019SB2][] [@Laplace] [@MLE] [@suff]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias. Is the m.l.e. a sufficient statistic?

### 2013FB5
[2018FB4] [@Unif] [@HypoT]

Let $X_1$ and $X_2$ be two independent random variables each having uniform distribution on the interval $(\theta,\theta+1)$. For testing $H_0:\theta=0$ against $H_a:\theta> 0$, we have two competing tests :

1. Test 1 : Reject $H_0 if X_1>0.95$

2. Test 2 : Reject $H_0 if X_1+X_2>c$.

Find the value of c so that the Test 2 has the same value of Type I error probability as Test 1.

## 2014S

Crain, Kochar

### 2014SA1
[@Pois]

Let $X$ given $\lambda$ be $Poissou(\lambda)$. Suppose $\lambda$ is a random variable whkh has Poisson distribution with parameter $\mu$. Find $E[X]$ and $Var[X]$.

### 2014SA2
[2015F2] [@Expo] [@Basu]

Assume that $X_1$ and $X_2$ have joint pdf $f(x_1,x_2)=exp(-x_1 ).exp(-x_2)$ for $0\le x_1,x_2<\infty$ and zero elsewhere. Define $Y_1=X_1/(X_1+X_2)$, $Y_2=X_1+X_2$ Use Basu's theorem to demonstrate that $Y_1$ and $Y_2$ are independent. Identify the marginal pdfs of $Y_1$ and $Y_2$ Find $E[X_1^3/(X_1+X_2)^2]$

### 2014SA3
[2014SA5][] [2015S3A][] [2016S3][] [@SNorm] [@mean]

Suppose $Z$ is a standard normal random variable with cdf $\Phi(.)$. Evaluate $E[\Phi(Z)]$ and $E[\Phi^{2}(Z)]$.

### 2014SA4
[@Unif] [@mean]

Let $U_1,U_2,..,U_n$ be iid $uniform[0,1]$ random variables. Let $0\le Y_1<Y_2<..<Y_n$ be the corresponding order statistics, ie, $Y_k$ is the $k^{th}$ smallest of the $U_i$ What is the joint pdf of $Y_1,Y_2,..,Y_n$? Find the marginal pdf of $Y_k$, where $1\le k\le n$. Find the mean and variance of $Y_k$.

### 2014SA5
[2014SA3][] [2015S3A][] [2016S3][] [@SNorm] [@mean]

Assume that $Z$ is a standard normal or $N(0,1)$ random variable. Find a formula for $E[Z^k]$ where $k$ is a positive integer.

### 2014SB1
[@Expo] [@CDF]

The lifetime (in hours) X of an electronic component is tt random variable with cumulative distribution functionfundion

$F(y)=\begin{cases}1-e^{-y/5}& y>0\\0& o.w.\end{cases}$

(a) What is the probability that a randomly selected component will operate for at least 10 hours?

(b) What is the probability that the lifetime of a randmnly selected component wi11 exceed its mean lifetime by more than two standard deviations?

(c) Three of these components operate independently in a piece of equipment. The equipment fails if at least two of the components fail. Find the probability that the equipment will operate for at least 10 hours without failure?

### 2014SB2
[2008S4B] [@Unif]

Let $X_1,X_2,..,X_{10}$ be random variables denoting 10 independent bids for an item that is for sale. Suppose that each $X_i$ is uniformly distributed on the interval $[\theta-50,\theta+50]$, where $\theta>100$. The seller sells to the highest bidder, how much can he expect to earn on the sale? 

### 2014SB3
[2014F4B] [@Norm] [@MLE]

Let $X_1,X_2,..,X_{n}$ be a random sample from a normal distribution, $N(\mu,\sigma^2)$, where $-\infty<\mu<+\infty$ and $\sigma>0$. Find the MLE of $\mu/\sigma$ and find itsexpected value.

### 2014SB4
[@Beta] [@CRLB]

Suppose $X_1,X_2,..,X_{n}$ is a random sample from a population with probability mass function

$p_\theta(X=x)=\theta^{x}(1-\theta)^{1-x},x=0,1; 0<\theta<1$

(a) Find the maxirnmn likelihood estimator of $Var_\theta(X) =\theta(1-\theta)$.

(h) Find the the Cramer-Rao lower bound for the variance of any unbiased estimator of $\theta(1-\theta)$.

### 2014SB5
[@Bino] [@Unif] [@BayesE] [@MLE]

Suppose X has Binomial distribution with pararneters $n$ and $\theta, 0<\theta<1$.

(a) Find the Bayes estimator of $\theta$ when the prior distribution is uniform on the interval $(0,1)$ and the loss function is square error loss function.

(b) Compare the risk of the above Bayes estimator with that of the MLE of $\theta$.

## 2014F

### 2014F1A
[@Bern] [@MGF]

Repeat a sequence of i.i.d. Bernoulli trials until you observe the frst success, where p = the probability of a success and $q=1-p=$ the probability of a failure on any one trial. Let the random variable Y count the number of failures before the frst success.

(a) State the name of this statistical experiment.

(b) Provide a mathematical formula for the probability mass function, $P(Y=y)$ where $y=$?.

(c) Give in closed form the $P(Y\ge y)$.

(d) Determine the $E(Y)$.

(e) Derive the moment generating function (M.G.F.) of Y . Remember to state the interval over which this M.G.F. exists.

### 2014F2A
[2008F2] [2010SA1][]

One percent of all individuals in a certain population are carriers of a particular disease. A diagnostic test for this disease has a 90% detection rate for carriers and a 5% detection rates for noncarriers. Suppose the test is applied independently to two different blood samples from the same randomly selected individual.

(a) What is the probability that both tests yield the same result?

(b) If both tests are positive, what is the probability that the selected individual is a carrier?

### 2014F3A
[2013S2][]

Suppose $X_1$ and $X_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:
$f(x)=\begin{cases}e^{-x}& x>0\\0& o.w.\end{cases}$

(a) Find the p.d.f. of $Y=4(X_1-X_2)$.

(b) Find the mean and variance of $Y$.

### 2014F4A
[2007F2A][] [2008S5A][] [2009FA1][] [2015S2A][] [2019SA2][]

The lifetime (in hours) Y of an electronic component is a random variable with density function
$f(y)=\begin{cases}\frac1{300}e^{-\frac1{300}y}& y>0\\0& o.w.\end{cases}$

(a) What is the probability that a randomly selected component will operate for at least 300 hours?

(b) Five of these components operate independently in a piece of equipment. The equipment fails if at least three of the components fail.

Find the probability that the equipment will operate for at least 300 hours without failure?

### 2014F5A

Let $Z_1,Z_2,..$ be a sequence of random variables random variables; and suppose that, for $n=1,2,..$, the distribution of $Z_n$ is given by $P(Z_n=n^2)=1/n$ and $P(Z_n=0)=1-1/n$.
Show that $\lim_{n\to\infty}E(Z_n)=\infty$ but $Z_n\overset{p}\to0$ as $n\to\infty$

### 2014F1B
[2007F5A][] [2013FB2][] [2015S1B][] [@CDF] [@MLE] [@CI]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find the MLE $\hat\theta$ of $\theta$.

(b) Prove that $\hat\theta$ is consistent for $\theta$.

(c) Find a 95% confidence interval for $\theta$ when $n=6$.

### 2014F2B
[2008S2B] [@Pois] [@FishI]

Let $X_1,X_2,..,X_{n}$ denote a random sample from a Poisson distribution with mean $\theta,\theta>0$.

(a) Find the Fisher information about $\theta$ in the sample.

(b) Suppose we want to estimate $m(\theta)=P(X_1=0)=e^{-\theta}$. Find a lower bound on the variance of any unbiased estimator of the parametric function $m(\theta)$.

### 2014F3B
[@UMP] [@HypoT] [@power]

Let $\theta$ be a parameter with space $\Omega=\{0; 1\}$. Let $X$ be a discrete random variable taking on values 1,2,3,or 4. Let the probability funtion of $X$ be given by the following table:

 ----------------------------------
            $X_1$ $X_2$ $X_3$ $X_4$
 ---------- ----- ----- ----- ----- 
 $\theta_0$ $1/2$ $1/4$ $1/8$ $1/8$
 
 $\theta_1$ $2/9$ $2/9$ $2/9$ $1/3$
 ----------------------------------

Find the UMP size 1/8 and 1/4 tests to test $H_0:\theta=0$ against $H_A:\theta=1$. Also find the powers of these two tests.

### 2014F4B
[2014SB3] [@Norm] [@MLE]

Let $X_1,X_2,..,X_{n}$ be a random sample from a normal distribution, $N(\mu,\sigma^2)$, where $-\infty<\mu<+\infty$ and $\sigma>0$. Find the MLE of $\mu/\sigma$ and find itsexpected value.

### 2014F5B
[2017FB2][] [@Expo] [@suff]

Let $X_1,X_2,..,X_{n}$ denote a random sample from exponential distribution
with pdf,
$f(x,\mu)=\begin{cases}e^{-(x-\mu)}& \mu<x<\infty\\0& e.w.\end{cases}$

(a) Show that $X_{(1)}=\min\{X_i\}$ is a complete sufficient statistic.

(b) Are $X_{(1)}$ and the sample variance independent statistics? Justify your answer.

## 2015S

Tableman, Kochar

### 2015S1Aa

Let X be a random variable with finite mean $\mu$, finite variance $\sigma^2$,
and assume $E(X^8)<\infty$. Prove or disprove:

i. $E[(X-\mu/\sigma)^2]\ge1$.

ii. $E[(X-\mu/\sigma)^4]\ge1$.

### 2015S1Ab
[2007F3A][] [2008S2A][] [2010SA4][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Suppose a box contains a large number of tacks, and the probability $X$ that a particular tack will land with its point up when it is tossed varies from tack to tack in accordance with the following pdf:

$f(x)=\begin{cases}2(1-x)&0<x<1\\0& o.w.\end{cases}$

Suppose a tack is selected at random from this box and this tack is then tossed three times independently. Determine the probability the tack will land with its point up on all three tosses.

### 2015S2A
[2007F2A][] [2008S5A][] [2009FA1][] [2014F4A][] [2019SA2][]

Suppose $Y_1$ and $Y_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:

$f(x)=\begin{cases}10e^{-10x}& x>0\\0& o.w.\end{cases}$

Find the p.d.f. of $X=Y_1-Y_2$.

### 2015S3A
[2014SA3][] [2014SA5][] [2016S3][] [@SNorm] [@mean]

First,let $\Phi(.)$ and $\phi(.)$ denote the standard normal cdf and pdf respectively. Then, let $X_1,..,X_n$ denotes a random sample from a normal distribution with means $\theta$ and variance $\sigma^2$, and let $F(.)$ and $f(.)$ denote the common cdf and pdf of the r.s. respectively. Assume the sample size $n$ is odd; that is, $n=2k-1$; $k=1,2,3,..$ In this situation, the sample
median is the $k^{th}$ order statistic, denoted by $Y_k$.

(a) (5) Let $g(y)$ denote the pdf of the sample median $Y_k$. Derive $g(y)$. You may use the symbols $F(.)$ and $f(.)$.

(b) (5) Determine the $E(Y_k|\bar X)$, where $\bar X$ is the sample mean of the above random sample. Justify your answer.

### 2015S4A
[2007F4A][] [@CDF]

(a) Let X be a continuous type random variable with cumulative distribution function $F(x)$. Find the distribution of the random variable $Y=\ln(1-F(X))$:

(b) Prove that for any $y\ge c$, the function $G_c(y)=P[X\le y|X\ge c]$

### 2015S5A
[@Pois]

Suppose X and Y are independent Poisson random variables with parameters $\lambda$ and $2\lambda$, respectively.

(a) Find the distribution of $X+Y$.

(b) Find $E[X|X+Y=5]$.

### 2015S1B
[2007F5A][] [2013FB2][] [2014F1B][] [@CDF] [@MLE]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find $\hat\theta$, the mle of $\theta$.

(b) Find $E[\hat\theta]$.

(c) Prove that $\hat\theta$ is consistent for $\theta$.

### 2015S2B
[2010SB1][] [2010FB1][] [2011S5][] [2013FB3][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 90th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2015S3B
[2007F4B][] [2018S1B][] [2019SB2][] [@Laplace] [@MLE] [@MLE]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2015S4B
[2007F5B][] [2018S3B][] [2019SB3][] [@SPower] [@power] [@UMP]

Suppose $Y$ is a random variable (sample size = 1) from a population with density function
$f(y|\theta)=\begin{cases}\theta y^{\theta-1}& 0<x<1,\theta>0\\0& o.w.\end{cases}$

(a) Sketch the power function of the test of the rejection: $Y>0.5$.

(b) Based on the single observation $Y$ , find the uniformly most powerful test of size $\alpha$ for testing $H_0:\theta=1$ against $H_A :\theta>1$.

### 2015S5B
[2009FA3][] [2019SA4][] [@FishI] [@CI]

Let $X_1,X_2,..,X_{5}$ denote a random sample size $n=5$ from a continuous distribution with cdf $F(.)$ with median $\theta$. Let $S(\theta)=$ the number of $\{X_i's>\theta\}$. We can express this as 
$S(\theta)=\sum_{i=1}^5I(X_i>\theta)$;
where I(.) is an indicator variable.

(a) State explicitly the distribution of $S(\theta)$.

(b) Sketch the graph of $S(\theta)$ as a function of $\theta$. Then describe the graph in words.

(c) Find a confidence interval for $\theta$ with confidence coefficient close to 0.95.


## 2015F 

### 2015F1
[2008S1B][] [2009FA4][] [@Unif] [@mean] [@Var] [@suff] [@UMVUE] 

Let $X_1,X_2,..,X_{n}$ be iid continuous uniform r.v.'s over $[0,\theta]$, where $0 <\theta<\infty$. Let $Y_{n}=\max\{X_1,X_2,..,X_{n}\}$

(a) (5 pts) Find $E[Y_n]$.

(b) (5 pts) Find $Var[Y_n]$.

(c) (5 pts) Show that $Y_n$ is a sufficient statistic for $\theta$.

(d) (5 pts) Assuming $Y_n$ is a complete sufficient statistic for $\theta$, find the UMVUE of $\theta$.

(e) (5 pts) Assuming $Y_n$ is a complete sufficient statistic for $\theta$, find the UMVUE of $\theta^2$.

### 2015F2
[2014SA2] [@Expo] [@Basu]

Suppose $X_1$ and $X_2$ are iid exponential with parameter = 1.

(a) (5 pts) Find the pdf of $Y_1=X_1/(X_1+X_2)$.

(b) (5 pts) Find the pdf of $Y_2=X_1+X_2$.

(c) (5 pts) Are $Y_1$ and $Y_2$ independent?

### 2015F3
[@Unif] [@LimD]

Let $X_1,X_2,..,X_{n}$ be iid $uniform[0,1]$ rv's. Let $0\le Y_1\le Y_2\le..\le Y_n\le1$ be the
corresponding order statistics.

(a) (5 pts) Find the pdf $g_k(y_k)$ of $Y_k$.

(b) (5 pts) Find $E[Y_k]$.

(c) (5 pts) Find $Var[Y_k]$.

(d) (5 pts) What is the limiting distribution of $W_1=nY_1$?

(e) (5 pts) What is the limiting distribution of $W_n=n(1-Y_n)$?

### 2015F4
[2010FB2] [@Beta] [@HypoT] [@power]

Let X1 be a random sample of size $n=1$ from the Beta distribution with pdf
$f(x|\theta)=\begin{cases}\frac{\Gamma(2\theta)}{\Gamma(\theta)\Gamma(\theta)}x^{\theta-1}(1-x)^{\theta-1}&0<x<1\\0&o.w.\end{cases}$

Suppose a researcher is interested in testing $H_0:\theta=1$ against $H_1:\theta=2$. The researcher decides to reject $H_0$ in favor of $H_1$ if $X_1<2/3$.

(a) (5 pts) Find the size of the test

(b) (5 pts) Compute the power of the test at $\theta=2$.

### 2015F5
[2010SB4][] [2010FB4][] [2011S6][] [@Expo] [@LRT] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample from the exponential distribution with mean $\theta_1$ and let $Y_1,Y_2,..,Y_{n}$ be an independent random sample from another exponential distribution with mean $\theta_2$. Find the likelihood ratio test for testing $H0:\theta_1=\theta_2$ vs $H_a:\theta_1\neq\theta_2$

## 2016S

Crain, Kim

### 2016S1
[@Pois] [@MVUE]

Let $X_1,X_2,..,X_n$ be iid (independent, identically distributed) Poisson random variables with parameter $\lambda>0$.

(a) Find a complete sufficient statistic for $\lambda$.

(b) Find the MVUE (Minimum Variance Unbiased Estimator) of $\lambda$.

(c) Find the MVUE of $\lambda^2$:

(d) Find the MVUE of $e^{-\lambda}$.

(e) Find the MVUE of $P(X_i=1)=\lambda^1e^{-\lambda}/1!$:

(f) Find the MVUE of $P(X_i=k)=\lambda^ke^{-\lambda}/k!$:

### 2016S2
[2017FB1][] [@Bino] [@MVUE]

Let $Y$ be $Binomial(n,p)$, with $n$ known and $p$ unknown. Among functions $u(Y)$ of $Y$,

(a) What is the MVUE of $p$?

(b) What is the MVUE of $p^2$?

(c) What is the MVUE of $pq=p(1-p)$?

(d) What is the MVUE of $P(Y=k)=\binom{n}{k}p^k(1-p)^{n-k}$?


### 2016S3
[2014SA3][] [2014SA5][] [2014SA5][] [2015S3A][] [@SNorm] [@mean]

Let $Z$ be $N(0,1)$. Let $\Phi(z)=\int_{-\infty}^z=\phi(x)dx$, where $\phi(x) = \frac1{\sqrt{2\pi}}e^{âˆ’x^2/2},-\infty<x<\infty$, where $\phi(x)$ is the standard normal pdf, and $\Phi(z)$ is the standard normal cdf.

(a) Find $E[\Phi(Z)]$.

(b) Find $E[\Phi^{2}(Z)]$.

(c) Find $E[n\Phi^{n-1}(Z)]$.

(d) Find $E[Z^4]$.

(e) Find $E[Z^5]$

### 2016S4
[2008F5][] [2009SB1][] [2009FB4][] [2016F7][] [2017FB4][] [2018FB2][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$.

(c) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

(d) (2 pts) What is the asymptotic distribution of $\hat\theta$?

### 2016S5
[2007F3A][] [2008S2A][] [2010SA4][] [2015S1Ab][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $X$ and $Y$ have the following joint pdf:
$f(x,y) =\begin{cases}6(y-x)& 0<x<y<1\\0& o.w.\end{cases}$
Define $Z=(X+Y)=2$ and $W=Y$, respectively.

(a) Find the joint pdf of Z and W.

(b) Find the marginal pdf of Z.

## 2016F

Fountain, Ian Dinwoodie

### 2016F1
[@Norm]

Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$.
Let $S_k^2=\frac1k\sum_{i=1}^n(X_i-\bar X)^2$ be an estimator of $\sigma^2$. Find the value of $k$ that minimizes the mean squared error of the estimator.

### 2016F2
[@MGF]

The moment generating function of a particular random variable is $M_X(t)=\frac{e^t}{4-3e^t}$. Find the coefficient of variation ($CV=\sigma/\mu$) of this distribution.

### 2016F3
[2008F3][]

If X is a random variable such that $E[X]=2$ and $E[X^2]=13$, determine a lower bound for the probability $P(-4<X<8)$. (Hint: Use a famous inequality.)

### 2016F4
[2008S3A][] [2008F1][] [@Unif] [@CDF] [@PDF]

Let $Y_1$ and $Y_2$ be a random sample of size 2 from $Uniform(0,1)$. Find the cumulative distribution and probability density functions of $U=Y_1+Y_2$.

### 2016F5
[2008F6][] [@Basu] 

Let $Y_n$ be the $n^{th}$ order statistic of a random sample of size n from the normal distribution $N(\theta,\sigma^2)$. Prove that $Y_n-\bar Y$ and $\bar Y$ are independent.

562-2

### 2016F6
[2008F7][] [@Expo] [@BayesE]

Suppose that $X_1,X_2,..,X_n$ i.i.d. $Exponential(\theta)$, i.e. $f(x;\theta)=\theta e^{-\theta x},x>0$. Also assume that the prior distribution of $\theta$ is $h(\theta)=\lambda e^{-\lambda\theta},\theta>0$. Find the Bayes estimator of $\theta$, assuming squared error loss.

### 2016F7
[2008F5][] [2009SB1][] [2009FB4][] [2016S4][] [2017FB4][] [2018FB2][] [2019SB4][] [@UMP]

Let $X_1,X_2,..,X_n$ be a random sample of size n from the following distribution:

$f(x;\theta)=(\theta+1)x^\theta,\ 0\le x\le 1$

where $\theta>-1$ is an unknown parameter.

(a) Find the method of moments estimator for $\theta$.

(b) Find the maximum likelihood estimator for $\theta$.

(c) Determine if your MLE is unbiased.

(d) Find the asmptotic variance of your MLE in part (b), as $n\to\infty$.

(e) Find the Cramer-Rao lower bound on the variance of an unbiased estimator of $\theta$.

(f) Identify the sufficient statistic for $\theta$.

(g) Suppose you've taken a sample of size $n=10$. Determine the UMP test of the null hypothesis $\theta=.5$ vs. the alternative $\theta>0.5$.


### 2016F8
[2007F3A][] [2008S2A][] [2009FA2][] [2010SA4][] [2015S1Ab][] [2016S5][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}c(1-y_2)&0\le y1\le y2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c.

(b) Find the marginal density functions of $Y_1$ and $Y_2$.

(c) Find $P(Y_2\le1/2|Y_1\le3/4)$



## 2017S

Ian Dinwoodie, Robert Fountain

### 2017S1

Let $\Theta$ be a real-valued random variable with density $f\Theta(\theta) = \frac1{\sqrt{2\pi}}e^{âˆ’\theta^2/2},\theta\in\mathbb R$, and let Y have
conditional density $f(y|\theta) = \frac1{\sqrt{2\pi}}e^{âˆ’(y-\theta)^2/2},y\in\mathbb R$. Determine

(a) the conditional density of $\Theta$ given $Y = y$, (2 pts)

(b) the marginal density of $Y$. (3 pts)

### 2017S2

If $Z_1,Z_2$ are independent standard normal random variables, find the density of $Z_1/Z_2$.


### 2017S3

Suppose $X_1,X_2,..,X_n$ is a random sample of $Exp(1)$ @Expo random variables. Find the moment generating function $M(t)=E(e^{tX_{(1)}}), t\in\mathbb R$, where $X_{(1)}$ is the minimum. (5 pts)

### 2017S4

Let $X=e^Z$ be a lognormal random variable, $Z\sim N(0,1)$. Find its skewness $E(X âˆ’ Î¼)^3/\sigma^3$.

### 2017S5

Suppose $X_1,X_2,..,X_n$ is a random sample of $N(\mu,\sigma^2)$ random variables.

(a) Find the expectation of the MLE for $\sigma^2$. (3 pts)

(b) Compute the correlation of $\bar X$ and $X_n$. (2 pts)

### 2017S6
[@UMP]

Let $X_1,X_2,X_3,...$ be i.i.d. $Poisson(\lambda)$ random variables. Find a UMP (uniformly most powerful) test of $H_0:\lambda\le1$ versus $H_1:\lambda>1$ at a level $\alpha$ near .05. (5 pts)

### 2017S7

Suppose that $(P,X)$ is a pair of random variables with $P\sim Beta(1/2, 1/2)$ and then $X_{|P=p}\sim bin(n,p)$. Find the variance of X.

### 2017S8
[@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample of $N(\mu,\sigma^2)$ random variables. Find the Cramer-Rao
lower bound  on the variance of an unbiased estimator for $\sigma^2$. (Assume $\mu$ is known.)


## 2017F

Kim, Kochar

### 2017FA1
[2018S3A][]

Suppose Xenophon and Yves meet for lunch, and Xenophon arrives at time X
uniformly from 1 to 2 P.M., and Yves arrives independently at time Y with the same distribution. Find the distribution of $|Y-X|$ and its expectation, that is, the expected waiting time of either party.



### 2017FA2
[@Expo]

Let $X_1,X_2,..,X_n$ be i.i.d. $Exp(\lambda)$ random variables with rate parameter $\lambda$ and density
$f(x)=\lambda e^{-\lambda x}, x>0$, with $\sigma^2=1/\lambda^2$. We are thinking about using the estimator $\bar X^2$ for the variance. Find the limiting distribution of
$\sqrt{n}(\bar X_n^2-1/\lambda^2)$

Let  be a random sample from $Poisson(\mu)$ @Pois. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

### 2017FA3
[@Expo]

If X and Y are independent $Exp(1)$ random variables, and the density of the ratio $X/(X+Y)$.

### 2017FA4

Let $F$ be the cdf of an exponential random variable with median 10 and let $G$ be
that of an independent exponential random variable Y with median 5. Find the distribution
of $V=F(X)+G(Y)$.

### 2017FB1
[2016S2][] @Bino

Let $Y$ be $Binomial(n,p)$, with $n$ known and $p$ unknown. Among functions $u(Y)$ of $Y$,

(a) What is the MVUE of $p$?

(b) What is the MVUE of $p^2$?

(c) What is the MVUE of $pq=p(1-p)$?

(d) What is the MVUE of $P(Y=k)=\binom{n}{k}p^k(1-p)^{n-k}$?

### 2017FB2
[2014F5B][] [@Expo]

Let $X_1,X_2,..,X_{10}$ be a random sample from an exponential distribution with location
parameter $\theta$ with pdf
$f(x;\theta)=\begin{cases}e^{-(x-\theta)}& \theta<x<\infty\\0& e.w.\end{cases}$

where $-\infty<\theta<\infty$is an unknown parameter. For testing the null hypothesis $H_0:\theta=0$
vs the alterative $H_1:\theta>0$, a reasonable test is to reject the null hypothesis if $X_{(1)}=\min\{X_1,X_2,..,X_{10}\}\ge C$. Find $C$ so that the size of the test is 0.05. Also find the power of this test at $\theta=1$. Is this test unbiased?

### 2017FB3
[2008S5B][] [@Norm] [@MLR] [@UMP] [@power] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample of size m from $N(\theta,1)$ distribution and
let $Y_1,..,Y_m$ be an independent random sample of size $m$ from $N(3\theta,1)$.

(a) Show that the joint distribution of X's and Y's has @MLR (monotone likelihood ratio) property.

(b) Find the UMP test of size $\alpha$ for testing $H_0:\theta\le0$ vs $H_1:\theta>0$.

(c) Find an expression of the power function of the UMP test.

### 2017FB4
[2008F5][] [2009SB1][] [2009FB4][] [2016S4][] [2016F7][] [2018FB2][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) What is the asymptotic distribution of $\hat\theta$?

(c) (2 pts) Using $\hat\theta$, find an unbiased estimator of $\theta$.

(d) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.



## 2018S

Kochar, Kim

### 2018S1A

An electric device has lifetime denoted by $T$. The device has value $V=5$ if it fails before time t = 3; otherwise, it has value $V = 2T$. Find the cdf of $V$ , if $T$ has pdf

$f_T(t)=\frac1{1.5}e^{-\frac1{1.5}t}, t>0$

### 2018S2A

Let $X$ have pdf

$fX(x)=\frac29(x+1), -1\le x\le2$

Find the pdf of $Y=X^2$.

### 2018S3A
[2017FA1][]

Suppose Xenophon and Yves meet for lunch, and Xenophon arrives at time X
uniformly from 1 to 2 P.M., and Yves arrives independently at time Y with the same distribution. Find the distribution of $|Y-X|$ and its expectation, that is, the expected waiting time of either party.

### 2018S4A

Let $X\sim N(\mu,\sigma^2)$ and $Y\sim N(\gamma,\sigma^2)$. Suppose that X and Y are independent. Deine $U=X+Y$ and $V=X-Y$.

(a) Show that U and V are independent.

(b) Find the distribution of each of them.

### 2018S1B
[2007F4B][] [2019SB2][] [@Laplace]

Let $X_1,X_2,..,X_1$ be a random sample of size 11 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2018S2B
[2010SB1][] [2010FB1][] [2011S5][] [2013FB3][] [2015S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 90th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2018S3B
[2007F5B][] [2015S4B][] [2019SB3][] [@SPower] [@power] [@UMP]

Let $X_1,X_2,..,X_n$ be a random sample from a a distribution with pdf

$f(x;\theta)=\begin{cases}\theta x^{\theta-1}& 0<x<1\\0& o.w.\end{cases}$

$H_0:\theta=1\quad H_1 :\theta>1$

Derive the UMP test of size $\alpha$ and obtain the null distribution of your test statistic.

### 2018S4B

The life time of an electronic component has exponential distribution @Expo with mean $\mu$.
10 such components are put on test at the same time and the experiment is terminated when all of them fail and the times of their failure, $X_1,X_2,..,X_n$ are noted. Based on this information,
derive the likelihood ratio test @LRT at the level $\alpha= 05$ of the null hypothesis $H_0:\mu=5$ against the alternative $H_1:\mu\neq5$. Also find an expression for the @power function of this test.

$T=\sim x_i$, $\frac{2T}{\theta}\sim\chi^2_{2n}$

reject $T>t_0$ two-side rejection region.

## 2018F

Kochar, Bruno

### 2018FA1
[2007F3A][] [2008S2A][] [2009FA2][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2019SA1 ][] [@joint] [@marg]

Suppose $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}C&0\le y_1\le y_2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c.

(b) Find the marginal density functions of $Y_1$ and $Y_2$ and check whether they are independent.

(c) Find $E[Y_1+Y_2]$

(d) Find $P(Y_1\le1/3|Y_1\le3/4)$

### 2018FA2

Let $X_1,X_2,..,X_n$ be a random sample from an  exponential distribution [@Expo] with mean 5.

$X_{(r)}=(n-j):(n-1)$

(a) Find the @CDF of the sample range.

pdf $X_{(n)}-X_{(1)}$

CDF $=\sum pdf$

(b) Find the expected value [@mean]of the sample range. @5.4.5

$E[X_{(r)}]=\sum_{j=1}^{n-1}\frac1{(n-1)-j+1}$

### 2018FA3

(a) (5 pts) In the daily production of a certain type of rope, the number of defects per foot, $X$ is assumed to have a Poisson distribution [@Pois] with mean $\lambda=3$. The profit per foot of the rope sold is given by
$P=30-3X-X^2$
Find the expected profit per foot.

$E[P]=30-3EX-EX^2$


(b) (5 pts) Suppose that $X$ is distributed as $U(0,1)$ and that $Y$ is a random variable with
$E(Y|X=x)=\alpha+\beta x^2$
Find $E[Y]$.

$E[Y]=E(E[Y|X])$

### 2018FA4
[2010SA3][] [2011S4][] [@Pois] [@LimD]

Let $X_1,X_2,..,X_n$ be a random sample from $Poisson(\mu)$. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

$\sqrt{n}(\bar X_n-\mu)\to n(0,\frac1{I(\mu)})$

Delta Method

$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})\to n(0,\frac{[(e^{-\mu})']^2}{I(\mu)})$

### 2018FB1
@LRT

(4+6 pts) Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution with mean 120 and unknown variance $\sigma^2$. Derive the likelihood ratio test for testing the null hypothesis $H_0:\sigma^2=4$ against the alternative $H_1:\sigma^2\neq4$. Also fnd the exact as well as the asymptotic null distributions of your test statistic.


### 2018FB2
[2008F5][] [2009SB1][] [2009FB4][] [2016S4][] [2016F7][] [2017FB4][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) What is the asymptotic distribution of $\hat\theta$?

(c) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$ of $\theta$.

(d) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

### 2018FB3
[@UMVUE]

Let $X_1,X_2,..,X_n$ be a random sample from $N(\mu,\sigma^2)$ distribution. Find a lower bound on the variance of any unbiased estimator of the 95th percentile of this distribution based on the Information Inequality. Also compare this bound to the variance of the uniformly minimum variance unbiased estimator .

$\hat\tau=\bar x+1.645s$

### 2018FB4
[2013FB5] [@Unif] [@HypoT]
Let $X_1$ and $X_2$ be two independent random variables each having uniform distribution
on the interval $(\theta,\theta+1)$. For testing $H_0:\theta=0$ against $H_a:\theta> 0$, we have two competing tests :

1. Test 1 : Reject $H_0 if X_1>0.95$

2. Test 2 : Reject $H_0 if X_1+X_2>c$.

Find the value of c so that the Test 2 has the same value of Type I error probability as Test 1.

$\alpha=P(x_1>0.95)=P(x_1+x_2>c)$

find pdf of $x_1+x_2$ (example of textbook)


## 2019S

Kochar, Bruno

### 2019SA1 
[2007F3A][] [2008S2A][] [2009FA2][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [@joint] [@marg]

Suppose X and Y have the joint pdf

$f(x;y) =\begin{cases}Cxy& 0\le x\le 2, 0\le y\le 2, x + y\le 2\\0& o.w.\end{cases}$

(a) (4 pts) Find the value of C;

(b) (4 pts) Find the marginal densities of X and Y and check whether they are independent or not;

(c) (2 pts) Compute $P(X<Y)$;

### 2019SA2
[2007F2A][] [2008S5A][] [2009FA1][] [2014F4A][] [2015S2A][]

Suppose $Y_1$ and $Y_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:

$f(y)=\begin{cases}\theta e^{-\theta y}& y\ge0\\0& o.w.\end{cases}$

with $\theta>0$.

Find the p.d.f. of $X=Y_1-Y_2$.

### 2019SA3 

Let $\theta$ be Beta distributed, $\theta\sim Beta(1,1)$. Let $N_1$ be Binomial given $\theta$, that is $N_1\sim Bin(n,\theta)$ given $\theta$.

(a) (4 pts) Compute $p(\theta|N_1=n_1)$ and $E[\theta|N_1=n_1]$

(b) (6 pts) Compute $p(N_1=n_1)$ for $n_1=0...n$.

### 2019SA4 
[2009FA3][] [2015S5B][] [@MOM]

Let $X_1,X_2,..,X_{10}$ be a random sample from a Poisson distribution with mean $\lambda$.

(a) (4 Ppts) Use the method of moment generating functions to find the distribution of $S_{10}=\sum^{10}_{i=1} X_i$.

(b) (6 pts) Let $S_4=\sum_{i=1}^4X_i$ Find the conditional distribution of $S_4$ given $S_{10}=s$, for $s>0$. This distribution belongs to a family of distributions that you know. Which family? which parameters?

### 2019SB1
[2009FB1][]

Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution $N(\mu,\sigma^2=25)$. Reject $H_0:\mu=50$ and accept $H_1:\mu=55$ if $\bar X_n\ge c$. Find the two equations in n and c that you would solve to get $P(\bar X_n\ge c|\mu)=K(\mu)$ to be equal to $K(50)=0.05$ and $K(55)=0.90$.
Solve these two equations. Round up if n is not an integer. Hint: $z_{.05}=1.645$ and $z_{.1}=1.28$

### 2019SB2 
[2007F4B][] [2015S3B][] [2018S1B][] [@Laplace] [@MLE]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2019SB3 
[2007F5B][] [2015S4B][] [2018S3B][] [@SPower] [@power] [@UMP]

Suppose $X_1,X_2,..,X_n$ is a random sample from a distribution with pdf

$f(x,\theta)=\begin{cases}\theta x^{\theta-1}& 0<x<1\\0& o.w.\end{cases}$

Suppose that the value of $\theta$ is unknown and it is desired to test the following hypotheses :

$H_0:\theta=1\quad H_1 :\theta>1$

Derive the UMP test of size $\alpha$ and obtain the null distribution of your test statistic.

### 2019SB4 
[2008F5][] [2009SB1][] [2009FB4][] [2016S4][] [2016F7][] [2017FB4][] [2018FB2][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$ of $\theta$.

(c) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

(d) (2 pts) What is the asymptotic distribution of $\hat\theta$?



# References