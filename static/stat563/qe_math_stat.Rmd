---
title: ""
subtitle: ""
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true

bibliography: bib_math_stat.bib
csl: index-for-stat.csl
---


```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
```

# 

## 2000F

### 2000F1

Suppose $X\sim Poisson(\lambda)$.

A) Find $E[X]$

B) Find $E[X(X − 1)]$

C) Find $E[X(X − 1)(X − 2)]$

### 2000F2


Suppose X1 and X2 are independent random variables with $X_1\sim Poisson(\lambda_1),X_2\sim Poisson(\lambda_2)$. Prove that $X_1+X_2\sim Poisson(\lambda_1+\lambda_2)$.

### 2000F3
[2003F6][]

Suppose $X\sim Binomial(n,p)$.

A) Find $E[X]$.

B) Find $E[X(X − 1)]$.

C) Find $E[X(n − X)]$.

### 2000F4
[@Bino] [@unbias] [@CRLB] [@MVUE] [@suff]

Suppose $X\sim Binomial(n,p)$

A) Find an unbiased estimator of $p^2$ and an unbiased estimator of $pq$ where $q=1−p$. (Hint:Use 3.)

B) Determine the Cramer-Rao lower bound of the variance of all unbiased estimators $T$ of $p^2$.

C) Find a MVUE (minimum variance unbiased estimator) of $p^2$. Is it unique wp1? Why or why not? State the name(s) of the theorem(s) you are using.

D) Is the estimator you found in part (c) an efficient estimator? Why or why not?


### 2000F5
[@SNorm] [@Norm] [@MGF]

A) Let $Z\sim N(0, 1)$. Find $E[Z^k]$ for $k=0,1,2,3,4$.

B) Let $X\sim N(\mu,\sigma^2)$. Find $E[X^k]$ for $k=0,1,2,3$.

### 2000F6

A) What is the numerical value of $\sum_{k=0}^6\binom{6}{k}$?

B) What is the numerical value of $\sum_{k=0}^6(-1)^k\binom{6}{k}$?

### 2000F7
[@Bino] [@UMP]

In genetic applications the truncated Binomial distribution has been used for
a model. We say X has a truncated binomial distribution if: 
$P(X=x)=\frac{\binom{n}{x}\theta^x(1-\theta)^{n-x}}{1-(1-\theta)^n}$ for $x=1,2,3,..,n$.

A) Construct in detail the most powerful critical region for testing
$H_0:\theta=\theta_0$ against $H_1:\theta=\theta_1$, with $\theta_0<\theta_1$.

B) Will this test be UMP (uniformly most powerful) for testing $H_0:\theta\le\theta_0$ against $H_1:\theta>\theta_0$?

### 2000F8
[@MLE] [@suff] [@MVUE] 

Suppose $X_1,X_2,..,X_n$ is a random sample from a distribution with density $f(x,\alpha,\beta)=\frac1{\beta}e^{-\frac{x-\alpha}\beta}$, where $x\ge\alpha;\alpha\in\mathbb R;\beta>0$. Define $\hat\alpha=\min(X_i)$, and $\beta=\bar X-\min(X_i)$.

A) Show that $\hat\alpha,\hat\beta$ are MLE’s for $\alpha,\beta$.

B) Show that $\hat\alpha,\hat\beta$ are sufficient for $\alpha,\beta$.

C) Using the fact that the above estimators are complete, find the MVUE’s of $\alpha,\beta$.

### 2000F9
[@BayesE]

Let $X_1,X_2,..,X_n$ be a random sample from $f(x|\theta)=\theta(1−\theta)^x$, with $x=0,1,2,..$ Let $g(\theta)=1$ when $0<\theta<1$ be a uniform prior distribution for $\Theta$.

A) Find the posterior distribution of $\theta$.

B) Find the Bayes estimator of $\theta$ (assuming squared error loss).

## 2003S

### 2003S1
[2008S1A][]

An urn contains 6 red and 3 blue balls. One ball is selected at random and replaced by a ball of the other color. A second ball is then chosen. What is the probability that the first ball selected is red given that the second was red?

### 2003S2

Let $X$ be a continuous random variable with PDF $f(x)=1−|x|$, with $−1<x<1$. Let $Y=X^2$. Find the PDF of $Y$.

### 2003S3
[2004F11][] [2007F3A][] [2008S2A][] [2009FA2][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [@joint] [@marg]

Let $X$ and $Y$ be continuous random variables with joint PDF $f(x,y)=8xy$, with $0\le x\le y\le1$, and zero elsewhere. Let $W=XY$. Find the PDF of $W$.

### 2003S4

The time X for an appliance dealer to travel between Cityville and Ruralville is a normally distributed random variable with mean 30 minutes and standard deviation 10 minutes. The time Y it takes to install an appliance is also a normally distributed random variable with mean 20 minutes and standard deviation 5 minutes. If $X$ and $Y$ are independent, what is:

A) The mean and variance of the total time to drive from Cityville to Ruralville, install an appliance, and return?

B) The probability that the total time required in (a) is over 95 minutes? Set up only.


### 2003S5
[@Pois] [@Bino] [@indep]

Suppose that $X\sim Poisson(\theta)$ and $(Y|X=x)\sim Binomial(x,p)$.

A) Find the distribution of $Y$.

B) Show that $Y$ and $X−Y$ are independent.

### 2003S6
[@MGF] [@LimD] [@mean] [@Var]

The MGF of a random variable $X$ is of the form: $M(t)=\frac{e^t+e^{−t}}2$.

A) Find the mean and variance of the sample mean $\bar X$ based upon a random sample of size n taken from the random variable $\bar X$.

B) Find the MGF of the sample mean $\bar X$.

C) What is the limiting distribution of $\sqrt{n}\bar X$? Why?

### 2003S7
[2008F5][] [2009SB1][] [2009FB4][] [2016S4][] [2016F7][] [2017FB4][] [2018FB2][] [2019SB4][] [@MOM] [@MLE] [@effi] [@CRLB]

Let X be a random variable with PDF $f(x)=\frac1\theta x^{-\frac1\theta-1}$,where $x>1$(and 0 elsewhere),$\theta>0$ Based on a sample of size n,

A) Find the method of moments estimator of $\theta$.

B) Find the maximum likelihood estimator of $\theta$.

C) Find the Cramer-Rao lower bound for the variance of an unbiased estimator of $\theta$.

D) Find the efficiency of the maximum likelihood estimator of $\theta$.

### 2003S8
2010SB3
2010FB3[@Norm] [@MVUE]

Let $X_1,X_2,..,X_n$ denote a random sample from a distribution that is $N(0,\theta)$. Find the unbiased minimum variance estimator of $\theta^2$.

### 2003S9
[2003S9][] [2007F5B][] [2015S4B][] [2018S3B][] [2019SB3][] [@SPower] [@power] [@UMP]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with PDF $f(x)=\theta x^{\theta-1}, x>1$ (and 0 elsewhere). Find the best critical region for testing $H_0:\theta=1$ against $H_1:\theta=2$.

### 2003S10
[2008F6][] [2016F5][] [@Norm] [@indep] [@Basu] 

Let $Y_n$ be the $n{th}$ order stat of a random sample of size n from the normal distribution $N(\theta,\sigma^2)$. Prove that $Y_n−\bar Y$ and $\bar Y$ are independent.


## 2003F

### 2003F1

Let random variables X and Y have joint PDF $f(x,y)=e^{−x−y}$ for $x>0, y>0$, and zero otherwise. Let $Z=X+Y$.

A) Find the joint PDF of $X$ and $Z$.

B) Find the PDF of $Z$.

C) Find the PDF of $Z$, given $X=x$.

D) Find the PDF of $X$, given $Z=z$.

### 2003F2
[@Var] [@Cov] [@Cor]

Suppose $X_1$ has variance $\sigma^2=4$, $X_2$ has variance $\sigma^2=3$, and $Cov(X_1,X_2)=−2$. If $U=X_1+2X_2$ and $V= 3X_1+4X_2$,

A) Find $Var[U]$ and $Var[V]$.

B) Find $Cov(U,V)$.

C) Find $Corr(U,V)$.

### 2003F3
[@Var] [@Unif] [@mean]

Let $X\sim uniform(0,1)$ and $Y=−\log(X)$.

A) Find the CDF and PDF of $Y$.

B) Find $E[Y]$ and $Var[Y]$.

### 2003F4
[@Pois] [@MLE] [@CRLB] [@suff]

Suppose $X_1,X_2,..,X_n$ iid $Poisson(\theta)$ random variables with common marginal PDF $f(x)=\frac{\theta^xe^{-\theta}}{x!},x=0,1,2,..$

A) Find the maximum likelihood estimator of $\theta$.

B) Find a sufficient statistic for $\theta$.

C) Find the Cramer-Rao lower bound for the variance of unbiased estimators of $\theta$.

D) Does the MLE achieve the CRLB?


### 2003F5
[@Bino] [@Norm] [@Pois] [@comp]

A) Show that the $Binomial(n,p)$, $0\le p\le1$ family of PDFs is complete. $f(x)=\frac{n!}{x!(n−x)!}p^x(1−p)^{n−x}$

B) Show that the $Normal(0,\sigma^2)$, $0<\sigma^2<1$ family of PDFs is not complete. $f(x)=\frac1{\sqrt{2\pi}}e^{\frac{-x^2}{2\sigma^2}}$.
But shouldn’t there be a $\sigma$ in the denominator of the constant?

C) Show that the $Poisson(\theta)$, $0<\theta<1$ family of PDFs is complete. $f(x)=\frac{\theta^xe^{-\theta}}{x!}$


### 2003F6
[2000F3][]

Let $X\sim Binomial(n,p)$ be a random variable.

A) Prove that $E[X]=np$.

B) Find $E[X(X−1)(X−2)]$.

C) Find $E[X(n−X)]$.

### 2003F7

Let X be a single random variable having PDF $f(x)=\frac1{\theta}e^{−\frac{x}{\theta}},x>0$, zero elsewhere. Consider
testing the null hypothesis $H_0:\theta=2$ versus the alternative $H_1:\theta=4$ using the critical region
$x\ge4$.

A) Find $\alpha$, the probability of a Type-I error.

B) Find $\beta$, the probability of a Type-II error.

### 2003F8
[2014SA2][] [2015F2][] [2017FA3][]  [@Expo] [@Basu] [@indep]

Let random variables X and Y have joint PDF $f(x,y)=e^{−x−y}$ for $x>0,y>0$, and zero otherwise. Define $U=\frac{X}{X+Y}$ and $V =X+Y$.

A) Find the joint PDF of $U$ and $V$.

B) Show that $U$ and $V$ are independent.

C) Find the PDF of $U$.

### 2003F9
[2011F6][] [2017S6][] [@Pois] [@UMP]

Suppose $X_1,X_2,..,X_n$ are iid $Poisson(\theta)$ random variables with common marginal PDF $f(x)=\frac{\theta^xe^{-\theta}}{x!}$ for $x = 0,1,2,..$

Find the form of a uniformly most powerful (UMP) test of $H_0:\theta=\theta_0$ versus $H_1:\theta>\theta_0$. Explain why your test is a UMP.

## 2004F

### 2004F1

Let $X_1,X_2,..,X_n$ be iid $uniform[0,1]$ random variables. Define $Y_1=\min(X_1,X_2,..,X_n)$ and $Y_n=\max(X_1,X_2,..,X_n)$ Prove

(a) $E(Y_1) =1/(n-1)$

(b) $E(Y_n) =n/(n-1)$

### 2004F2

Let $X_1,X_2,..,X_n$ be iid $uniform[\theta_1,\theta_2]$ random variables, where $-\infty<\theta_1<\theta_2<\infty$. Define $Y_1=\min(X_1,X_2,..,X_n)$ and $Y_n=\max(X_1,X_2,..,X_n)$. Find the joint sufficient statistics for $\theta_1$ and $\theta_2$.

### 2004F3

Let $Y=e^X$, where $X\sim N(\mu,\sigma^2)$. Find

(a) the mean of $Y$, and

(b) the variance of $Y$.

### 2004F4

Let $T$ be a positive random variable with cdf $F(t)$. Define the function $H(t)$ as $H(t)=-\log(1-F(t))$. Show that $H (T)\sim\exp(\lambda=1)$.
Note: The pdf of an exponential is $f(x|\lambda)=\lambda\exp(\lambda x)$, for $0<x<\infty$ and $\lambda>0$. It equals 0 elsewhere.

### 2004F5
[2009SB2][] [@Norm] [@indep]

Let $X_1,X_2,..,X_n$ be a random sample of size $n=5$ from a normal distribution $N(0,\theta)$.

(a) Argue that the ratio and its denominator are independent.
$R=(X_1^2+ X_2^2)/(X_1^2+ X_2^2+X_3^2+ X_4^2+X_5^2)$

(b) Does $5R/2$ have an F-distribution with 2 and 5 degrees of freedom? Explain.

### 2004F6
[@Bino] [@Var] [@unbias] 

Let Y be $binomial(n,p)$.

(a) Find an unbiased estimator $a(Y)$ of $p$.

(b) Find an unbiased estimator $b(Y)$ of $pq$, where $q=1-p$.

(c) Determine a lower bound for the variance of the estimator $b(Y)$ in part (b).

### 2004F7
[@Pois]

Let $X_1,X_2,..,X_n$ be lid $Poisson(\lambda)$. 

Let$\bar X=\frac{\sum_{i=1}^{n}X_{i}}{n}$ and $S^2=\frac{\sum_{i=1}^{n}(X_{i}-\bar X)^2}{n-1}$. 

Determine $E(S^2|\bar X)$. State your argument clearly.

### 2004F8
[2007F4B][] [2013FB4][] [2015S3B][] [2018S1B][] [2019SB2][] [@Laplace] [@MLE]

Suppose the $X_1,X_2,..,X_n$ form a random sample from a population
with density function
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty, -\infty<\theta<\infty$
Find the M.L.E. of $\theta$.

### 2004F9
[2003S9][] [2007F5B][] [2015S4B][] [2018S3B][] [2019SB3][] [@SPower] [@power] [@UMP]

Suppose $Y$ is a random variable of size 1 from a population with density function
$f(y|\theta)=\begin{cases}\theta y^{\theta-1}& 0\le y\le1\\0& o.w.\end{cases}$, where $\theta>0$

(a) Sketch the power function of the test of the rejection: $Y>0.5$.

(b) Based on the single observation $Y$ , find the uniformly most powerful test of size $\alpha$ for testing $H_0:\theta=1$ against $H_A :\theta>1$.

### 2004F10
[2014F5A][]

Let $Z_1,Z_2,..$ be a sequence of random variables random variables; and suppose that, for $n=1,2,..$, the distribution of $Z_n$ is follows: $P(Z_n=n^2)=1/n$ and $P(Z_n=0)=1-1/n$.
Show that 

(a) $\lim_{n\to\infty}E(Z_n)=\infty$ and

(b) $Z_n\overset{p}\to0$ as $n\to\infty$

### 2004F11
[2003S3][] [2007F3A][] [2008S2A][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1][] [@joint] [@marg]

Suppose a box contains a large number of tacks, and the probability $X$ that a particular tack will land with its point up when it is tossed varies from tack to tack in accordance with the following pdf:

$f(x)=\begin{cases}2(1-x)&0<x<1\\0& o.w.\end{cases}$

Suppose a tack is selected at random from this box and this tack is then tossed three times independently. Determine the probability the tack will land with its point up on all three tosses.

### 2004F12
[2010SB4][] [2010FB4][] [2011S6][] [2015F5][] [2018S4B][] [@Expo] [@LRT] [@HypoT]

Let $T_1,T_2,..,T_n$ be a random sample with density function
$f(t|\theta)=\frac1\theta\exp(-t/\theta)$ for $0<t<\infty$ and $0<\theta<\infty$, $f(t|\theta)=0$ elsewhere.

(a) Show that the likelihood ratio test (LRT) to test $H_0:\theta=\theta_0$ against $H_A:\theta\neq\theta_0$ is equivalent to the two-sided test based on the test statistic
$T^*=\frac{2}{\theta_0}\sum^n_{i=1}T_i$

(b) Under $H_0:\theta=\theta_0$, what is the distribution of $T*$?


## 2005S
Kochar

### 2005S1
[@joint]

Let $F(x,y)=1$ if $x+y\ge1$, and zero otherwise. Show that $F(x,y)$ cannot be a joint cdf of two random variables $X$ and $Y$.

### 2005S2

Let $X_1,X_2,..,X_n$ be iid rv's from a distribution which has pdf $f(x)=e^{-x},0\le x<\infty$, and zero gtherwise.
Let $0\le Y_1<Y_2<..<Y_n$ denote the order statistics of the sample. Define $W_i= Y_i-Y_{i-1}$ for $i = 1,2,..,n$, with $Y_0=0$.

(a) (6) Show that the $W_i$'s are independent random variables.

(b) (3) Find $E(W_i)$ for $i = 1,2,..,n$.

(c) (3) Find $E(Y_i)$ for $i = 1,2,..,n$.

### 2005S3
[2015S1Aa][]

Let X be a rv with finite mean $\mu$, finite variance $\sigma^2$, and assume $E(X^8)<\infty$. Prove or disprove:

(a) $E[(\frac{X-\mu}{\sigma})^2]\ge1$.

(b) $E[(\frac{X-\mu}{\sigma})^4]\ge1$.

### 2005S4
[@MGF] [@Cor] [@joint]

Let X andY have joint mgf
$M(t_1,t_2)=E(e^{t_1X + t_2Y})=e^{t_1^2+t_1t_2+2t_@^2}$
$-\infty<t_1,t_2<\infty$

(a) (10) State the formal name and the defining parameter values for this joint distribution.

(b) (5) Find the correlation between $X$ and $Y$; that is, $\rho(X, Y)$.

### 2005S5
[@suff]

Let the rv's $X_1,X_2,..,X_n$ form a random sample from a distribution with pdf denoted by $f(x|\theta)$. The unknown value of $\theta$ belongs to some parameter space $\Omega$; that is, $\theta\in\Omega\subset\mathbb R$. Define what we
mean when we say $T=T(X_1,X_2,..,X_n)$ is a sufficient statistic for the parameter $\theta$. That is, state the definition of a sufficient statistic for $\theta$.

### 2005S6

Let $X_1,X_2,..,X_n$ form a random sample from $N(\theta,\sigma^2)$, $-\infty<\theta<\infty$,$0<\sigma^2<\infty$. Argue that statistic $Z$ defined as $Z=\frac{\sum_{i=1}^{n-1}(X_{i+1}-X_{i})^2}{\sum_{i=1}^{n}(X_{i}-\bar X)^2}$
is independent from the sample mean $\bar X$ and the sample variance $S^2$

### 2005S7

Let X have pdf of the form $f(x|\theta)=1/\theta, 0<x<\theta$, zero elsewhere. Let $Y_1<Y_2,<Y_3<Y_4$ denote the order statistics of a random sample of size 4 from this distribution. Let the observed value
of $Y_4$ be $y_4$· We reject $H_0:\theta=1$ and accept $H_1:\theta\neq1$ if either $y_4\le1/2$or $y_4\ge1$.

(a) (6) Find the power function $K(\theta), 0<\theta$, of the test.

(b) (4) What is the signficance level (size) of the test?

### 2005S8
[2014SA3][] [2014SA5][] [2015S3A][] [2016S3][] [@SNorm] [@mean]

First,let $\Phi(.)$ and $\phi(.)$ denote the standard normal cdf and pdf respectively. Then, let $X_1,..,X_n$ denotes a random sample from a normal distribution with means $\theta$ and variance $\sigma^2$, and let $F(.)$ and $f(.)$ denote the common cdf and pdf of the r.s. respectively. Assume the sample size $n$ is odd; that is, $n=2k-1$; $k=1,2,3,..$ In this situation, the sample
median is the $k^{th}$ order statistic, denoted by $Y_k$.

(a) (5) Let $g(y)$ denote the pdf of the sample median $Y_k$. Derive $g(y)$. You may use the symbols $F(.)$ and $f(.)$.

(b) (3) Show that the pdf $g(y)$ is symmetric about $\theta$.

(c) (2) Find $E(Y_k)$·

(d) (5) Determine the $E(Y_k|\bar X)$, where $\bar X$ is the sample mean of the above random sample. Justify your answer.


### 2005S9
[2013FB5][] [2018FB4][] [@Unif] [@MLE] [@suff]

Suppose $X_1,X_2,..,X_n$ form a random sample from a uniform distribution over the interval $(\theta,\theta+1)$, where the value of the parameter $\theta$ is unknown $-\infty<theta<\infty$. The joint pdf $f_n(\underline{x}|\theta)$ of the random sample is expressed as follows:
$f_n(\underline{x}|\theta)=\begin{cases}1&\theta\le x_i\le\theta+1\\0& o.w.\end{cases}$

(a) Express the joint pdf in terms of the $\min(x_i)$ and $\max(x_i)$.

(b) Show that the statistics $\min(X_i)$ and $\max(X_i)$ are jointly sufficient statistics for $\theta$.

(c) If the MLE of $\theta$ exists, find it. Is it unique?



## 2007F

### 2007F1A
[2008S4A] [2013FB1] [@Norm] [@MLE] [@suff] [@consi] [@unbias]

Let $Y_1,Y_2,..,Y_{n}$ be a random sample from $N(\mu,\sigma^2)$ distribution and let $X_1,X_2,..,X_{m}$ be an independent random sample from $N(2\mu,\sigma^2)$ distribution.

(a) Find minimal sufficient statistics for $(\mu,\sigma^2)$

(b) Find maximum likelihood estimators of $\mu$ and $\sigma^2$

(c) Show that
$\hat\sigma^2=\frac{\sum_{i=1}^m(X_i-\bar X)^2+\sum_{j=1}^n(Y_j-\bar Y)^2}{m+n-2}$
is unbiased and consistent for estimating $\sigma^2$

### 2007F2A
[2008S5A][] [2009FA1][] [2014F4A][] [2015S2A][] [2019SA2][]

Suppose $Y_1$ and $Y_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:

$f(x)=\begin{cases}10e^{-10x}& x>0\\0& o.w.\end{cases}$

Find the p.d.f. of $X=Y_1-Y_2$.

### 2007F3A
[2003S3][] [2004F11][] [2008S2A][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Suppose $Y_1$ and $Y_2$ have the joint pdf
$f(y_1,y_2)=\begin{cases}2&0\le y1\le y2\le 1\\0& o.w.\end{cases}$

(a) Find the marginal density functions of $Y_1$ and $Y_2$ and check whether they are independent.

(b) Find $E[Y_1+Y_2]$

(c) Find $P(Y_1\le3/4|Y_2>1/3)$

### 2007F4A
[2015S4A][] [@CDF]

(a) Let $X$ be a continuous type random variable with cumulative distribution function $F(x)$. Find the distribution of the random variable $Y=\ln(1-F(X))$:

(b) Prove that for any $y\ge c$, the function $G_c(y)=P[X\le y|X\ge c]$ has the properties of a distribution function.

### 2007F5A
[2013FB2][] [2014F1B][] [2015S1B][] [@CDF] [@MLE]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find $\hat\theta$, the mle of $\theta$.

(b) Find $E[\hat\theta]$.

(c) Prove that $\hat\theta$ is consistent for $\theta$.

### 2007F1B
[2010SB3] [2010FB3][] [@Norm] [@MLE]  [@UMVUE]

Let $X_1,X_2,..,X_n$ be a random sample of size $m$ from $N(\theta,1)$ distribution. Find MLE and UMVUE of $\theta^2$.

### 2007F2B
[2014F5B][] [2017FB2][] [@Unif] [@HypoT] [@power]

Let $Y_1,Y_2,..,Y_{10}$ be a random sample from uniform distribution over $(0,\theta)$.
For testing $H_0:\theta=0$ against the alternative $H_a:\theta>1$, a reasonable test is to reject $H_0$ if $X_{(n)}=\max\{X_1,X_2,..,X_{10}\}\ge C$. Find $C$ so that type I error probability is .05. Also find the power of the above test at $\theta=1.5$.

### 2007F3B
[2010SB1][] [2010FB1][] [2011S5][] [2013FB3][] [2015S2B][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 90th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2007F4B
[2004F8][] [2013FB4][] [2015S3B][] [2018S1B][] [2019SB2][] [@Laplace] [@MLE]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2007F5B
[2003S9][] [2004F9][] [2015S4B][] [2018S3B][] [2019SB3][] [@SPower] [@power] [@UMP]

Suppose $X_1,X_2,..,X_n$ is a random sample from a distribution with pdf

$f(x,\theta)=\begin{cases}\theta x^{\theta-1}& 0<x<1\\0& o.w.\end{cases}$

Suppose that the value of $\theta$ is unknown and it is desired to test the following hypotheses :

$H_0:\theta=1\quad H_1 :\theta>1$

Derive the UMP test of size $\alpha$ and obtain the null distribution of your test statistic.

## 2008S

### 2008S1A
[2003S1][]

A box contains 2 red balls, 2 white balls, and 3 blue balls. If 5 balls are selected at random without replacement, what is the probability that only one color is missing from the selection?

### 2008S2A
[2003S3][] [2004F11][] [2007F3A][] [2009FA2][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}c(1-y_2)&0\le y_1\le y_2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c.

(b) Find the marginal density functions of $Y_1$ and $Y_2$.

(c) Find $P(Y_2\le1/2|Y_1\le3/4)$

### 2008S3A
[2008F1][] [2016F4][] [@Unif] [@CDF] [@PDF]

Let $(Y_1,Y_2)$ denote a random sample of size $n=2$ from the uniform distribution on the interval $(0, 1)$. Find the probability density and cumulative distribution functions of $U=Y_1+Y_2$..

### 2008S4A
[2007F1A][] [2013FB1][] [@Norm] [@unbias] [@consi]

Let $Y_1,Y_2,..,Y_{n}$ be a random sample of size $n$ from a normal population with mean $\mu$ and variance $\sigma^2$. Assuming $n=2k$ for some integer $k$, one possible estimator for $\sigma^2$ is given by:
$\hat\sigma^2=\frac1{2k}\sum_{i=1}^k(Y_{2i}-Y_{2i-1})^2$

(a) Show that $\hat\sigma^2$ is an unbiased estimator for $\sigma^2$

(b) Show that $\hat\sigma^2$ is a consistent estimator for $\sigma^2$

### 2008S5A
[2007F2A][] [2009FA1][] [2014F4A][] [2015S2A][] [2019SA2][]

The lifetime (in hours) Y of an electronic component is a random variable with density function
$f(y)=\begin{cases}\frac1{300}e^{-\frac1{300}y}& y>0\\0& o.w.\end{cases}$

(a) What is the probability that a randomly selected component will operate for at least 300 hours?

(b) Five of these components operate independently in a piece of equipment. The equipment fails if at least three of the components fail.

Find the probability that the equipment will operate for at least 300 hours without failure?

### 2008S1B
[2009FA4][] [2015F1][] [@Unif] [@mean] [@Var] [@suff] [@UMVUE] 

Let $X_1,X_2,..,X_{n}$ be a random sample of size $n$ from a uniform distribution over the interval $[-\theta/2,\theta/2], \theta>0$ being unknown.

(a) Prove tbat $T=\max_{1\le i\le n}|X_{i}|$ is complete and sufficient for $\theta$.

(b) Find the UMVU estimator of $\theta$.

### 2008S2B
[2014F2B][] [@Pois] [@FishI]

Let $X_1,X_2,..,X_{n}$ be a random sample from Poisson distribution with parameter $\lambda(>0)$.

(a) Find the Fisher's information in the sample about the parameter $\lambda$.

(b) Suppose we want to estimate $P[X_1=0]=e^{-\lambda}$. Find a lower bound on the variance of any unbiased estimator of this parametric function.

### 2008S3B
[@consi]

Let $X_1,X_2,..,X_{n}$ be a random sample from a distribution with probability density function
$f_{\theta_1}(x)=\begin{cases}\frac1{\theta_1} e^{-\frac{x}{\theta_1}}& x>0\\0& o.w.\end{cases}$
and $Y_1,Y_2,..,Y_{n}$ an independent random sample from
$f_{\theta_2}(x)=\begin{cases}\frac1{\theta_2} e^{-\frac{x}{\theta_2}}& x>0\\0& o.w.\end{cases}$

(a) Find $p_{\theta_1,\theta_2}= P[X_1\le Y_1]$.

(b) Find the MLE, $\hat p_n$, of $p_{\theta_1,\theta_2}= P[X_1\le Y_1]$.

(c) Show that $\hat p_n$ is a consistent estimator of $p_{\theta_1,\theta_2}$.

### 2008S4B
[2014SB2][] [@Unif]

Let $X_1,X_2,..,X_{10}$ be independent random variables such that $X$, has $U(0,i\theta)$ distribution for $i= 1,2,..,10$. Based on these 10 observations, find the maximum likelihood estimator of $\theta$ and find its bias.

### 2008S5B
[2017FB3][] [@Norm] [@MLR] [@UMP] [@power] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample of size m from $N(\theta,1)$ distribution and
let $Y_1,..,Y_m$ be an independent random sample of size $m$ from $N(3\theta,1)$.

(a) Show that the joint distribution of X's and Y's has @MLR (monotone likelihood ratio) property.

(b) Find the UMP test of size $\alpha$ for testing $H_0:\theta\le0$ vs $H_1:\theta>0$.

(c) Find an expression of the power function of the UMP test.


## 2008F
Fountain

### 2008F1
[2008S3A][] [2016F4][] [@Unif] [@CDF] [@PDF]

Let $Y_1$ and $Y_2$ be a random sample of size 2 from $Uniform(0,1)$. Find the cumulative distribution and probability density functions of $U=Y_1+Y_2$.

### 2008F2
[2010SA1][] [2014F2A][]

Only 5 in 1000 adults are afflicted with a rare disease for which a diagnostic test has been developed. The test is such that when an individual actually has the disease, a positive result will occur 99% of the time, whereas an individual without the disease will show a positive result only 2% of the time. If a randomly selected individual is tested and the result is positive, what is the probability that the individual has the disease? A man committed a suicide in a week after learning from his doctor that he has a terminal cancer. What do you think of his reaction based on your answer to this problem?

### 2008F3
[2016F3][] [@Cheb]

If X is a random variable such that $E[X]=3$ and $E[X^2]=13$, determine a lower bound for
the probability $P(-2<X<8)$. (Hint: Use a famous inequality.)

### 2008F4
[@LimD]

Let $Y_1$ be the minimum of a random sample of size n from a distribution that has p.d.f.
$f(x)=e^{-(x-\theta)},\theta<x<\infty$, zero elsewhere. Let $Z_n= n(Y_1-\theta)$. Determine the limiting distribution of $Z_n$. (Hint: Determine the p.d.f. of Y, and then apply the change of variable technique.)

### 2008F5
[2003S7][] [2009SB1][] [2009FB4][] [2016S4][] [2016F7][] [2017FB4][] [2018FB2][] [2019SB4][] [@MOM] [@MLE] [@MSE] [@CRLB]

Let $X_1,X_2,..,X_n$$\sim$i.i.d.$f(x;\theta)=\theta(x+1)^{-\theta-1},\ x>0,\theta>2$

a. Find $\hat\theta_{MOM}$, the method of moments estimator of $\theta$.

b. Find $\hat\theta_{MLE}$, the maximum likelihood estimator of $\theta$.

c. Find the MSE (mean squared error) of $\hat\theta_{MLE}$.

d. Using $\hat\theta_{MLE}$, create an unbiased estimator $\hat\theta_{U}$.

e. Find the efficiency of $\hat\theta_{U}$.

f. Construct the most powerful test of $H_0:\theta=3$ vs. $H_1:\theta=4$.

### 2008F6
[2003S10][] [2016F5][] [@Norm] [@indep] [@Basu] 

Let $Y_n$ be the $n^{th}$ order statistic of a random sample of size n from the normal distribution $N(\theta,\sigma^2)$. Prove that $Y_n-\bar Y$ and $\bar Y$ are independent.

562-2

### 2008F7
[2016F6][] [@Expo] [@BayesE]

Suppose that $X_1,X_2,..,X_n$ i.i.d. $Exponential(\theta)$, i.e. $f(x;\theta)=\theta e^{-\theta x},x>0$. Also assume that the prior distribution of $\theta$ is $h(\theta)=\lambda e^{-\lambda\theta},\theta>0$. Find the Bayes estimator of $\theta$, assuming squared error loss.

## 2009S
unkown, Fountain

### 2009SA1
[@joint] [@marg]

Suppose random variables X and Y have a joint probability mass function
$p(x,y)=\begin{cases}\frac{x+y+1}{30}& x,y=0,1,2,..,x+y\le3\\0& o.w.\end{cases}$
Determine the marginal probability mass function of Y.

### 2009SA2
[@Pois]

Suppose a random variable $X$ has a probability mass function $p(x)=\frac{e^{-\mu}\mu^x}{x!},x=0,1,2,..$,zero, elsewhere. Find the values of $\mu$, so that $x=1$ is the unique mode.

### 2009SA3
[@Pois]

Let $X_1,X_2,..,X_n$ be the independent $Poisson(m_i)$ random variables. Show that $Y=\sum_{i=1}^n X_i$ has $Poisson(\sum_{i=1}^n m_i)$.

### 2009SA4
[@Cor] [@Cheb]

Let $\sigma_1^2=\sigma_2^2=\sigma^2$ be the common variance, $\rho$ the correlation coefficient, $\mu_1$ and $\mu_2$ the means of $X_1$ and $X_2$ , respectively. Show that

$P[|(X_1-\mu_1)+(X_2-\mu_2)|\ge k\sigma]\le\frac{2(1+p)}{k^2}$

### 2009SA5
[@Expo]

Let Xn have a probability density function
$f(x;n)=\begin{cases}ne^{-nx}& 0<x<\infty\\0& o.w.\end{cases}$
Find the limiting distribution of $Y_n=X_n/n$.

### 2009SB1
[2003S7][] [2008F5][] [2009FB4][] [2016S4][] [2016F7][] [2017FB4][] [2018FB2][] [2019SB4][] [@MOM] [@MLE] [@MSE] [@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample of size n from the following distribution:

$f(x;\theta)=(\theta+1)x^\theta,\ 0\le x\le 1,\theta>-1$

(a) Find $\hat\theta_{MOM}$, the method of moments estimator for $\theta$.

(b) Find $\hat\theta_{MLE}$, the maximum likelihood estimator for $\theta$.

(c) Using $\hat\theta_{MLE}$, create an unbiased estimator $\hat\theta_{U}$.

(d) Find the Cramer-Rao lower bound on the variance of an unbiased estimator of $\theta$.

(e) Construct the most powerful test of $H_0:\theta=0$ vs. $H_1:\theta=1$, showing as much detail as possible.

### 2009SB2
[2004F5][] [@Norm] [@indep]
 
Let $X_1,X_2,..,X_5$ be a random sample of size 5 from the normal distribution $N(0,\sigma^2)$. Prove
that $R=(X_1^2+ X_2^2)/(X_1^2+ X_2^2+X_3^2+ X_4^2+X_5^2)$ and $D=X_1^2+ X_2^2+X_3^2+ X_4^2+X_5^2$ are independent.

### 2009SB3
[@Pois] [@Gamma] [@BayesE]

Suppose thfrt $X_1,X_2,..,X_{n}$ have i.i.d. $Poisson(\theta)$. Also assume that the prior distribution
of is e is $Gamma(\alpha,\beta)$. Find the Bayes estimator of $\theta$, assuming squared-error loss.

## 2009F

### 2009FA1
[2007F2A][] [2008S5A][] [2014F4A][] [2015S2A][] [2019SA2][]

The lifetime (in hours) Y of an electronic component is a random variable with density function
$f(y)=\begin{cases}\frac1{200}e^{-\frac1{200}y}& y>0\\0& o.w.\end{cases}$

(a) What is the probability that a randomly selected component will operate for at least 400 hours?

(b) What is the probability that the lifetime of a randomly selected component will exceed its mean lifetime by more than two standard deviations?

(c) Four of these components operate independently in a piece of equipment. The equipment fails if at least three of the components fail. Find the probability that the equipment will operate for at least 400 hours without failure?

### 2009FA2
[2003S3][] [2004F11][] [2007F3A][] [2008S2A][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Suppose $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}c&0\le y_1\le y_2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c.

(b) Find the marginal density functions of $Y_1$ and $Y_2$ and check whether they are independent.

(c) Find $P(Y_1\le1|Y_2>1)$

### 2009FA3
[2015S5B][] [2019SA4][] [@MOM] [@Pois]

Let $Y_1,Y_2,..,Y_{12}$ be a random sample from a Poisson distribution with mean $\lambda$.

(a) (4 Ppts) Use the method of moment generating functions to find the distribution of $S_{12}=\sum^{12}_{i=1} Y_i$.

(b) (6 pts) Let $S_4=\sum_{i=1}^4Y_i$ Find the conditional distribution of $S_4$ given $S_{12}=s$.


### 2009FA4
[2008S1B][] [2015F1][] [@Unif] [@PDF] [@mean] [@Var] [@consi]

Suppose $X_1,X_2,..,X_{n}$ is a random sample from a unform distribution over $[1,\theta]$, where $\theta>1$. Let $Y_{n}=\max\{X_1,X_2,..,X_{n}\}$

(a) (3 pts) Find the probability density-function of $Y_{n}$.

(b) (4 pts) Find the mean and the variance of $Y_{n}$.

(c) (3 pts) Examine whether $Y_{n}$ is a consistent estimator of $\theta$.


### 2009FB1
[2019SB1][]

Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution $N(\mu,\sigma^2=25)$. Reject $H_0:\mu=50$ and accept $H_1:\mu=55$ if $\bar X_n\ge c$. Find the two equations in n and c that you would solve to get $P(\bar X_n\ge c|\mu)=K(\mu)$ to be equal to $K(50)=0.05$ and $K(55)=0.90$.
Solve these two equations. Round up if n is not an integer. Hint: $z_{.05}=1.645$ and $z_{.1}=1.28$

### 2009FB2
[@MLE] [@LRT]

The Pareto distribution is a frequently used model in study of incomes and has the distribution
function
$F(x;\theta_1,\theta_2)=\begin{cases}1-(\theta_1/x)^{\theta_2}& \theta_1<x\\0& o.w.\end{cases}$
where $\theta_1>0$ and $\theta_2>0$.

(a) (4 pts) Let $X_1,X_2,..,X_n$ be a random sample from this distribution. Find the MLEs of $\theta_1$ and $\theta_2$.

(b) (3 pts) Find the likelihood ratio test for testing $H_0:\theta_1=1$ against $H_1:\theta_1\neq1$.

(c) (3 pts) Using $\alpha=.05$, find out the critical value for your test. Hint: $\chi^2_{1,.025}= 5.024$;$\chi^2_{1,.05}= 3.841$; $\chi^2_{1,.975}=.001$; $\chi^2_{1,.95}=.004$; $\chi^2_{2,.025}=7.378$; $\chi^2_{2,.05}= 5.991$; $\chi^2_{2,.975}=.051$;$\chi^2_{2,.95}=.103$

### 2009FB3
[@UMVUE] [@Expo] [@indep]

Let $X_1,X_2$ denote a random sample of size $n=2$ from a distribution with pdf
$f(x;\theta)=\begin{cases}\frac1\theta e^{-\frac{x}\theta}& 0<x<\infty\\0& o.w.\end{cases}$
where $0<\theta<\infty$ is an unknown parameter.

(a) (5 pts) Show that $Y_1=X_1+X_2$ is independent of $X_1/X_2$.

(b) (5 pts) Find the UMVUE of $\theta^2$

### 2009FB4
[2003S7][] [2008F5][] [2009SB1][] [2016S4][] [2016F7][] [2017FB4][] [2018FB2][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$ of $\theta$.

(c) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

(d) (2 pts) What is the asymptotic distribution of $\hat\theta$?


## 2010S

### 2010SA1
[2008F2][] [2014F2A][]

Only 1 in 1000 adults is afflicted with a rare disease for which a diagnostic test has been developed. The test is such that when an individual actually has the disease, a positive result will occur 99% of the time, whereas an individual without the disease will show a positive result only 2% of the time (false positive). If a randomly selected individual is tested and the result is positive, what is the probability that the individual has the disease?

### 2010SA2

Let $X_1$ and $X_2$ be a random sample of size 2 from the following pdf

$f(x,\beta)=\begin{cases} \frac1{2\beta^3}x^2e^{-x/\beta}& x\ge0\\0& o.w.\end{cases}$

(a) Compute the expected value of $X_1/X_2$

(b) Compute the variance of $X_1/X_2$

### 2010SA3
[2011S4][] [2018FA4][] [@Pois] [@LimD]

Let $X_1,X_2,..,X_n$ be a random sample from $Poisson(\mu)$. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

### 2010SA4
[2003S3][] [2004F11][] [2007F3A][] [2008S2A][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $X$ and $Y$ have the following joint pdf:
$f(x,y) =\begin{cases}6(y-x)& 0<x<y<1\\0& o.w.\end{cases}$
Define $Z=(X+Y)=2$ and $W=Y$, respectively.

(a) Find the joint pdf of Z and W.

(b) Find the marginal pdf of Z.

### 2010SB1
[2007F3B][] [2010FB1][] [2011S5][] [2013FB3][] [2015S2B][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_{20}$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 75th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.


### 2010SB2
[@power]

Let $f_1(x)=\begin{cases}1&0<x\le1\\0& o.w.\end{cases}$
and
$f_2(x)=\begin{cases}4x&0<x\le1/2\\4(1-x)& 1/2<x\le1\\0& o.w.\end{cases}$

Based on a single observation $X$, derive the most powerful level $\alpha=0.1$ test for testing $H_0:X\sim f_1$ against the alternative $H_2:X\sim f_2$. Also find the power of your test.

### 2010SB3
[2007F1B] [2010FB3][] [@Norm] [@MLE]  [@UMVUE]

Let $X_1,X_2,..,X_n$ be a random sample from $N(1,\sigma^2)$ distribution.

(a) Find the MLE of $\sigma^2$

(b) Is it an unbiased estimator of $\sigma^2$? Justify your answer.

(c) Is it a UMVUE of $\sigma^2$? Justify your answer.

### 2010SB4
[2004F12][] [2010FB4][] [2011S6][] [2015F5][] [2018S4B][] [@Expo] [@LRT] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample from the exponential distribution with mean $\theta_1$ and let $Y_1,Y_2,..,Y_{n}$ be an independent random sample from another exponential distribution with mean $\theta_2$. 

(a) Find the likelihood ratio test for testing $H0:\theta_1=\theta_2$ vs $H_a:\theta_1\neq\theta_2$

(b) Show that the test in (a) is equivalent to to an exact F test. (Hint : Transform $\sum X_i$; and
$\sum Y_i$ to $\chi^2$ random variables).

## 2010F

### 2010FA1
[2013FA2] [@Unif] [@mean] [@Cov] [@Cor]

Suppose $X$ is $uniform[0,1]$. Assume Y, given $X=x$, is $uniform[0,x]$ Find the joint pdf of $X$ and $Y$. Find the mean and variance of $X$ and $Y$. Find the covariance and correlation of $X$ and Y.

### 2010FA2 
[2013FA3] [@Unif] [@mean] [@LimD]

Let $X_1,X_2,..,X_{n}$ be iid uniform[0,1] random variables, and define $Y_1=\min{X_1,X_2,..,X_{n}}$. Find the cdf of $Y_1$. Suppose $W_1=nY_1$. Note that $0<Y_1<1$, but $0<W_1<n$. Find the limiting distribution of $W_1$ as $n\to\infty$. 


### 2010FA3
[@Norm] [@mean] [@Var]

Suppose $X$ is $N(\mu,\sigma^2)$. Define $Y=e^X$. Find the mean and variance of $Y$.

### 2010FA4
[@Pois] [@MGF]

Assume that $X_i$ is $Poisson(\mu_i),i=1,..,n$. If the $X_i$'s are independent, use moment generating
functions to show that $\sum_{i=1}^n X_i$ is also Poisson. Do you think $\sum_{i=1}^n iX_i$ is Poisson?

### 2010FB1
[2007F3B][] [2010SB1][] [2011S5][] [2013FB3][] [2015S2B][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_{20}$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 75th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2010FB2
[2015F4] [@Beta] [@HypoT] [@power]

Let X1 be a random sample of size $n=1$ from the Beta distribution with pdf
$f(x|\theta)=\begin{cases}\frac{\Gamma(2\theta)}{\Gamma(\theta)\Gamma(\theta)}x^{\theta-1}(1-x)^{\theta-1}&0<x<1\\0&o.w.\end{cases}$

Suppose a researcher is interested in testing $H_0:\theta=1$ against $H_1:\theta=2$. The researcher decides to reject $H_0$ in favor of $H_1$ if $X_1<2/3$.

(a) Find the size of the test

(b) Compute the power of the test at $\theta=2$.

### 2010FB3
[2007F1B] [2010SB3][] [@Norm] [@MLE]  [@UMVUE]

Let $X_1,X_2,..,X_n$ be a random sample from $N(1,\sigma^2)$ distribution.

(a) Find the MLE of $\sigma^2$

(b) Is it an unbiased estimator of $\sigma^2$? Justify your answer.

(c) Is it a UMVUE of $\sigma^2$? Justify your answer.

### 2010FB4
[2004F12][] [2010SB4][] [2011S6][] [2015F5][] [2018S4B][] [@Expo] [@LRT] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample from the exponential distribution with mean $\theta_1$ and let $Y_1,Y_2,..,Y_{n}$ be an independent random sample from another exponential distribution with mean $\theta_2$. 

(a) Find the likelihood ratio test for testing $H0:\theta_1=\theta_2$ vs $H_a:\theta_1\neq\theta_2$

(b) Show that the test in (a) is equivalent to to an exact F test. (Hint : Transform $\sum X_i$; and
$\sum Y_i$ to $\chi^2$ random variables).

## 2011S

### 2011S1
[@Cor] [@Cov] [@Var]

Let $U$ and $V$ be r.v.'s such that $Var(U+V)=30$ and $Var(U-V)=10$.

(a) Find $Cov(U,V)$.

(b) If additionally, we know $Var(U)=Var(V)$, find the correlation of $U$ and $V$.

### 2011S2
[@Unif]

Let $X_1,X_2,..,X_{n}$, be iid $Uniform[0,\theta]$ r.v. 's.

(a) Find an unbiased estimator of $\theta$.

(b) Finri the minimum variance unbiased estimator of $\theta$.

(c) Find an unbiased estimator of $\theta^2$

(d) Find the minimum variance unbiased estimator of $\theta^2$

### 2011S3 
[@Unif] [@Weib] [@trans] 

Let $f(x)=2xe^{-x^2},0<x<\infty$, and zero elsewhere.

(a) Show $f(x)$ is a probability density function.

(b) If $X$ has pdf $f(x)$, find $E(X)$.

(c) If $X$ has pdx $f(x)$, find $E(X^2)$.

### 2011S4
[2010SA3][] [2018FA4][] [@Pois] [@LimD]

Let $X_1,X_2,..,X_n$ be a random sample from $Poisson(\mu)$. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

### 2011S5
[2007F3B][] [2010SB1][] [2010FB1][] [2013FB3][] [2015S2B][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_{20}$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 75th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2011S6
[2004F12][] [2010SB4][] [2010FB4][] [2015F5][] [2018S4B][] [@Expo] [@LRT] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample from the exponential distribution with mean $\theta_1$ and let $Y_1,Y_2,..,Y_{n}$ be an independent random sample from another exponential distribution with mean $\theta_2$. 

(a) Find the likelihood ratio test for testing $H0:\theta_1=\theta_2$ vs $H_a:\theta_1\neq\theta_2$

(b) Show that the test in (a) is equivalent to to an exact F test. (Hint : Transform $\sum X_i$; and
$\sum Y_i$ to $\chi^2$ random variables).

## 2011F

### 2011F1
[@Norm] [@mean]

Let $X$ be a $N(0,\sigma^2)$ random variable. Find $E(X^4)$.

### 2011F2
[@Gamma]

Let $(X,Y)$ have bivariate density $f(x,y)=\frac{\Gamma(\alpha+\beta+\gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}x^{\alpha-1}y^{\beta-1}(1-x-y)^{\gamma-1},0<x<1,0<yx<1,0<x+y<1$ for parameters $\alpha>0,\beta>0,\gamma>0$. Determine

(a) the conditional density of $Y$ given $X=.5$,

(b) the density of $Y/.5$ given $X=.5$,

(c) the marginal density of $X$.

### 2011F3
[@Unif] [@Weib] [@trans] 

Determine the transformation $g$ that will make $X=g(U)$ have the Weibull density $f(x)=2xe^{-x^2},x>0$, where $U$ is a $unifonn(0,1)$ random variable.

### 2011F4
[@Norm] [@t] [@MGF]

Suppose $X_1,X_2,..,X_{n}$ is a random sample of $N(\mu,\sigma^2)$ random variables. Find the moment generating function $M(t)=E(e^{tT}),t\in\mathbb R$, where $T=\frac{\bar X-\mu}{S/\sqrt{n}}$ is the usual t-statistic.

### 2011F5
[@Norm] [@Cor] [@Cov] [@Var]

Suppose $X_1,X_2,..,X_{n}$ is a random sample of $N(\mu,\sigma^2)$ random variables.

(a) Find the correlation of $\bar X$ and $S^2$, the sample mean and sample variance.

(b) Find the variance of $S^2$.

(c) Compute the covariance of $X_1$ and $\bar X$.

### 2011F6
[2003F9][] [2011F6][] [@Pois] [@UMP] [@HypoT]

Let $X_1,X_2,X_3$ be iid $Poisson(\lambda)$ random variables. Find a UMP (uniformly most powerful)
test of $H_0:\lambda\ge1$ versus $H_1:\lambda<1$ at a level $\alpha$ near .05.

### 2011F7
[@Pois] [@MLE] [@MSE]

Let $X_1,..,X_{n}$ be lid $Poisson(\lambda)$ random variables.

(a) Find the best unbiased estimator of $e^{-\lambda}$, the probability that $X=0$.

(b) Find the MLE (maximum likelihood estimator) for $e^{-\lambda}$.

(c) Compute the MSE (mean-squared error) for the MLE as a function of $\lambda$,

### 2011F8
[@Bino] [@CRLB] [@MLE]

Let $X$ have tbe binomial distribution $bin(n,p)$. Find the Cramer-Rao lower bound on the variance of an unbiased estimator for $p$, and compare it to the variance of the MLE for $p$.

## 2013S

### 2013S1
[@Norm]

Let $X_1,X_2,..,X_{n}$ be iid $N(\mu,\sigma^2)$ random variables. If we have convergence in distribution $\sqrt{n}(S^2-\sigma^2)\to N(0,2\sigma^4)$ for the sample variance $S^2$, use it to get a normal approximation for the distribution of $S$.

### 2013S2
[2014F3A][]

Let $(X,Y)$ have bivariate density $f(x,y)=e^{-x},0<y<x$. Determine

(a) the marginal density of $X$,

(b) the conditional density of $Y$ given $X=x$.

### 2013S3
[@Pois] [@MLE] [@MOM]

Let X have the $Poisson(\lambda)$ distribution. Find the Cramer-Rao lower bound on the variance of an unbiased estimator for $\lambda$ and compare it to the variance of the Method of Moments estimator for $\lambda$. 

### 2013S4

Find a transformation $g$ that will make $X=g(U)$ have the $\chi^2(2)$ density where $U$ is a $uniform(0,1)$ random variable (useful for the Box-Mueller method of simulating standard normal random variables). 

### 2013S5
[@Norm] [@MLE]

Suppose $X_1,X_2,..,X_{n}$ is a random sample of $N(\mu,\sigma^2)$ random variables. Find the
mean-squared error of the MLE for $\sigma^2$ and the mean-squared error of its best unbiased estimator. 

### 2013S6
[@Norm]

Suppose $X_1,X_2,..,X_{n}$ is a random sample of $N(\mu,\sigma^2)$ random variables.

(a) Find the exact distribution of $\bar X$.

(b) Compute the covariance of $X_1-\bar X$ and $X$.

### 2013S7
[@Bino] [@UMP] 

Let $X_1,X_2$ be two iid $bin(5,p)$ random variables. Find a UMP (uniformly most powerful) test of $H_0: p\le 0.5$ versus $H_1: p>.5$ at a level $\alpha$ near .01.

### 2013S8
[@Gamma] [@MLE]

Let $X_1,X_2,..,X_{n}$ be iid $Gamma(\alpha=1,\beta)$ random variables. Find the expectation of
the MLE $1/\bar X$ for the rate $\lambda=1/\beta$ and say whether it is greater than or less than $\lambda$


## 2013F

### 2013FA1

Assume an urn contains R red and B blue marbles. Marbles are drawn from the urn, one at a time and without replacement, until all the marbles have been drawn.

(a) What is the probability that the first marble drawn is red?

(b) What is the probability that the second marble is red?

(c) What is the probability that the last marble is red?

(d) What is the probability that the first ai:td last marbles are red?

### 2013FA2
[2010FA1] [@Unif] [@mean] 

Let $X$ be a $uniform[0,1]$ random variable. Let $Y$, given $X$, be $uniform[0,X]$.

(a) What are the mean and variance of $X$?

(b) What are the mean and variance of $Y$?

(c) What is the joint pdf $f(x,y)$ of $X$ and $Y$?


### 2013FA3
[2010FA2] [@Unif] [@mean] [@asym]

Suppose $U_1,U_2,..,U_n$ are iid $uniform[0,\theta]$ random variables, where $0<\theta<\infty$. Let $W_n=n\times\min\{U_1,U_2,..,U_n\}$, so that $0\le W_n\le n\times\theta$.
Let $H_n(w)=P(W_n\le w)$ be the cdf of $W_n$, and let $h_n(w)$ be the pdf of $W_n$

(a) Find the limit $H(w)$ of $H_n(w)$ as $n\to\infty$. Is it a cdf of a random variable?

(b) Find the limit $h(w)$ of $h_n(w)$ as $n\to\infty$.

(c) What is the asymptotic distribution of $W_n$?

(d) What is the mean, $E(W_n)$, of $W_n$?

### 2013FA4
[@Bino]

Suppose $X$ has a negative binomial distribution, with pdf.
$P(X=x)=\binom{x-1}{r-1})p^rq^{x-r},x=r,r+1,r+2,...$,
where $p+ q=1$, and $r$ is a fixed positive integer, namely the required number of successes to stop.

(a) Find the mean $E[X]$ of $X$.

(b) Find the variance $Var[X]$ of $X$.

### 2013FA5

Let X and Y be two continuous type independent random variables with distribution functions $F$ and $G$, respectively. Find

(a) the pdf of $V=F(X)+G(Y)$,

(b) the pdf of $W=\min\{F(X),G(Y)\}$.

### 2013FB1
[2007F1A] [2008S4A] [@Norm] [@MLE] [@suff] [@consi] [@unbias]

Let $Y_1,Y_2,..,Y_{n}$ be a random sample from $N(\mu,\sigma^2)$ distribution and let $X_1,X_2,..,X_{m}$ be an independent random sample from $N(2\mu,\sigma^2)$ distribution.

(a) Find minimal sufficient statistics for $(\mu,\sigma^2)$

(b) Find maximum likelihood estimators of $\mu$ and $\sigma^2$

(c) Show that
$\hat\sigma^2=\frac{\sum_{i=1}^m(X_i-\bar X)^2+\sum_{j=1}^n(Y_j-\bar Y)^2}{m+n-2}$
is unbiased and consistent for estimating $\sigma^2$

### 2013FB2
[2007F5A][] [2014F1B][] [2015S1B][] [@CDF] [@MLE]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find $\hat\theta$, the mle of $\theta$.

(b) Find $E[\hat\theta]$.

(c) Prove that $\hat\theta$ is consistent for $\theta$.

### 2013FB3
[2007F3B][] [2010SB1][] [2010FB1][] [2011S5][] [2015S2B][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 90th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2013FB4
[2004F8][] [2007F4B][] [2015S3B][] [2018S1B][] [2019SB2][] [@Laplace] [@MLE] [@suff]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias. Is the m.l.e. a sufficient statistic?

### 2013FB5
[2018FB4] [@Unif] [@HypoT]

Let $X_1$ and $X_2$ be two independent random variables each having uniform distribution on the interval $(\theta,\theta+1)$. For testing $H_0:\theta=0$ against $H_a:\theta> 0$, we have two competing tests :

1. Test 1 : Reject $H_0 if X_1>0.95$

2. Test 2 : Reject $H_0 if X_1+X_2>c$.

Find the value of c so that the Test 2 has the same value of Type I error probability as Test 1.

## 2014S

Crain, Kochar

### 2014SA1
[@Pois]

Let $X$ given $\lambda$ be $Poissou(\lambda)$. Suppose $\lambda$ is a random variable whkh has Poisson distribution with parameter $\mu$. Find $E[X]$ and $Var[X]$.

### 2014SA2
[2003F8][] [2015F2][] [2017FA3][]  [@Expo] [@Basu] [@indep]

Assume that $X_1$ and $X_2$ have joint pdf $f(x_1,x_2)=exp(-x_1 ).exp(-x_2)$ for $0\le x_1,x_2<\infty$ and zero elsewhere. Define $Y_1=X_1/(X_1+X_2)$, $Y_2=X_1+X_2$ Use Basu's theorem to demonstrate that $Y_1$ and $Y_2$ are independent. Identify the marginal pdfs of $Y_1$ and $Y_2$ Find $E[X_1^3/(X_1+X_2)^2]$

### 2014SA3
[2005S8][] [2014SA5][] [2015S3A][] [2016S3][] [@SNorm] [@mean]

Suppose $Z$ is a standard normal random variable with cdf $\Phi(.)$. Evaluate $E[\Phi(Z)]$ and $E[\Phi^{2}(Z)]$.

### 2014SA4
[@Unif] [@mean]

Let $U_1,U_2,..,U_n$ be iid $uniform[0,1]$ random variables. Let $0\le Y_1<Y_2<..<Y_n$ be the corresponding order statistics, ie, $Y_k$ is the $k^{th}$ smallest of the $U_i$ What is the joint pdf of $Y_1,Y_2,..,Y_n$? Find the marginal pdf of $Y_k$, where $1\le k\le n$. Find the mean and variance of $Y_k$.

### 2014SA5
[2005S8][] [2014SA3][] [2015S3A][] [2016S3][] [@SNorm] [@mean]

Assume that $Z$ is a standard normal or $N(0,1)$ random variable. Find a formula for $E[Z^k]$ where $k$ is a positive integer.

### 2014SB1
[@Expo] [@CDF]

The lifetime (in hours) X of an electronic component is tt random variable with cumulative distribution functionfundion

$F(y)=\begin{cases}1-e^{-y/5}& y>0\\0& o.w.\end{cases}$

(a) What is the probability that a randomly selected component will operate for at least 10 hours?

(b) What is the probability that the lifetime of a randmnly selected component wi11 exceed its mean lifetime by more than two standard deviations?

(c) Three of these components operate independently in a piece of equipment. The equipment fails if at least two of the components fail. Find the probability that the equipment will operate for at least 10 hours without failure?

### 2014SB2
[2008S4B] [@Unif]

Let $X_1,X_2,..,X_{10}$ be random variables denoting 10 independent bids for an item that is for sale. Suppose that each $X_i$ is uniformly distributed on the interval $[\theta-50,\theta+50]$, where $\theta>100$. The seller sells to the highest bidder, how much can he expect to earn on the sale? 

### 2014SB3
[2014F4B] [@Norm] [@MLE]

Let $X_1,X_2,..,X_{n}$ be a random sample from a normal distribution, $N(\mu,\sigma^2)$, where $-\infty<\mu<+\infty$ and $\sigma>0$. Find the MLE of $\mu/\sigma$ and find itsexpected value.

### 2014SB4
[@Beta] [@CRLB]

Suppose $X_1,X_2,..,X_{n}$ is a random sample from a population with probability mass function

$p_\theta(X=x)=\theta^{x}(1-\theta)^{1-x},x=0,1; 0<\theta<1$

(a) Find the maxirnmn likelihood estimator of $Var_\theta(X) =\theta(1-\theta)$.

(h) Find the the Cramer-Rao lower bound for the variance of any unbiased estimator of $\theta(1-\theta)$.

### 2014SB5
[@Bino] [@Unif] [@BayesE] [@MLE]

Suppose X has Binomial distribution with pararneters $n$ and $\theta, 0<\theta<1$.

(a) Find the Bayes estimator of $\theta$ when the prior distribution is uniform on the interval $(0,1)$ and the loss function is square error loss function.

(b) Compare the risk of the above Bayes estimator with that of the MLE of $\theta$.

## 2014F

### 2014F1A
[@Bern] [@MGF]

Repeat a sequence of i.i.d. Bernoulli trials until you observe the frst success, where p = the probability of a success and $q=1-p=$ the probability of a failure on any one trial. Let the random variable Y count the number of failures before the frst success.

(a) State the name of this statistical experiment.

(b) Provide a mathematical formula for the probability mass function, $P(Y=y)$ where $y=$?.

(c) Give in closed form the $P(Y\ge y)$.

(d) Determine the $E(Y)$.

(e) Derive the moment generating function (M.G.F.) of Y . Remember to state the interval over which this M.G.F. exists.

### 2014F2A
[2008F2] [2010SA1][]

One percent of all individuals in a certain population are carriers of a particular disease. A diagnostic test for this disease has a 90% detection rate for carriers and a 5% detection rates for noncarriers. Suppose the test is applied independently to two different blood samples from the same randomly selected individual.

(a) What is the probability that both tests yield the same result?

(b) If both tests are positive, what is the probability that the selected individual is a carrier?

### 2014F3A
[2013S2][]

Suppose $X_1$ and $X_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:
$f(x)=\begin{cases}e^{-x}& x>0\\0& o.w.\end{cases}$

(a) Find the p.d.f. of $Y=4(X_1-X_2)$.

(b) Find the mean and variance of $Y$.

### 2014F4A
[2007F2A][] [2008S5A][] [2009FA1][] [2015S2A][] [2019SA2][]

The lifetime (in hours) Y of an electronic component is a random variable with density function
$f(y)=\begin{cases}\frac1{300}e^{-\frac1{300}y}& y>0\\0& o.w.\end{cases}$

(a) What is the probability that a randomly selected component will operate for at least 300 hours?

(b) Five of these components operate independently in a piece of equipment. The equipment fails if at least three of the components fail.

Find the probability that the equipment will operate for at least 300 hours without failure?

### 2014F5A
[2004F10][]

Let $Z_1,Z_2,..$ be a sequence of random variables random variables; and suppose that, for $n=1,2,..$, the distribution of $Z_n$ is given by $P(Z_n=n^2)=1/n$ and $P(Z_n=0)=1-1/n$.
Show that $\lim_{n\to\infty}E(Z_n)=\infty$ but $Z_n\overset{p}\to0$ as $n\to\infty$

### 2014F1B
[2007F5A][] [2013FB2][] [2015S1B][] [@CDF] [@MLE] [@CI]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find the MLE $\hat\theta$ of $\theta$.

(b) Prove that $\hat\theta$ is consistent for $\theta$.

(c) Find a 95% confidence interval for $\theta$ when $n=6$.

### 2014F2B
[2008S2B] [@Pois] [@FishI]

Let $X_1,X_2,..,X_{n}$ denote a random sample from a Poisson distribution with mean $\theta,\theta>0$.

(a) Find the Fisher information about $\theta$ in the sample.

(b) Suppose we want to estimate $m(\theta)=P(X_1=0)=e^{-\theta}$. Find a lower bound on the variance of any unbiased estimator of the parametric function $m(\theta)$.

### 2014F3B
[@UMP] [@HypoT] [@power]

Let $\theta$ be a parameter with space $\Omega=\{0; 1\}$. Let $X$ be a discrete random variable taking on values 1,2,3,or 4. Let the probability funtion of $X$ be given by the following table:

 ----------------------------------
            $X_1$ $X_2$ $X_3$ $X_4$
 ---------- ----- ----- ----- ----- 
 $\theta_0$ $1/2$ $1/4$ $1/8$ $1/8$
 
 $\theta_1$ $2/9$ $2/9$ $2/9$ $1/3$
 ----------------------------------

Find the UMP size 1/8 and 1/4 tests to test $H_0:\theta=0$ against $H_A:\theta=1$. Also find the powers of these two tests.

### 2014F4B
[2014SB3] [@Norm] [@MLE]

Let $X_1,X_2,..,X_{n}$ be a random sample from a normal distribution, $N(\mu,\sigma^2)$, where $-\infty<\mu<+\infty$ and $\sigma>0$. Find the MLE of $\mu/\sigma$ and find itsexpected value.

### 2014F5B
[2007F2B][] [2017FB2][] [@Expo] [@suff]

Let $X_1,X_2,..,X_{n}$ denote a random sample from exponential distribution
with pdf,
$f(x,\mu)=\begin{cases}e^{-(x-\mu)}& \mu<x<\infty\\0& e.w.\end{cases}$

(a) Show that $X_{(1)}=\min\{X_i\}$ is a complete sufficient statistic.

(b) Are $X_{(1)}$ and the sample variance independent statistics? Justify your answer.

## 2015S

Tableman, Kochar

### 2015S1Aa
[2005S3][] [Jesen's Inequality p190]

Let X be a random variable with finite mean $\mu$, finite variance $\sigma^2$,
and assume $E(X^8)<\infty$. Prove or disprove:

i. $E[(\frac{X-\mu}{\sigma})^2]\ge1$.

ii. $E[(\frac{X-\mu}{\sigma})^4]\ge1$.

### 2015S1Ab
[2003S3][] [2004F11][] [2007F3A][] [2008S2A][] [2010SA4][] [2016S5][] [2016F8][] [2018FA1][] [2019SA1][] [@joint] [@marg]

Suppose a box contains a large number of tacks, and the probability $X$ that a particular tack will land with its point up when it is tossed varies from tack to tack in accordance with the following pdf:

$f(x)=\begin{cases}2(1-x)&0<x<1\\0& o.w.\end{cases}$

Suppose a tack is selected at random from this box and this tack is then tossed three times independently. Determine the probability the tack will land with its point up on all three tosses.

<span style="color:red"> 
$E[X^3]=\int_0^1y^3f(y)dy=1/10$

$E^3[X]=(\int_0^1xf(x)dx)^3=1/27$


### 2015S2A
[2007F2A][] [2008S5A][] [2009FA1][] [2014F4A][] [2019SA2][]

Suppose $Y_1$ and $Y_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:

$f(x)=\begin{cases}10e^{-10x}& x>0\\0& o.w.\end{cases}$

Find the p.d.f. of $X=Y_1-Y_2$.

$f(y_1,y_2)$

$f(x,w)$

$f(x)=5e^{-10|x|}$  [@Laplace] [Double Expo]

### 2015S3A
[2005S8][] [2014SA3][] [2014SA5][] [2016S3][] [@SNorm] [@mean]

First,let $\Phi(.)$ and $\phi(.)$ denote the standard normal cdf and pdf respectively. Then, let $X_1,..,X_n$ denotes a random sample from a normal distribution with means $\theta$ and variance $\sigma^2$, and let $F(.)$ and $f(.)$ denote the common cdf and pdf of the r.s. respectively. Assume the sample size $n$ is odd; that is, $n=2k-1$; $k=1,2,3,..$ In this situation, the sample
median is the $k^{th}$ order statistic, denoted by $Y_k$.

(a) (5) Let $g(y)$ denote the pdf of the sample median $Y_k$. Derive $g(y)$. You may use the symbols $F(.)$ and $f(.)$.

(b) (5) Determine the $E(Y_k|\bar X)$, where $\bar X$ is the sample mean of the above random sample. Justify your answer.

[7.3.23 p347]

 $E(Y_k|\bar X)=\bar X$

### 2015S4A
[2007F4A][] [@CDF]

(a) Let X be a continuous type random variable with cumulative distribution function $F(x)$. Find the distribution of the random variable $Y=\ln(1-F(X))$:

$1-e^{-y}$

(b) Prove that for any $y\ge c$, the function $G_c(y)=P[X\le y|X\ge c]$

### 2015S5A
[@Pois] [@cond]

Suppose X and Y are independent Poisson random variables with parameters $\lambda$ and $2\lambda$, respectively.

(a) Find the distribution of $X+Y$.$Pois(3\lambda)$

trans or MGF p158

(b) Find $E[X|X+Y=5]$.

$P(x|x+y=5)=\frac{P(x)P(y=5-x)}{P(x+y=5)}$

### 2015S1B
[2007F5A][] [2013FB2][] [2014F1B][] [@CDF] [@MLE]

Let $X_1,X_2,..,X_n$ be a random sample from a distribution with cumulative distribution function

$F(x)=\begin{cases}0&x<0\\(\frac{x}\theta)^2& 0\le x<\theta\\1& x\ge\theta\end{cases}$

(a) Find $\hat\theta$, the mle of $\theta$. 

$X_{(1)}$ or $X_{(n)}$ ?

(b) Find $E[\hat\theta]$.

$EX_{(1)}=\frac{14n}{3\theta^2}(\frac{\theta^2-1}{\theta^2})^{n-1}$

$EX_{(n)}=\frac{2n}{2n+1}\theta$

(c) Prove that $\hat\theta$ is consistent for $\theta$.

$\lim\to0$, Bias$\to0$


### 2015S2B
[2007F3B][] [2010SB1][] [2010FB1][] [2011S5][] [2013FB3][] [2018S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

$\frac{n}{\theta^2}$

(b) Find the 90th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

$0.9=\int_0^{g(\theta)}f(x,\theta)$

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

$Var\ge\frac{g'(\theta)^2}{I_{\theta}}$

### 2015S3B
[2004F8][] [2007F4B][] [2013FB4][] [2018S1B][] [2019SB2][] [@Laplace] [@MLE]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2015S4B
[2003S9][] [2004F9][] [2007F5B][] [2018S3B][] [2019SB3][] [@SPower] [@power] [@UMP]

Suppose $Y$ is a random variable (sample size = 1) from a population with density function
$f(y|\theta)=\begin{cases}\theta y^{\theta-1}& 0<x<1,\theta>0\\0& o.w.\end{cases}$

(a) Sketch the power function of the test of the rejection: $Y>0.05$.

$\alpha=P_{\theta_1}(T<t_0)$

(b) Based on the single observation $Y$ , find the uniformly most powerful test of size $\alpha$ for testing $H_0:\theta=1$ against $H_A :\theta>1$.

$f_1/f_0c_0$ reject

### 2015S5B
[2009FA3][] [2019SA4][] [@FishI] [@CI]

Let $X_1,X_2,..,X_{5}$ denote a random sample size $n=5$ from a continuous distribution with cdf $F(.)$ with median $\theta$. Let $S(\theta)=$ the number of $\{X_i's>\theta\}$. We can express this as 
$S(\theta)=\sum_{i=1}^5I(X_i>\theta)$;
where I(.) is an indicator variable.

(a) State explicitly the distribution of $S(\theta)$.

(b) Sketch the graph of $S(\theta)$ as a function of $\theta$. Then describe the graph in words.

(c) Find a confidence interval for $\theta$ with confidence coefficient close to 0.95.


## 2015F 

### 2015F1
[2008S1B][] [2009FA4][] [@Unif] [@mean] [@Var] [@suff] [@UMVUE] 

Let $X_1,X_2,..,X_{n}$ be iid continuous uniform r.v.'s over $[0,\theta]$, where $0 <\theta<\infty$. Let $Y_{n}=\max\{X_1,X_2,..,X_{n}\}$

(a) (5 pts) Find $E[Y_n]$.

$f(x)$, $F(x)$, $f_{Y_{(n)}}(x)$, $n\theta/(n-1)$

(b) (5 pts) Find $Var[Y_n]$.

(c) (5 pts) Show that $Y_n$ is a sufficient statistic for $\theta$.

(d) (5 pts) Assuming $Y_n$ is a complete sufficient statistic for $\theta$, find the UMVUE of $\theta$.

7.3.23

(e) (5 pts) Assuming $Y_n$ is a complete sufficient statistic for $\theta$, find the UMVUE of $\theta^2$.

### 2015F2
[2003F8][] [2014SA2][] [2017FA3][]  [@Expo] [@Basu] [@indep]

Suppose $X_1$ and $X_2$ are iid exponential with parameter = 1.

(a) (5 pts) Find the pdf of $Y_1=X_1/(X_1+X_2)$.

[@trans] $Expo(1)\sim Gamma(1,1)$

$\frac{G(\alpha_1,\beta)}{G(\alpha_1,\beta)+G(\alpha_2,\beta)}\sim Beta(\alpha_1,\alpha_2)$

(b) (5 pts) Find the pdf of $Y_2=X_1+X_2$.

$f_{Y_2}(y_2)$

(c) (5 pts) Are $Y_1$ and $Y_2$ independent?

$f(y_1,y_2)=f(y_1)f(y_2)$

### 2015F3
[@Unif] [@LimD]

Let $X_1,X_2,..,X_{n}$ be iid $uniform[0,1]$ rv's. Let $0\le Y_1\le Y_2\le..\le Y_n\le1$ be the
corresponding order statistics.

(a) (5 pts) Find the pdf $g_k(y_k)$ of $Y_k$.

(b) (5 pts) Find $E[Y_k]$.

$f(x)$, $F(x)$, $f_{Y_{(n)}}(x)$

(c) (5 pts) Find $Var[Y_k]$.

(d) (5 pts) What is the limiting distribution of $W_1=nY_1$?

p235

$P(|y_{(n)}-1|\ge\varepsilon)=(1-\varepsilon)^n$

(e) (5 pts) What is the limiting distribution of $W_n=n(1-Y_n)$?

p236

$P(y_{(n)}\le1-t/n)=(1-t/n)^n\to e^{-t}$

$P(y_{(n)}\le1-t/n)=P(n(1-y_n)\ge t)\to 1-e^{-t}$


### 2015F4
[2010FB2] [@Beta] [@HypoT] [@power]

Let X1 be a random sample of size $n=1$ from the Beta distribution with pdf
$f(x|\theta)=\begin{cases}\frac{\Gamma(2\theta)}{\Gamma(\theta)\Gamma(\theta)}x^{\theta-1}(1-x)^{\theta-1}&0<x<1\\0&o.w.\end{cases}$

Suppose a researcher is interested in testing $H_0:\theta=1$ against $H_1:\theta=2$. The researcher decides to reject $H_0$ in favor of $H_1$ if $X_1<2/3$.

(a) (5 pts) Find the size of the test

(b) (5 pts) Compute the power of the test at $\theta=2$.

### 2015F5
[2004F12][] [2010SB4][] [2010FB4][] [2011S6][] [2018S4B][] [@Expo] [@LRT] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample from the exponential distribution with mean $\theta_1$ and let $Y_1,Y_2,..,Y_{n}$ be an independent random sample from another exponential distribution with mean $\theta_2$. Find the likelihood ratio test for testing $H0:\theta_1=\theta_2$ vs $H_a:\theta_1\neq\theta_2$

## 2016S

Crain, Kim

### 2016S1
[@Pois] [@MVUE]

Let $X_1,X_2,..,X_n$ be iid (independent, identically distributed) Poisson random variables with parameter $\lambda>0$.

(a) Find a complete sufficient statistic for $\lambda$.

$T=\sum x_i\sim Pois(n\lambda)$, $\bar X$

(b) Find the MVUE (Minimum Variance Unbiased Estimator) of $\lambda$.

 $\bar X/n$

(c) Find the MVUE of $\lambda^2$:

 $\bar X^2-\bar X$

(d) Find the MVUE of $e^{-\lambda}$.

 $X_1=0$

$E(T|y)=(\frac{n-1}{n})^y$



(e) Find the MVUE of $P(X_i=1)=\lambda^1e^{-\lambda}/1!$:

7.3.23 p347

$X_1=1$

$E(T|y)= \sim Bino(y,1/n)$

(f) Find the MVUE of $P(X_i=k)=\lambda^ke^{-\lambda}/k!$:

 $X_1=k$

$E(T|y)= \sim Bino(y,1/n)$

### 2016S2
[2017FB1][] [@Bino] [@MVUE] [Bino+MLE 7.2.9]

Let $Y$ be $Binomial(n,p)$, with $n$ known and $p$ unknown. Among functions $u(Y)$ of $Y$,

(a) What is the MVUE of $p$?

$y/n$

(b) What is the MVUE of $p^2$?

$\frac{y^2-y}{n^2-n}$

(c) What is the MVUE of $pq=p(1-p)$?

$\frac{y^2-y}{n^2-n}-\frac{y}n$

(d) <span style="color:red"> What is the MVUE of $P(Y=k)=\binom{n}{k}p^k(1-p)^{n-k}$?</span>




### 2016S3
[2005S8][] [2014SA3][] [2014SA5][] [2014SA5][] [2015S3A][] [@SNorm] [@mean]

Let $Z$ be $N(0,1)$. Let $\Phi(z)=\int_{-\infty}^z=\phi(x)dx$, where $\phi(x) = \frac1{\sqrt{2\pi}}e^{−x^2/2},-\infty<x<\infty$, where $\phi(x)$ is the standard normal pdf, and $\Phi(z)$ is the standard normal cdf.

(a) Find $E[\Phi(Z)]$. =1/2

$\Phi^n |_{-\infty}^{\infty}=1$

(b) Find $E[\Phi^{2}(Z)]$. =1/3

(c) Find $E[n\Phi^{n-1}(Z)]$.

(d) Find $E[Z^4]$. =3

$E(Z^{2k})=\frac{(2k)!}{2^KK!}$, k=1,2,..

(e) Find $E[Z^5]$ =0

$E(Z^{2k+1})=0$

### 2016S4
[2003S7][] [2008F5][] [2009SB1][] [2009FB4][] [2016F7][] [2017FB4][] [2018FB2][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

$\hat\theta=-\frac{n}{\sum\ln x_i}-1$

(b) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$.

$\hat\theta_U=-\frac{n-1}{\sum\ln x_i}-1$

(c) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

$\frac{(\theta+1)^2}{n}$

(d) (2 pts) What is the asymptotic distribution of $\hat\theta$?



### 2016S5
[2003S3][] [2004F11][] [2007F3A][] [2008S2A][] [2010SA4][] [2015S1Ab][] [2016F8][] [2018FA1][] [2019SA1 ][] [@joint] [@marg] [@trans]

Let $X$ and $Y$ have the following joint pdf:
$f(x,y) =\begin{cases}6(y-x)& 0<x<y<1\\0& o.w.\end{cases}$
Define $Z=(X+Y)/2$ and $W=Y$, respectively.


(a) Find the joint pdf of Z and W.

$|J=2|$

$f_{Z,W}(z,w)=24(w-z)$, $0<2z<2w<1+w$, $0<w/2<z<w<1$

(b) Find the marginal pdf of Z.

$f_{Z}(z)=\int_{2z}^{1}24(w-z)dw=24z-12$

$f_{Z}(z)=\int_{z}^{2z}24(w-z)dw=12z^2$

$f_{Z}(z)=\int_{z}^{1}24(w-z)dw=12(z-1)^2$

## 2016F

Fountain, Ian Dinwoodie

### 2016F1
[@Norm]

Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$.
Let $S_k^2=\frac1k\sum_{i=1}^n(X_i-\bar X)^2$ be an estimator of $\sigma^2$. Find the value of $k$ that minimizes the mean squared error of the estimator. =n+1

$W$, $E[W]$, $Var[W]$, $Bias$, $MSE$

$S_k^2=\frac{n-1}{k}S^2$

$\frac{\partial MSE}{\partial k}=0$

$\hat\theta=S_{n+1}^2$

### 2016F2
[@MGF] [561-me2]

The moment generating function of a particular random variable is $M_X(t)=\frac{e^t}{4-3e^t}$. Find the coefficient of variation ($CV=\sigma/\mu$) of this distribution. $=\sqrt3/2$

$M'_X(t)$ 

$EX=M'_X(0)$ 

$M''_X(t)$ 

$EX^2=M''_X(0)$ 


### 2016F3
[2008F3][] [@Cheb]

If X is a random variable such that $E[X]=2$ and $E[X^2]=13$, determine a lower bound for the probability $P(-4<X<8)$. (Hint: Use a famous inequality.) 

Chebyshev Inequality p122


$P(-4<x<8)\ge3/4$

### 2016F4
[2008S3A][] [2008F1][] [@Unif] [@CDF] [@PDF] [@trans]

Let $Y_1$ and $Y_2$ be a random sample of size 2 from $Uniform(0,1)$. Find the cumulative distribution and probability density functions of $U=Y_1+Y_2$.

$f_u(u)=\begin{cases}&0\le\mu\le1\\&1\le\mu\le2\end{cases}$

$F_u(u)=\begin{cases}&0\le\mu\le1\\&1\le\mu\le2\end{cases}$

### 2016F5
[2003S10][] [2008F6][] [@Norm] [@indep] [@Basu] 

Let $Y_n$ be the $n^{th}$ order statistic of a random sample of size n from the normal distribution $N(\theta,\sigma^2)$. Prove that $Y_n-\bar Y$ and $\bar Y$ are independent.

[562-me2] [Ancillary p284]

### 2016F6
[2008F7][] [@Expo] [@BayesE]

Suppose that $X_1,X_2,..,X_n$ i.i.d. $Exponential(\theta)$, i.e. $f(x;\theta)=\theta e^{-\theta x},x>0$. Also assume that the prior distribution of $\theta$ is $h(\theta)=\lambda e^{-\lambda\theta},\theta>0$. Find the Bayes estimator of $\theta$, assuming squared error loss.

$f(\bar X|\theta)$

$\pi(\theta|\bar X)\propto f(\bar X|\theta)\pi(\theta)$


### 2016F7
[2003S7][] [2008F5][] [2009SB1][] [2009FB4][] [2016S4][] [2017FB4][] [2018FB2][] [2019SB4][] [@UMP] [@MOM]

Let $X_1,X_2,..,X_n$ be a random sample of size n from the following distribution:
$f(x;\theta)=(\theta+1)x^\theta,\ 0\le x\le 1$
where $\theta>-1$ is an unknown parameter.

$\sim Beta(\theta+1,1)$


(a) Find the method of moments estimator for $\theta$.

p312, p107

$E[X^n]=\frac{\Gamma(\alpha+n)\Gamma\beta+n)}{\Gamma(\alpha+\beta+n)\Gamma(\alpha)}$

$E[X^1]=\bar X=\frac{\theta+1}{\theta+2}$

$\theta_{MOM}=\frac{2\bar X-1}{1-\bar X}$

(b) Find the maximum likelihood estimator for $\theta$.

p315


(c) Determine if your MLE is unbiased.

Bias$=E[\hat\theta]-\theta$

(d) Find the asmptotic variance of your MLE in part (b), as $n\to\infty$.

$\sqrt{n}[\tau(\hat\theta)-\tau(\theta)]\to n(0,\frac{[\tau'(\theta)]^2}{I(\theta)})$

(e) Find the Cramer-Rao lower bound on the variance of an unbiased estimator of $\theta$.

$(\theta+1)^2/n$

(f) Identify the sufficient statistic for $\theta$.

Expo family

(g) Suppose you've taken a sample of size $n=10$. Determine the UMP test of the null hypothesis $\theta=.5$ vs. the alternative $\theta>0.5$.

Step 1 suff; Step 2 Expo has MLR; Step 3 $T\le t_0$ is UMP

### 2016F8
[2003S3][] [2004F11][] [2007F3A][] [2008S2A][] [2009FA2][] [2010SA4][] [2015S1Ab][] [2016S5][] [2018FA1][] [2019SA1 ][] [@joint] [@marg]

Let $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}c(1-y_2)&0\le y1\le y2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c. =6

$\int_0^1\int_0^{y_2}c(1-y_2)dy_1dy_2=1$

(b) Find the marginal density functions of $Y_1$ and $Y_2$.

$f_{Y_1}(y_1)=$

$f_{Y_2}(y_2)=$

(c) Find $P(Y_1\le1/2|Y_2\le3/4)$ =25/27

$P(Y_1\le1/2, Y_2\le3/4)=25/32$

$P(Y_2\le3/4)=27/32$



## 2017S

Ian Dinwoodie, Robert Fountain

### 2017S1

Let $\Theta$ be a real-valued random variable with density $f\Theta(\theta) = \frac1{\sqrt{2\pi}}e^{−\theta^2/2},\theta\in\mathbb R$, and let Y have
conditional density $f(y|\theta) = \frac1{\sqrt{2\pi}}e^{−(y-\theta)^2/2},y\in\mathbb R$. Determine

(a) the conditional density of $\Theta$ given $Y = y$, (2 pts)

$f(y,\theta)$

$f(\theta|Y=y)$

(b) the marginal density of $Y$. (3 pts) 

$f(y)\sim Norm$ 

### 2017S2
[@trans] [@SNorm] [Cauchy]

If $Z_1,Z_2$ are independent standard normal random variables, find the density of $Z_1/Z_2$.


### 2017S3
[@MGF] [@Expo]

Suppose $X_1,X_2,..,X_n$ is a random sample of $Exp(1)$ @Expo random variables. Find the moment generating function $M(t)=E(e^{tX_{(1)}}), t\in\mathbb R$, where $X_{(1)}$ is the minimum. (5 pts)

$f_{X_{1}}(x)\sim Expo(1/n)$

MGF= $\frac1{1-\beta t}=\frac1{1-t/n}$

### 2017S4
p625

Let $X=e^Z$ be a lognormal random variable, $Z\sim N(0,1)$. Find its skewness $E(X − μ)^3/\sigma^3$.

$E[X^n]=e^{n\mu+n^2\sigma^2/2}$

$E[X^1]=$
$E[X^2]=$
$E[X^3]=$

### 2017S5
[@Norm] [@MLE] [@Cor]

Suppose $X_1,X_2,..,X_n$ is a random sample of $N(\mu,\sigma^2)$ random variables.

(a) Find the expectation of the MLE for $\sigma^2$. (3 pts)
p214

$\hat\sigma^2=\frac{\sum(x_i-\mu)^2}{n}$

$\frac{n-1}{n}S^2=\hat\sigma^2$

(b) Compute the correlation of $\bar X$ and $X_n$. (2 pts)
p169

$\sigma^2/n$

### 2017S6
[2003F9][] [2011F6][] [@Pois] [@UMP]

Let $X_1,X_2,X_3,...$ be i.i.d. $Poisson(\lambda)$ random variables. Find a UMP (uniformly most powerful) test of $H_0:\lambda\le1$ versus $H_1:\lambda>1$ at a level $\alpha$ near .05. (5 pts)
[p391]

Step1 suff $T=\sum x_i\sim Pois(n\lambda)$

Step2 $T\sim Pois$ Expo family has MLR

Step3 $T>t_0$ is UMP test at $\alpha=P_{\theta_0}(T>t_0)=0.05$

### 2017S7
[@Var] [@Beta] [@Bino]

Suppose that $(P,X)$ is a pair of random variables with $P\sim Beta(1/2, 1/2)$ and then $X_{|P=p}\sim bin(n,p)$. Find the variance of X. 

$E[P]$, $E[P^2]$, $Var[P]$

$Var[X|P]=n(n+1)/8$

### 2017S8
[@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample of $N(\mu,\sigma^2)$ random variables. Find the Cramer-Rao
lower bound  on the variance of an unbiased estimator for $\sigma^2$. (Assume $\mu$ is known.)

$2\sigma^4/n$

## 2017F

Kim, Kochar

### 2017FA1
[2018S3A][]

Suppose Xenophon and Yves meet for lunch, and Xenophon arrives at time X
uniformly from 1 to 2 P.M., and Yves arrives independently at time Y with the same distribution. Find the distribution of $|Y-X|$ and its expectation, that is, the expected waiting time of either party.



### 2017FA2
[@Expo]

Let $X_1,X_2,..,X_n$ be i.i.d. $Exp(\lambda)$ random variables with rate parameter $\lambda$ and density
$f(x)=\lambda e^{-\lambda x}, x>0$, with $\sigma^2=1/\lambda^2$. We are thinking about using the estimator $\bar X^2$ for the variance. Find the limiting distribution of
$\sqrt{n}(\bar X_n^2-1/\lambda^2)$

Let  be a random sample from $Poisson(\mu)$ @Pois. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

### 2017FA3
[2003F8][] [2014SA2][] [2015F2][] [@Expo] [@Basu] [@indep]

If X and Y are independent $Exp(1)$ random variables, and the density of the ratio $X/(X+Y)$.

### 2017FA4
[@Expo] [@trans]

Let $F$ be the cdf of an exponential random variable with median 10 and let $G$ be
that of an independent exponential random variable Y with median 5. Find the distribution
of $V=F(X)+G(Y)$.


### 2017FB1
[2016S2][] [@Bino]

Let $Y$ be $Binomial(n,p)$, with $n$ known and $p$ unknown. Among functions $u(Y)$ of $Y$,

(a) What is the MVUE of $p$?

(b) What is the MVUE of $p^2$?

(c) What is the MVUE of $pq=p(1-p)$?

(d) What is the MVUE of $P(Y=k)=\binom{n}{k}p^k(1-p)^{n-k}$?

### 2017FB2
[2007F2B][] [2014F5B][] [@Expo]

Let $X_1,X_2,..,X_{10}$ be a random sample from an exponential distribution with location
parameter $\theta$ with pdf
$f(x;\theta)=\begin{cases}e^{-(x-\theta)}& \theta<x<\infty\\0& e.w.\end{cases}$

where $-\infty<\theta<\infty$is an unknown parameter. For testing the null hypothesis $H_0:\theta=0$
vs the alterative $H_1:\theta>0$, a reasonable test is to reject the null hypothesis if $X_{(1)}=\min\{X_1,X_2,..,X_{10}\}\ge C$. Find $C$ so that the size of the test is 0.05. Also find the power of this test at $\theta=1$. Is this test unbiased?

$\alpha=P_{\theta_0}(x_{(1)}\ge c)$, $c=$

$\beta=P_{\theta_1}(x_{(1)}\ge c)$

### 2017FB3
[2008S5B][] [@Norm] [@MLR] [@UMP] [@power] [@HypoT]

Let $X_1,X_2,..,X_{m}$ be a random sample of size m from $N(\theta,1)$ distribution and
let $Y_1,..,Y_m$ be an independent random sample of size $m$ from $N(3\theta,1)$.

(a) Show that the joint distribution of X's and Y's has @MLR (monotone likelihood ratio) property.

(b) Find the UMP test of size $\alpha$ for testing $H_0:\theta\le0$ vs $H_1:\theta>0$.

(c) Find an expression of the power function of the UMP test.

### 2017FB4
[2003S7][] [2008F5][] [2009SB1][] [2009FB4][] [2016S4][] [2016F7][] [2018FB2][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) What is the asymptotic distribution of $\hat\theta$?

(c) (2 pts) Using $\hat\theta$, find an unbiased estimator of $\theta$.

(d) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.



## 2018S

Kochar, Kim

### 2018S1A
[@CDF][@trans][@Expo] [@lifetime]
[Ex1.55]

An electric device has lifetime denoted by $T$. The device has value $V=5$ if it fails before time t = 3; otherwise, it has value $V = 2T$. Find the cdf of $V$ , if $T$ has pdf

$f_T(t)=\frac1{1.5}e^{-\frac1{1.5}t}, t>0$

### 2018S2A
[@trans][Ex 2.7]

Let $X$ have pdf 
$fX(x)=\frac29(x+1), -1\le x\le2$ 
Find the pdf of $Y=X^2$.

<span style="color:red"> different answer $\begin{cases}&1\le y\le4\\& 0\le y\le1\end{cases}$ </span>

### 2018S3A
[2017FA1][] 

Suppose Xenophon and Yves meet for lunch, and Xenophon arrives at time X
uniformly from 1 to 2 P.M., and Yves arrives independently at time Y with the same distribution. Find the distribution of $|Y-X|$ and its expectation, that is, the expected waiting time of either party.

### 2018S4A
[@trans][@indep][@Norm][@MGF]

Let $X\sim N(\mu,\sigma^2)$ and $Y\sim N(\gamma,\sigma^2)$. Suppose that X and Y are independent. Deine $U=X+Y$ and $V=X-Y$.

(a) Show that U and V are independent.

(b) Find the distribution of each of them.

### 2018S1B
[2004F8][] [2007F4B][] [2013FB4][] [2019SB2][] [@Laplace] [@MLE]

Let $X_1,X_2,..,X_1$ be a random sample of size 11 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;
where $-\infty<\theta<\infty$ is unknown.
Find the m.l.e. of $\theta$ and find its bias.

$\theta$= sample median unbiased

### 2018S2B
[2007F3B][] [2010SB1][] [2010FB1][] [2011S5][] [2013FB3][] [2015S2B][] [@Expo] [@FishI] [@CRLB]

Let $X_1,X_2,..,X_n$ be a random sample from exponential distribution with p.d.f.

$f(x,\theta)=\begin{cases}\theta e^{-\theta x}& x\ge0\\0& o.w.\end{cases}$

for which the parameter $\theta>0$ is unknown.

(a) Find the Fisher information $I(\theta)$ about $\theta$ in the sample.

(b) Find the 90th percentile of this distribution as a function of $\theta$ and call it $g(\theta)$.

(c) Find the Cramer-Rao lower bound on the variance of any unbiased estimator of $g(\theta)$.

### 2018S3B
[2003S9][] [2004F9][] [2007F5B][] [2015S4B][] [2019SB3][] [@SPower] [@power] [@UMP] [@HypoT]

Let $X_1,X_2,..,X_n$ be a random sample from a a distribution with pdf

$f(x;\theta)=\begin{cases}\theta x^{\theta-1}& 0<x<1\\0& o.w.\end{cases}$

$H_0:\theta=1\quad H_1 :\theta>1$

Derive the UMP test of size $\alpha$ and obtain the null distribution of your test statistic.

### 2018S4B
[2004F12][] [2010SB4][] [2010FB4][] [2011S6][] [2015F5][] [@Expo] [@LRT] [@HypoT] [@power] [@HypoT]

The life time of an electronic component has exponential distribution with mean $\mu$.
10 such components are put on test at the same time and the experiment is terminated when all of them fail and the times of their failure, $X_1,X_2,..,X_n$ are noted. Based on this information, derive the likelihood ratio test LRT at the level $\alpha= 05$ of the null hypothesis $H_0:\mu=5$ against the alternative $H_1:\mu\neq5$. Also find an expression for the power function of this test.

$T=\sim x_i$, $\frac{2T}{\theta}\sim\chi^2_{2n}$

reject $T>t_0$ two-side rejection region.

## 2018F

Kochar, Bruno

### 2018FA1
[2003S3][] [2004F11][] [2007F3A][] [2008S2A][] [2009FA2][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2019SA1 ][] [@joint] [@marg]

Suppose $(Y_1,Y_2)$ have the joint pdf
$f(y_1,y_2)=\begin{cases}C&0\le y_1\le y_2\le 1\\0& o.w.\end{cases}$

(a) Find the value of c. =2

(b) Find the marginal density functions of $Y_1$ and $Y_2$ and check whether they are independent.

$f_{Y_1}=2-2y_1$
$f_{Y_2}=2y_2$ dependent

(c) Find $E[Y_1+Y_2]$ =1

(d) Find $P(Y_1\le3/4|Y_1>1/3)$ 495/576 55/64

### 2018FA2
[@CDF] [range]

Let $X_1,X_2,..,X_n$ be a random sample from an  exponential distribution [@Expo] with mean 5.

$X_{(r)}=(n-j):(n-1)$

(a) Find the CDF of the sample range.

pdf $X_{(n)}-X_{(1)}$

CDF $=\int pdf$

(b) Find the expected value [@mean]of the sample range. Example 5.4.5

$E[X_{(r)}]=\sum_{j=1}^{n-1}\frac1{(n-1)-j+1}$

### 2018FA3

(a) (5 pts) In the daily production of a certain type of rope, the number of defects per foot, $X$ is assumed to have a Poisson distribution [@Pois] with mean $\lambda=3$. The profit per foot of the rope sold is given by
$P=30-3X-X^2$
Find the expected profit per foot.

$E[P]=30-3EX-EX^2$ =9


(b) (5 pts) Suppose that $X$ is distributed as $U(0,1)$ and that $Y$ is a random variable with
$E(Y|X=x)=\alpha+\beta x^2$
Find $E[Y]$. p164-7(190-3)

$\alpha+\beta/3$

$E[Y]=E(E[Y|X])$

### 2018FA4
[2010SA3][] [2011S4][] [@Pois] [@LimD]

Let $X_1,X_2,..,X_n$ be a random sample from $Poisson(\mu)$. Derive the limiting distribution of
$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})$.

$\sqrt{n}(\bar X_n-\mu)\to n(0,\frac1{I(\mu)})$

Delta Method

$\sqrt{n}(e^{-\bar X_n}-e^{-\mu})\to n(0,\frac{[(e^{-\mu})']^2}{I(\mu)})$

### 2018FB1
[@LRT]

(4+6 pts) Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution with mean 120 and unknown variance $\sigma^2$. Derive the likelihood ratio test for testing the null hypothesis $H_0:\sigma^2=4$ against the alternative $H_1:\sigma^2\neq4$. Also fnd the exact as well as the asymptotic null distributions of your test statistic.


### 2018FB2
[2003S7][] [2008F5][] [2009SB1][] [2009FB4][] [2016S4][] [2016F7][] [2017FB4][] [2019SB4][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) What is the asymptotic distribution of $\hat\theta$?

(c) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$ of $\theta$.

(d) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

### 2018FB3
[@UMVUE] [@Norm] [@FishI]

Let $X_1,X_2,..,X_n$ be a random sample from $N(\mu,\sigma^2)$ distribution. Find a lower bound on the variance of any unbiased estimator of the 95th percentile of this distribution based on the Information Inequality. Also compare this bound to the variance of the uniformly minimum variance unbiased estimator .

$\hat\tau=\bar x+1.645s$

### 2018FB4
[2013FB5] [@Unif] [@HypoT]
Let $X_1$ and $X_2$ be two independent random variables each having uniform distribution
on the interval $(\theta,\theta+1)$. For testing $H_0:\theta=0$ against $H_a:\theta> 0$, we have two competing tests :

1. Test 1 : Reject $H_0 if X_1>0.95$

2. Test 2 : Reject $H_0 if X_1+X_2>c$.

Find the value of c so that the Test 2 has the same value of Type I error probability as Test 1.

$\alpha=P(x_1>0.95)=P(x_1+x_2>c)$

find pdf of $x_1+x_2$ (example of textbook)


## 2019S

Kochar, Bruno

### 2019SA1 
[2003S3][]  [2004F11][] [2007F3A][] [2008S2A][] [2009FA2][] [2010SA4][] [2015S1Ab][] [2016S5][] [2016F8][] [2018FA1][] [@joint] [@marg]

Suppose X and Y have the joint pdf

$f(x;y) =\begin{cases}Cxy& 0\le x\le 2, 0\le y\le 2, x + y\le 2\\0& o.w.\end{cases}$

(a) (4 pts) Find the value of C;

(b) (4 pts) Find the marginal densities of X and Y and check whether they are independent or not;

(c) (2 pts) Compute $P(X<Y)$;

### 2019SA2
[2007F2A][] [2008S5A][] [2009FA1][] [2014F4A][] [2015S2A][]

Suppose $Y_1$ and $Y_2$ are i.i.d. random variables and the p.d.f. of each of them is as follows:

$f(y)=\begin{cases}\theta e^{-\theta y}& y\ge0\\0& o.w.\end{cases}$

with $\theta>0$.

Find the p.d.f. of $X=Y_1-Y_2$.

### 2019SA3 

Let $\theta$ be Beta distributed, $\theta\sim Beta(1,1)$. Let $N_1$ be Binomial given $\theta$, that is $N_1\sim Bin(n,\theta)$ given $\theta$.

(a) (4 pts) Compute $p(\theta|N_1=n_1)$ and $E[\theta|N_1=n_1]$

(b) (6 pts) Compute $p(N_1=n_1)$ for $n_1=0...n$.

### 2019SA4 
[2009FA3][] [2015S5B][] [@MOM]

Let $X_1,X_2,..,X_{10}$ be a random sample from a Poisson distribution with mean $\lambda$.

(a) (4 Ppts) Use the method of moment generating functions to find the distribution of $S_{10}=\sum^{10}_{i=1} X_i$.

(b) (6 pts) Let $S_4=\sum_{i=1}^4X_i$ Find the conditional distribution of $S_4$ given $S_{10}=s$, for $s>0$. This distribution belongs to a family of distributions that you know. Which family? which parameters?

### 2019SB1
[2009FB1][]

Let $X_1,X_2,..,X_n$ be a random sample from a normal distribution $N(\mu,\sigma^2=25)$. Reject $H_0:\mu=50$ and accept $H_1:\mu=55$ if $\bar X_n\ge c$. Find the two equations in n and c that you would solve to get $P(\bar X_n\ge c|\mu)=K(\mu)$ to be equal to $K(50)=0.05$ and $K(55)=0.90$.
Solve these two equations. Round up if n is not an integer. Hint: $z_{.05}=1.645$ and $z_{.1}=1.28$

### 2019SB2 
[2004F8][] [2007F4B][] [2013FB4][] [2015S3B][] [2018S1B][] [@Laplace] [@MLE]

Let $X_1,X_2,..,X_9$ be a random sample of size 9 from a distribution with pdf
$f(x,\theta) =\frac12e^{-|x-\theta|}, -\infty<x<\infty$;

where $-\infty<\theta<\infty$ is unknown.

Find the m.l.e. of $\theta$ and find its bias.

### 2019SB3 
[2003S9][] [2004F9][] [2007F5B][] [2015S4B][] [2018S3B][] [@SPower] [@power] [@UMP]

Suppose $X_1,X_2,..,X_n$ is a random sample from a distribution with pdf

$f(x,\theta)=\begin{cases}\theta x^{\theta-1}& 0<x<1\\0& o.w.\end{cases}$

Suppose that the value of $\theta$ is unknown and it is desired to test the following hypotheses :

$H_0:\theta=1\quad H_1 :\theta>1$

Derive the UMP test of size $\alpha$ and obtain the null distribution of your test statistic.

### 2019SB4 
[2003S7][] [2008F5][] [2009SB1][] [2009FB4][] [2016S4][] [2016F7][] [2017FB4][] [2018FB2][]

Let $X_1,X_2,..,X_n$ be a random sample of size n from a probability density function

$f(x;\theta)=\begin{cases}(\theta+1)x^\theta& 0<x<1\\0& o.w.\end{cases}$

where $\theta>-1$ is an unknown parameter.

(a) (3 pts) Find $\hat\theta$, the maximum likelihood estimator of $\theta$.

(b) (2 pts) Using $\hat\theta$, create an unbiased estimator $\hat\theta_U$ of $\theta$.

(c) (3 pts) Find the Cramer-Rao lower bound for an unbiased estimator of $\theta$.

(d) (2 pts) What is the asymptotic distribution of $\hat\theta$?



# References