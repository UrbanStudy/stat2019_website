---
title: ''
fontsize: 10pt
geometry: margin=2mm
output:
  pdf_document:
    toc: no
    number_sections: no
  word_document:
    toc: no
  html_document:
    toc: no
    df_print: paged
classoption: null
pagenumbering: no
whitespace: none
linestretch: 0.1
header-includes:
- \usepackage{multicol}
- \usepackage{multirow}
- \usepackage{lscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
- \setlength\tabcolsep{0.1pt}
- \setlength\lineskip{1pt}
- \setlength\parskip{0.1pt}
- \usepackage[fleqn]{mathtools}
- \setlength{\columnsep}{0.1pt}
---

\fontsize{10pt}{0pt}
\setlength{\columnseprule}{0.2pt}




\begin{multicols}{2}

Conditional variance identity $Var[X]=Var(E[X|Y]+E(Var[X|Y]))$

Law of iterated Expectation, $E(E[X|Y])=E[X]=\mu_X$.



Cauchy-Schwarz inequality $|EXY|\le E|XY|\le \sqrt{E|X|^2E|Y|^2}$

\resizebox{1\hsize}{!}{$\text{Cov(X,Y)}$=$|\text{E(X}-\mu_X)(\text{Y}-\mu_Y)|\le \sqrt{\text{E(X}-\mu_X)^2\text{E(Y}-\mu_Y)^2}$=$\sqrt{\text{VarXVarY}}$}

\hrulefill

\textbf{Proposition 1.3.1} $MSE(T(X)) = Bias^2(T(X)) + Var(T(X))$

\textbf{Unbiased Estimator} Bias($\theta$)=$E_{\theta}[T(X)]-q(\theta)$


$\{P_\eta: \eta\in\varepsilon\}$ be a k-dim \textbf{canonical exp. family(*)} with s.s. $T(X)$ 

$q(\mathbf{x,\eta})={h(x)}\exp[{\eta}{T(x)}-{A(\eta)}]$, $x\in\mathcal{X}\subset\mathbb{R}^q$ is free from para, 

where $A(\eta)=\log(\idotsint{h(x)}\exp[{\eta}{T(x)}]dx)$ (similar to m.g.f.)

and $\varepsilon=\{\eta: A(\eta)<\infty\}$ is the natural para space;

\textbf{Facotrization Theorem} $T(X)$ is sufficient. exp. family form (*) 

$\varepsilon\neq\emptyset$, then $T(X)$ is complete. 

\dotfill

\textbf{Theorem 1.6.2}, $X$ is distributed in form (*), $\eta(\theta)\implies A(\eta)$;

$\eta$ is an interior point of the natural parameter space $\varepsilon$.

the moment-generating function of T(X) exist and is given by 

$M_X(t)=\exp\left[A(t+\eta)-A(\eta)\right]$; $E(T(\underline X))=A'(\eta)$; $Var(T(\underline X))=A''(\eta)$


\hrulefill

\textbf{Rao-Blackwell Theorem}$X\sim\{P_\theta: \theta\in\Theta\}$, $T(X)$ is a sufficient statistic for $\theta$, $S(x)$ is a unbiased estimator for $q(\theta)$. function of $T$. 

Then $\tilde S(x)=E_{\theta}[S(x)|T(X)]$ is unbiased for$q(\theta)$ and 

$Var_{\theta}\tilde S(x)\le Var_{\theta}S(x)$, $\forall\theta\in\Theta$ *not proof unique*

\dotfill

\textbf{Completemess} if the solution to $E_{\theta}[g(T(X))]=0$, $\theta\in\Theta$ is $g(T(X))=0$ a.s., then T(X) is complete 




\dotfill

\textbf{UMVUE} if $E_{\theta}[T]=\mu(\theta)$ and $Var_{\theta}[T])=\inf\{Var_{\theta}[S|E_{\theta}[S]=\mu(\theta)\}$ 

Then T is uniformly minimum variance unbiased estimator.

\textbf{Lehmann-Scheffe theorem} if $T$ is a complete sufficient statistic 

$E_{\theta}[h(T)])=\mu(\theta)\}$. 
Then, $h(T)$ is the UMVUE of $\mu(\theta)$.

\hrulefill

\textbf{Regularity Assumptions} on the family $\{P_\theta: \theta\in\Theta\}$:

(I) The set $A = \{x: p(x,\theta) > 0\}$ does not depend on $\theta$, $\forall x\in A, \theta\in\Theta$, score function $\frac{\partial}{\partial\theta}\log p(X,\theta)$ exists and is finite. 

(II) If $T$ is any statistic such that $E\theta(|T|) <\infty, \theta\in\Theta$, then the operations of integration and differentiation by $\theta$ can be interchanged in

$\frac{\partial}{\partial\theta}E\theta(T(X)))=\frac{\partial}{\partial\theta}\int T(x)p(x,\theta)dx=\int T(x)\frac{\partial}{\partial\theta}p(x,\theta)dx$

\dotfill

(Proposition 3.4.1.) $\{P\theta\}$ is an exponential family and $\eta(\theta)$ has a nonvanishing continuous derivative on $\Theta$, then (I) and (II) hold. 

$I(\theta)=E\left([\frac{\partial}{\partial\theta}\log p(x,\theta)]^2\right)=\int\left([\frac{\partial}{\partial\theta}\log p(x,\theta)]^2\right)p(x,\theta)dx$

\dotfill

\textbf{Theorem 3.4.1.Cramer-Rao Inequality} $\{P_\theta: \theta\in\Theta\}$ has density $p(x,\theta)$,$x\in{A}\subseteq\mathbb{R}^q$, $E_\theta(T(X))=\psi(\theta)$ is differentiable$\forall\theta$

Suppose that I and II hold $0<I(\theta)<\infty$ $\forall\theta$, $Var_\theta(T(X))\ge\frac{[\psi'(\theta)]^2}{I(\theta)}$

\textbf{Corollary 3.4.1} if $T$ is an unbiased estimate of $\theta$, $Var_\theta(T(X))\ge\frac{1}{I(\theta)}$

\dotfill

\textbf{Proposition 3.4.2} $X_1,\cdots,X_n$ is a sample from a popu with density, 

$I(\theta)=nI_1(\theta)$; 
$Var_\theta(T(X))\ge\frac{[\psi'(\theta)]^2}{nI_1(\theta)}$; $[\psi(\theta)]^T_{1\times k}I(\theta)^{-1}_{k\times k}[\psi'(\theta)]_{k\times 1}$

\hrulefill

\textbf{Theorem 3.4.2.} $\{P_\theta: \theta\in\Theta\}$ satisfies assumptions (I) and (II). There exists u.b. est. $T^*$ of $\psi(\theta)$, which achieves CRLB $\forall\theta$.

Then $\{P\theta\}$ is a one-parameter exponential family of the form $p(x,\theta)=h(x)\exp\{\eta(\theta)T(x)-B(\theta)\}$

Conversely, if $\{P_\theta\}$ a one-para exp family of the form (*) with n.s.s. $T({X})$.

$\eta(\theta)$ has a continuous nonvanishing derivative on $\theta$,

Then $T(X)$ achieves CRLB and is a UMVUE of $E_\theta(T(X))$

\hrulefill

CRLB obtained from $Q_\eta$ evaluated at $\eta = h(\theta)$ is the same as the bound obtained from $P_\theta$

$\frac{\partial}{\partial\eta}\log q(x,\eta) =\left[\frac{\partial}{\partial\theta}\log p(x,\theta)\right]\frac{\partial\theta}{\partial\eta}, \frac{\partial}{\partial\eta} E_\eta(T(X))=\left( \frac{\partial}{\partial\theta} \psi(\theta)\right)\frac{\partial\theta}{\partial\eta}$, where $\psi(\theta) = E_\theta(T(X))$. $Q_\eta = P_\theta\implies\frac{\partial}{\partial\eta}\ Q_\eta= \frac{\partial}{\partial\theta}P_\theta\frac{\partial\theta}{\partial\eta}$

$\frac{\left[\frac{\partial}{\partial\eta} E_\eta(T(X))\right]^2}{I(\eta)}=\frac{\left[\frac{\partial}{\partial\theta} E_\theta(T(X))\right]^2}{I(\theta)}$

\hrulefill

\textbf{Conv} $X_n\overset{\mathcal{D}}{\to}X$ if $F_n(x){\to}F(x)$ $\forall$ continuity points of $F$

\textbf{Theorem1 Scheffe's Thm} if $X_n$ has a pdf $f_n(x)\to f(x)$,$\forall x\in$ support.
$f(x)$ is a pdf of a r.v $x$, then $f_n(x)\overset{\mathcal{D}}{\to}X$

\textbf{Theorem2} $X_n\overset{\mathcal{D}}{\to}X$ iff $E[f_n(x)]\to E[f(x)]$, $f\in C$

$C$ is the set of bounded and cont. fn.s

\textbf{Conv} $X_n\overset{\mathcal{P}}{\to}X$ iff $P(|X_n-c|>\varepsilon){\to}0$ $\varepsilon>0$ as $n\to\infty$

\dotfill

\textbf{Theorem} $X_n\overset{\mathcal{P}}{\to}c$, iff $X_n\overset{\mathcal{D}}{\to}c$ degenerating.

\dotfill

\textbf{Theorem} if $h: \mathbb{R}^k\to\mathbb{R}^p$ is cont. in $c\subset\mathbb{R}^k$, $X_n\overset{\mathcal{P}}{\to}c$,
Then $h(X_n)\overset{\mathcal{P}}{\to}h(c)$

\dotfill

\textbf{Theorem} $X_n\overset{\mathcal{P}}{\to}X$, $C_n{\to}c$
Then $C_nX_n\overset{\mathcal{P}}{\to}cX$


\dotfill

\textbf{Theorem} $X_n\overset{\mathcal{P}}{\to}X$, $Y_n\overset{\mathcal{P}}{\to}b$
Then $X_n+Y_n\overset{\mathcal{P}}{\to}X+b$

\dotfill

\textbf{Cont. Mapping Theorem} $X_n\overset{\mathcal{p}}{\to}X$, $g$ is a cont. fn.
Then $g(X_n)\overset{\mathcal{P,D}}{\to}g(X)$

\dotfill

\textbf{Slutsky's Theorem} $X_n\overset{\mathcal{D}}{\to}X$, $Y_n\overset{\mathcal{P}}{\to}c$
$(X_n,Y_n)\overset{\mathcal{D}}{\to}(X,c)$

\textbf{Corollary} $X_n+Y_n\overset{\mathcal{D}}{\to}X+c$, $c\neq0$
$\frac{X_n}{Y_n}\overset{\mathcal{D}}{\to}\frac{X}{c}$

\dotfill

\textbf{Continuity Theorem}

\dotfill

\textbf{Weak Law of Large Numbers}

\dotfill

\textbf{Consistent} a sequence of r.v.'s $\{X_n\}$, $X_n\overset{\mathcal{P}}{\to}\theta$, all $\theta\in\Theta$

Then $\{X_n\}$ is consitent for $\theta$

\hrulefill

\textbf{Asym}

\textbf{Central Limit Theorem} $X_1,..X_n\overset{iid}{=}\mu,\sigma^2<\infty$ 

$\frac{\sum X-n\mu}{\sqrt{n}\sigma}=\frac{\sqrt{n}(\bar X -\mu)}{\sigma}\overset{\mathcal{D}}{\to} N(0,1)$

\hrulefill

\textbf{Delta Method} $\{X_n\}$ a sequence of estimators $\sqrt{n}(X_n-\mu)\overset{\mathcal{D}}{\to}N(0,\sigma^2)$

Let $f$ is a continuous  differentiable fn at $\mu$ $f'(\mu)\neq0$

$\sqrt{n}(f(X_n)-f(\mu))\overset{\mathcal{D}}{\to} N(0,\sigma^2[f'(\mu)]^2)$

\hrulefill

The frequency substitution estimates  $h(\frac{\mathbf{N}}n)$ of $\theta$ satisfies $h(\mathbf{p}(\theta))=\theta$ for all $\theta\in\Theta$ where $\mathbf{p}(\theta)=(p(x_0,\theta),..,p(x_k,\theta))^T$

Many $h(\mathbf{p}(\theta))$ exist if $K>1$. $h$ is differentiable.

$\sqrt{n}\left(h(\frac{\mathbf{N}}n)-h(\mathbf{p}(\theta))\right)=\sqrt{n}\sum_{j=1}^3\frac{\partial h}{\partial p_j}(\mathbf{p}(\theta))\left(\frac{N_j}n-p(x_j,\theta)\right)+\mathcal{O}_p(1)$

\textbf{Theorem 5.4.1} $\sqrt{n}(h(\frac{\mathbf{N}}n)-\theta)\overset{\mathcal{D}}{\to} N(0,\sigma^2(\theta,h)$

Delta Method
$\sqrt{n}\left(h(\frac{\mathbf{N}}n)-h(\mathbf{p}(\theta))\right)$ is asym normal with mean 0

asymptotic variance $\sigma^2(\theta,h)=$

\resizebox{1\hsize}{!}{$Var_\theta\left[\sum_{j=1}^k\frac{\partial h}{\partial p_j}(\mathbf{p}(\theta))\mathbf{1}_{(x_1=x_j)}\right]=\sum_{j=1}^k\left[\frac{\partial h}{\partial p_j}(\mathbf{p}(\theta))\right]^2p(x_j,\theta)-\left[\sum_{j=1}^k\frac{\partial h}{\partial p_j}(\mathbf{p}(\theta))p(x_j,\theta)\right]^2$}



\hrulefill

\textbf{Multivariate CLT} $X_1,..X_n\overset{iid}{=}(\mu,\sigma^2)$ , $\mu_4$ is finite.

 $\sqrt{n}((\overline{X_n},\overline{X_n^2})-(\mu_1,\mu_2))\overset{\mathcal{D}}{\to} N(0,\Sigma)$

where $\Sigma=\begin{bmatrix}Var(X_1)&Cov(X_1,X_1^2)\\Cov(X_1,X_1^2)&Var(X_1^2)\end{bmatrix}=\begin{bmatrix}\mu_2-\mu_1^2&\mu_3-\mu_1\mu_2\\\mu_3-\mu_1\mu_2&\mu_4-\mu_2^2\end{bmatrix}$

$J({\mu_1},{\mu_2})=\left.(\frac{\partial f}{\partial t_1},\frac{\partial f}{\partial t_1})\right|_{(t_1,t_2)=(\mu_1,\mu_2)}=\begin{bmatrix}-2\mu_1&1\end{bmatrix}$, $f(t_1,t_2)=t_2-t_1^2$

\resizebox{1\hsize}{!}{$J\Sigma J'=\begin{bmatrix}-2\mu_1&1\end{bmatrix}\begin{bmatrix}{\mu_2}-{\mu_1}^2&{\mu_3}-{\mu_1}{\mu_2}\\{\mu_3}-{\mu_1}{\mu_2}&{\mu_4}-{\mu_2}^2\end{bmatrix}\begin{bmatrix}-2\mu_1\\1\end{bmatrix}={\mu_4}-{\mu_2}^2-2({\mu_3}-{\mu_1}{\mu_2})+{\mu_2}-{\mu_1}^2$}


Let $S_n^2=\frac{n}{n-1}f(\overline{X_n},\overline{X_n^2})$, where $f(x,y)=y-x^2$

By Delta Method,

$${\sqrt{n}\left(f(\overline{X_n},\overline{X_n^2})-f({\mu_1},{\mu_2})\right)}\overset{\mathcal{D}}{\to} N(0,J(\mu_1,\mu_2)\Sigma J')$$
${\sqrt{n}\left(S_n^2-\sigma^2\right)}\overset{\mathcal{D}}{\to} N(0,\eta^2)$,$\eta^2=Var[-2\mu_1X+X^2]$


Under the Cramer-Rao conditions for asymptotic normality, $\sqrt{n}(\hat p_{MLE} -p)\overset{\mathcal{L}}{\to} N(0,I(p)^{-1})=N(0,p(1-p))$

\hrulefill

\textbf{Edgeworth Expansion} 

\hrulefill

\textbf{Lindberg CLT} 

\hrulefill

$F_{X}(x|\theta_1,\theta_2)=\frac{x-\theta_1}{\theta_2-\theta_1}$

$Y\sim U(0,1)$, $f_{Y}(y)=1$, $F_{Y}(y)=y$, $Y_{(1)}\sim Beta(1,n)$, $Y_{(n)}\sim Beta(n,1)$. 

$f_{Y_{(1)}}(y)=\frac{n!}{(1-1)!(n-1)!}f_{Y}(y)[F_{Y}(y)]^{1-1}[1-F_{Y}(y)]^{n-1}=n\left(1-y\right)^{n-1},\ 0\le y\le1$

$f_{Y_{(n)}}(y)=\frac{n!}{(n-1)!(n-n)!}f_{Y}(y)[F_{Y}(y)]^{n-1}[1-F_{Y}(y)]^{n-n}=ny^{n-1},\ 0\le y\le1$


\end{multicols}





