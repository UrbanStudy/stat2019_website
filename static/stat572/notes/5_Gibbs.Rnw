\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espaÃ±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{float}

\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbs}[1]{\boldsymbol{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\renewcommand{\d}{\text{d}}
\newcommand{\by}{\mbf{y}}
\newcommand{\mts}{\tilde{Y}}
\newcommand{\mtsv}{\tilde{\bv}}
\newcommand{\btw}{\tilde{\bw}}
\newcommand{\bhw}{\hat{\bw}}
\newcommand{\btx}{\tilde{\bx}}
\newcommand{\pt}{\tilde{p}}
\newcommand{\ba}{\mbf{a}}
\newcommand{\bb}{\mbf{b}}
\newcommand{\bc}{\mbf{c}}
\newcommand{\bd}{\mbf{d}}
\newcommand{\boe}{\mbf{e}}
\newcommand{\bk}{\mbf{k}}
\newcommand{\bq}{\mbf{q}}
\newcommand{\br}{\mbf{r}}
\newcommand{\bs}{\mbf{s}}
\newcommand{\bh}{\mbf{h}}
\newcommand{\bff}{\mbf{f}}
\newcommand{\bt}{\mbf{t}}
\newcommand{\bu}{\mbf{u}}
\newcommand{\bm}{\mbf{m}}
\newcommand{\bv}{\mbf{v}}
\newcommand{\bx}{\mbf{x}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\bz}{\mbf{z}}
\newcommand{\tby}{\tilde{\mbf{y}}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\bp}{\mbs{p}}
\newcommand{\bA}{\mbf{A}}
\newcommand{\tbA}{\tilde{\bA}}
\newcommand{\bB}{\mbf{B}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\tc}{\tilde{c}}
\newcommand{\tC}{\tilde{C}}
\newcommand{\tbC}{\tilde{\bC}}
\newcommand{\bF}{\mbf{F}}
%\newcommand{\bBs}{\mbf{B}^\star}
\newcommand{\bD}{\mbf{D}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bK}{\mbf{K}}
\newcommand{\bQ}{\mbf{Q}}
\newcommand{\bV}{\mbf{V}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\hN}{\hat{N}}
\newcommand{\tbc}{\tilde{\mbf{c}}}
\newcommand{\tba}{\tilde{\mbf{a}}}
\newcommand{\tbX}{\tilde{\mbf{X}}}
\newcommand{\tbW}{\tilde{\mbf{W}}}
\newcommand{\bSigma}{\mbs{\Sigma}}
\newcommand{\bGamma}{\mbs{\Gamma}}
\newcommand{\bUps}{\mbs{\Upsilon}}
\newcommand{\bPsi}{\mbs{\Psi}}
\newcommand{\vs}{v^\star}
\newcommand{\Vs}{V^\star}
\newcommand{\bvs}{\bv_i^\star}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bP}{\mbf{P}}
\newcommand{\bBs}{\bB^\star}
\newcommand{\bXs}{\bX^\star}
\newcommand{\bxs}{\bx^\star}
\newcommand{\bWs}{\bW^\star}
\newcommand{\bws}{\bw_i^\star}
\newcommand{\bwsp}{\left.\bw_i^\star\right.^\prime}
\newcommand{\bBsp}{\left.\bB^\star\right.^\prime}
\newcommand{\bVs}{\bV^\star}
\newcommand{\bpsi}{\mbs{\psi}}
\newcommand{\bphi}{\mbs{\phi}}
\newcommand{\bmu}{\mbs{\mu}}
\newcommand{\bdelta}{\mbs{\delta}}
\newcommand{\bbeta}{\mbs{\beta}}
\newcommand{\bxi}{\mbs{\xi}}
\newcommand{\bchi}{\mbs{\chi}}
\newcommand{\blambda}{\mbs{\lambda}}
\newcommand{\bLambda}{\mbs{\Lambda}}
\newcommand{\LamT}{\tilde{\Lambda}}
\newcommand{\GamT}{\tilde{\Gamma}}
\newcommand{\WT}{\tilde{W}}
\newcommand{\balpha}{{\mbs{\alpha}}}
\newcommand{\bepsilon}{{\mbs{\varepsilon}}}
\newcommand{\bgamma}{{\mbs{\gamma}}}
\newcommand{\btheta}{{\mbs{\theta}}}
\newcommand{\bseta}{{\mbs{\eta}}}
\newcommand{\bpi}{{\mbs{\pi}}}
\newcommand{\bI}{\mbf{I}}
\newcommand{\bH}{\mbf{H}}
\newcommand{\tbH}{\tilde{\mbf{H}}}
\newcommand{\1}{\mbs{1}}
\newcommand{\0}{\mbs{0}}
\newcommand{\detstar}[1]{\text{det}^+\left(#1\right)}
\renewcommand{\det}[1]{\text{det}\left(#1\right)}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\renewcommand{\exp}[1]{\text{exp}\left[#1\right]}
\newcommand{\M}{{M}}
\newcommand{\K}{{K}}
\newcommand{\peq}{{p}}
\newcommand{\MB}{{M_B}}
\newcommand{\MF}{{M_F}}
\newcommand{\MT}{{M_T}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\graph}{\Gamma}
\newcommand{\kset}{\Upsilon}
\newcommand{\order}{order}
\newcommand{\parents}{\mcal{P}}
\newcommand{\children}{\mcal{C}}
\newcommand{\extreme}{\mcal{E}}
\newcommand{\combined}{\mcal{A}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left\{#1\right\}}
\newcommand{\lrno}[1]{\left.#1\right.}
\newcommand{\lrsqb}[1]{\left[#1\right]}
\newcommand{\G}[1]{\Gamma_{#1}}
\newcommand{\N}{\mcal{N}}
\renewcommand{\d}{\text{d}}
\newcommand{\Ps}[1]{\Pr{\left(#1\right)}}
\newcommand{\BF}[2]{{BF}_{#1,#2}(Y)}
\newcommand{\BFd}[3]{{BF}_{#1,#2}(Y,#3)}
\newcommand{\xmark}{\ding{55}}
\newcommand{\ben}{\begin{equation*}}
\newcommand{\een}{\end{equation*}}
\newcommand{\bean}{\begin{eqnarray*}}
\newcommand{\eean}{\end{eqnarray*}}
\newcommand{\bsm}{\begin{smallmatrix}}
\newcommand{\esm}{\end{smallmatrix}}
\newcommand{\bmat}{\begin{matrix}}
\newcommand{\emat}{\end{matrix}}
\newcommand{\tI}{\text{I}}
\newcommand{\tN}{\text{N}}
\newcommand{\trN}{\text{trunc.N}}
\newcommand{\nl}[1]{\text{log}{\lrp{#1}}}
\newcommand{\e}[1]{\text{exp}{\lrb{#1}}}
\newcommand{\indf}[1]{\tI_{\left\{#1\right\}}}
\newcommand{\parent}{\mcal{P}}
\newcommand{\gp}{\text{GP}{\lrp{\0,\mcal{C}(\cdot\given\bphi)}}}
\newcommand{\gpd}[3]{\text{GP}_{#1}{\lrp{\0,\mcal{C}_{#2}(\cdot\given\phi_{#3})}}}
\newcommand{\mnngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mbs{\mcal{C}}}_{#2}(\cdot,\cdot;\bphi_{#3})}}}
\newcommand{\nngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mcal{C}}_{#2}(\cdot,\cdot;\phi_{#3})}}}
\newcommand{\nngpw}[4]{\text{NNGP}_{#1}^{#2}{\lrp{0,\tilde{\mcal{C}}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpw}[4]{\text{GP}_{#1}^{#2}{\lrp{0,\mcal{C}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpq}{\text{GP}_q{\lrp{\0,\mbs{\mcal{C}}(\cdot\given\bphi)}}}
\newcommand{\gpk}{\text{GP}{\lrp{0,\mcal{C}(\cdot\given\tilde{\phi}_k)}}}
\newcommand{\nngp}{\text{NNGP}{\lrp{\0,\tilde{\mcal{C}}(\cdot\given\phi)}}}
\newcommand{\refset}{\mcal{T}}
\newcommand{\uset}{\mcal{U}}
\newcommand{\oset}{\mcal{T}}
\newcommand{\Xall}{\mbb{X}}
\newcommand{\given}{\,|\,}
\newcommand{\dtr}[1]{\textcolor{blue}{(#1)}}
\newcommand{\bemph}[1]{\bf \emph{#1}}
\newcommand{\var}[1]{\text{var}{\left(#1\right)}}

%-------------------------------
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{exa}{Example}
\newtheorem{note}{Note}
\newcommand{\bex}{\begin{exer}}
\newcommand{\eex}{\end{exer}}
\newcommand{\bexa}{\begin{exa}}
\newcommand{\eexa}{\end{exa}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\bdes}{\begin{description}}
\newcommand{\edes}{\end{description}}

\newcommand{\bsh}{\begin{shaded}}
\newcommand{\esh}{\end{shaded}}
%-------------------------------

%-------------------------------
%hiding proof solutions
%-------------------------------


\begin{document}


\setcounter{section}{0}
\title{Gibbs Sampling}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf Gibbs Sampling}\\
{\large STAT 572: Bayesian Statistics}\\
Fall 2019
\end{center}
\section{Motivation}

So far, we have considered straightforward problems, for most of which we can either obtain the quantities of interest in closed form, or for which we can sample directly from the distribution of interest and design a Monte Carlo (or Importance Sampling) algorithm that enables us to approximate such quantities. However, the distributions associated to more interesting problems in statistics usually have complicated forms from which we cannot sample directly.  

Let's consider an example we are familiar with to bring the idea home.

\section{Semi-conjugate priors for the Normal likelihood problem}

Without straying too far from what we have done up to now, consider the situation where we have $Y_1,\ldots,Y_n\overset{iid}{\sim}N(\theta,\lambda^{-1})$. Recall that the conjugate prior for $(\btheta,\blambda)$ was the Normal-Gamma$(\mu_0,\nu,\alpha,\beta)$, which is equivalent to $\btheta\given \lambda \sim \tN\lrp{\mu_0,(\nu\lambda)^{-1}}$ and $\blambda\sim \text{Gamma}(\alpha/2,\beta/2)$.  However, this conjugate family is restrictive in the sense that we are forcing the variance of $\btheta$ to be proportional to that of the likelihood, which could be too small or too large.  In fact, instead of assuming the conjugate prior for the normal model, in practice it is more common to assume that the joint prior is $$p(\theta,\lambda)=p(\theta)p(\lambda),$$
with
\bean
\btheta &\sim& N(\mu_0,\lambda_0)\\
\blambda &\sim& \text{Gamma}(\alpha/2,\beta/2).
\eean

Note that if we go through the calculations and derive the posterior in this case (convince yourself), we would have that $$p(\theta,\lambda\given y_{1:n})=p(\theta \given\lambda, y_{1:n})p(\lambda \given y_{1:n}),$$
where $$\theta \given\lambda, y_{1:n}\sim \tN\lrp{\frac{n\lambda \bar{y}+\lambda_0 \mu_0}{n\lambda +\lambda_0}, (n\lambda +\lambda_0)^{-1}},$$
which we clearly can sample from.  However
$$ p(\lambda \given y_{1:n})\propto \frac{\lambda^{\frac{n+\alpha}{2}-1}}{(n\lambda +\lambda_0)^{1/2}}\e{-\frac{1}{2} \lrp{\lambda\lrp{\sum y_i^2 +\beta}-\frac{(n\lambda \bar{y}+\lambda_0 \mu_0)^2}{n\lambda +\lambda_0} }},$$
so $\lambda \given y_{1:n}$ is no longer Gamma and it is not immediately evident how to sample from this distribution.

Then, what can we do in this situation to determine quantities from the joint posterior?

\subsection{Approximating the posterior by discretizing the parameter space}

Notice that in the previous example, obtaining $p(\theta,\lambda, y_{1:n})$ for particular values $\theta$ and $\lambda$ is a simple calculation.  The issue was that we were not able to sample from this joint posterior density, and so we cannot take the Monte Carlo approach.

However, if you only have a few parameters and have some idea of the plausible range of values that your parameters can take (some careful exploration is required if your parameters live in $\mathbb{R}$ or $\mathbb{R}^+$, but this is not a big issue), then an effective strategy is to simply define a grid of values for the parameters in the joint parameter space, and calculate the numerator of the posterior.

Let's use the normal problem described above to illustrate why this works.  Instead of having $(\theta,\lambda)\in \Theta=\mathbb{R}\times \mathbb{R}^+$, let's consider a discretized version of the parameter space made up only by a $100\times 100$ grid of values for $\theta$ and $\lambda$, given by $\Theta_d=\lrb{\theta_1,\theta_2,\ldots,\theta_{100}}\times \lrb{\lambda_1,\lambda_2,\ldots,\lambda_{100}}$.  Notice that we can define the posterior density over this space by making use the posterior over the original parameter space $\Theta$ as follows.  Denote the posterior over the discretized space by $p_d(\theta,\lambda\given y_{1:n})$ and the one over the original space by $p(\theta,\lambda\given y_{1:n})$.  For a particular pair $(\theta_l,\lambda_k)\in \Theta_d$, we have that
\bean
p_d(\theta_l,\lambda_k\given y_{1:n})&=&\frac{p(\theta_l,\lambda_k\given y_{1:n})}{\sum_{j=1}^{100}\sum_{h=1}^{100} p(\theta_j,\lambda_h \given y_{1:n}) }
\eean
but since $p(\theta_l,\lambda_k\given y_{1:n})=\frac{p(\theta_l,\lambda_k, y_{1:n})}{p( y_{1:n})}$, we can rewrite the discretized joint density above as
\bea
p_d(\theta_l,\lambda_k\given y_{1:n})&=&\frac{p(\theta_l,\lambda_k, y_{1:n})/ p( y_{1:n})}{\sum_{j=1}^{100}\sum_{h=1}^{100} p(\theta_j,\lambda_h, y_{1:n})/ p( y_{1:n}) }\nonumber\\
&=&\frac{p(\theta_l,\lambda_k, y_{1:n})}{\sum_{j=1}^{100}\sum_{h=1}^{100} p(\theta_j,\lambda_h, y_{1:n})}, \label{eq:discrjoint}
\eea
which implies that we can calculate the posterior density over the grid by simply calculating $p(\theta_l,\lambda_k, y_{1:n})$ for all pairs $(\theta_l,\lambda_k)\in\Theta_d$.

Furthermore, it is straightforward to calculate the marginal posterior for $\btheta$ and $\blambda$ over the discretized space using \eqref{eq:discrjoint} for each value $\theta_l\in\Theta_d$ and $\lambda_k\in\Theta_d$, as follows:
\bea
p_d(\theta_l\given y_{1:n})&=&\sum_{k=1}^{100}p_d(\theta_l,\lambda_k\given y_{1:n}),\nonumber\\ 
p_d(\lambda_k\given y_{1:n})&=&\sum_{l=1}^{100}p_d(\theta_l,\lambda_k\given y_{1:n})\label{eq:discrmaginals}
\eea

To illustrate the procedure, let's consider the \emph{midge wing length} example (page 72 in the book), where the wing length of 9 midge specimens was measured in mm. Previous studies from other populations suggested an a priori mean of $\mu_0=1.9$, and since the values are expected to be positive, the prior standard deviation $\tau_0$ is chosen so that $\mu_0-2\tau >0$, hence $\tau > \mu_0/2=0.95$.  Finally, to approximate the posterior probabilities, we need to discretize the space, so we need to define the grid (i.e., the discretized parameter space $\Theta_d$).  Let $$\Theta_d=\lrb{(\theta,\lambda):\theta\in \lrb{1.505,1.510,\ldots,2},\;\lambda\in\lrb{1.75,3.5,\ldots,175}}$$

The three panels in Figure \ref{fig:discnormal} display (from left to right) the joint posterior $p_d(\theta_l,\lambda_k\given y_{1:n})$ (which provides a good approximation of $p(\theta_l,\lambda_k\given y_{1:n}))$, and the marginal posterior densities for $\btheta$ and $\blambda$, respectively.

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=12, fig.height=4, fig.align='center'>>=
# priors
mu0<-1.9  ; t20<-0.95^2
s20<-.01 ; nu0<-1

#data
y<-c(1.64,1.70,1.72,1.74,1.82,1.82,1.82,1.90,2.08)
n<-length(y) ; mean.y<-mean(y) ; var.y<-var(y)


####
G<-100 ; H<-100

mean.grid<-seq(1.505,2.00,length=G) 
prec.grid<-seq(1.75,175,length=H) 

post.grid<-matrix(nrow=G,ncol=H)

for(g in 1:G) {
  for(h in 1:H) { 
    
    post.grid[g,h]<- dnorm(mean.grid[g], mu0, sqrt(t20)) *
      dgamma(prec.grid[h], nu0/2, s20*nu0/2 ) *
      prod( dnorm(y,mean.grid[g],1/sqrt(prec.grid[h])) )
  }
}

post.grid<-post.grid/sum(post.grid)

# pdf("fig6_1.pdf",height=1.75,width=5,family="Times")
par(mfrow=c(1,3),mar=c(2.75,2.75,.5,.5),mgp=c(1.70,.70,0))
image( mean.grid,prec.grid,post.grid,col=gray( (10:0)/10 ),
       xlab=expression(theta), ylab=expression(lambda) )

mean.post<- apply(post.grid,1,sum)
plot(mean.grid,mean.post,type="l",xlab=expression(theta),
     ylab=expression( paste(italic("p("),
                            theta,"|",italic(y[1]),"...",italic(y[n]),")",sep="")))

prec.post<-apply(post.grid,2,sum)
plot(prec.grid,prec.post,type="l",xlab=expression(lambda),
     ylab=expression( paste(italic("p("),
                            lambda,"|",italic(y[1]),"...",italic(y[n]),")",sep=""))) 

CDF.theta <- cumsum(mean.post)
CDF.prec <- cumsum(prec.post)
CI.theta<- mean.grid[c(which(CDF.theta<=0.025)[1],which(CDF.theta>=0.975)[1])]
CI.prec <- prec.grid[c(which(CDF.prec<=0.025)[1],which(CDF.prec>=0.975)[1])]
@
\caption{Discrete approximation to the joint posterior for $(\btheta,\blambda)$ (left), marginal posterior density for $\btheta$ (center), and marginal posterior density for $\blambda$ (right).}
\label{fig:discnormal}
\end{figure}


From the discrete approximation to the joint and marginal posteriors we may derive many quantities of interest.  Among them, we may consider for example the 95\% Credible Interval for the mean $\btheta$ and precision $\blambda$, which can be obtained from the marginal $(\theta_{2.5},\theta_{97.5})=(\Sexpr{CI.theta})$ and $(\lambda_{2.5},\lambda_{97.5})=(\Sexpr{CI.prec})$.

\bsh \note The discretization strategy is only feasible for problems that involve a few parameters.  In the problem above, with 2 parameters the discretized parameter space had $100\times 100=100^2$ points (ie., pairs $(\theta,\lambda)$).  If we choose a grid with 100 values for each parameter in a problem with $m$ parameters, our discretized space would have $100^m$ points (e.g., with 4 parameters, the space would contain $10^8$ points), so this is no longer a feasible alternative for larger $m$.
\esh

What can we do in situations where discretizing is not an option?  

\section{Gibbs Sampling}


Most real-world applications involve with complex probability distributions on complicated high-dimensional spaces. It is the exception rather than the norm to be able to sample exactly from the distribution of interest. Furthermore, high-dimensional spaces are very large, and distributions on these spaces are hard to visualize, making it difficult to even guess where the regions of high probability are located. As a result, it may be challenging to even design a reasonable proposal distribution to use with importance sampling. 

In these cases, Markov chain Monte Carlo (MCMC) is a sampling technique that can work remarkably well. MCMC generates a sequence of correlated samples $Y_1, Y_2, \ldots$ that explore the region of high probability by making a sequence of incremental movements. Even though the samples are not independent, it turns out that under very general conditions, sample averages $\frac{1}{S}\sum_k g(Y_k)$ can be used to approximate expectations $E(g(Y))$ just as in the case of simple Monte Carlo approximation, and by a powerful result called the {\bemph{ergodic theorem}}, these approximations are guaranteed to converge to the true value.

The simplest MCMC algorithm to understand is Gibbs sampling (Geman \& Geman, 1984). In this section we'll work on simple examples with two variables, and then we’ll generalize to multiple variables. Some of the most important problems where Gibbs sampling is useful, are
\benum
\item[(a)] semi-conjugate priors
\item[(b)] hyper-priors and hierarchical models
\item[(c)] data augmentation / auxiliary variables
\eenum

\bdes
\item[Advantages] the Gibbs Sampler has the following convenient features:

\bit
\item can be used even when we can't directly draw samples from the distribution of interest
\item reliable
\item works with high-dimensional parameter spaces and complicated distributions, even when it is not clear where the high probability regions are.
\item relatively easy to implement
\eit
\item[Dissadvantages] there are a few drawbacks, which is a good idea to keep in the back of our minds:
\bit
\item slower convergence than MC or IS because of correlated samples
\item convergence is hard to evaluate
\eit
\edes

As you will see, the Gibbs Sampler opens the door to study much more interesting and realistic problems. 

\subsection{General two-variable Gibbs Sampler}

Suppose $p(x, y)$ is a pdf or pmf that is difficult to sample from directly. Suppose, though, that we can easily sample from the conditional distributions $p(x|y)$ and $p(y|x)$. Roughly speaking, the Gibbs sampler proceeds as follows: set $x$ and $y$ to some initial starting values, then sample $x|y$, then sample $y|x$, then $x|y$, and so on. More precisely,
\benum
\item[0.] Set $(x^{(0)}, y^{(0)})$ to some starting value.
\item[1.] Sample $x^{(1)} \sim p(x|y^{(0)})$, that is, from the conditional distribution $X|Y = y^{(0)}$.

Sample $y^{(1)} \sim p(y|x^{(1)})$, that is, from the conditional distribution $Y|X = x^{(1)}$.

\item[2.] Sample $x^{(2)} \sim p(x|y^{(1)})$, that is, from the conditional distribution $X|Y = y^{(1)}$. 

Sample $y^{(2)} \sim p(y|x^{(2)})$, that is, from the conditional distribution $Y|X = x^{(2)}$.

$\vdots$
\eenum
Each iteration in the Gibbs algorithm is referred to as a sweep or scan. The sampling steps within each iteration are sometimes referred to as Gibbs updates. Importantly, when updating one variable, we use the most recent value of the other variable (even in the middle of an iteration).
This defines a sequence of pairs of random variables $$(X^{(0)}, Y^{(0)}), (X^{(1)}, Y^{(1)}), (X^{(2)}, Y^{(2)}), (X^{(3)}, Y^{(3)}),\ldots$$
which defines a Markov chain because the conditional distribution of $(X^{(k)},Y^{(k)})$ given all of the previous pairs depends only on $(X^{(k-1)},Y^{(k-1)})$. Under quite general conditions, for any $g(x, y)$ such that $E(|g(X, Y )|) <\infty$, where $(X, Y ) \sim p(x, y)$, a sequence constructed in this way has the property that
$$\frac{1}{S}\sum_{k=1}^S g ( X^{(k)} , Y^{(k)}) \longrightarrow E(g( X , Y ))$$
as $S \rightarrow \infty$, with probability 1. This justifies the use of the sample average $\frac{1}{S}\sum_{k=1}^S g ( X^{(k)} , Y^{(k)})$ to approximate $E(g(X, Y ))$, as with the simple MC approximation, even though the pairs $(X^{(k)}, Y^{(k)})$ are not iid. %This is where the name Markov chain Monte Carlo comes from.

\bsh \note
Although the Gibbs Sampler is guaranteed to converge no matter the starting point, ideally, the starting point $(x^{(0)}, y^{(0)})$ should be in a region of high
probability under $p(x, y)$. This is not always an easy task, and because of this we usually run the chain for a while before using the samples obtained. In other words, it is common to discard the first $B$ samples $(X^{(1)},Y^{(1)}),\ldots,(X^{(B)},Y^{(B)})$. This is referred to as the burn-in period. When using a burn-in period, the choice of starting point it is not particularly important, since a poor choice will simply require a longer burn-in period.
\esh

The performance of an MCMC algorithm (that is, how quickly the sample averages $\sum_{k=1}^S g ( X^{(k)} , Y^{(k)})$ converge) is referred to as the mixing rate. 

\subsection{Back to Semi-conjugate priors}

Going back to the Normal likelihood problem with (independent) priors for $p(\theta,\lambda)=p(\theta)p(\lambda)$, recall that it cannot be attacked using a simple Monte Carlo approach since the form for $p(\lambda\given y_{1:n})$ is not known. 

This independent prior is referred to as semi-conjugate since the prior for $\theta$ is conjugate for each fixed value of $\lambda$, and the prior on $\lambda$ is conjugate for each fixed value of $\theta$. From our study of the Normal–Normal model (i.e., the case where the variance was fixed), we know that for any fixed value of $\lambda$
\bean
\theta \given\lambda, y_{1:n}&\sim& \tN\lrp{\frac{n\lambda \bar{y}+\lambda_0 \mu_0}{n\lambda +\lambda_0}, (n\lambda +\lambda_0)^{-1}},
\eean
additionally, for a particular value of $\theta$, we have that
\bean
p(\lambda \given\theta, y_{1:n})&\propto& p(\lambda, \theta, y_{1:n})\\
&=& p(y_{1:n} \given \lambda, \theta) p(\lambda)p(\theta)\\
&\propto& p(y_{1:n} \given \lambda, \theta) p(\lambda)\\
&\propto& \lrp{\lambda^{n/2}\e{-\frac{\lambda}{2}\sum_{i=1}^n(y_i-\theta)^2}} \lrp{\lambda^{\alpha/2 -1}\e{-\lambda\frac{\beta}{2}}}\\
&\propto& \lambda^{(n+\alpha)/2 -1 }\e{-\lambda \frac{1}{2}\lrp{\sum_{i=1}^n(y_i-\theta)^2+\beta}},
\eean
or in other words $$\lambda \given\theta, y_{1:n} \sim \text{Gamma}\lrp{\frac{n+\alpha}{2}, \frac{1}{2}\lrp{\sum_{i=1}^n(y_i-\theta)^2+\beta}}.$$

As such, we can sample from each of these distributions if we know the other parameter. And so, if we have a value of $\theta$ we may sample a value for $\lambda$, and we if we have a value for $\lambda$, we can then sample a new value for $\theta$.  This heuristic enables us to define the following iterative algorithm:
\benum
\item Define a starting value for $\lambda$, which we will denote by $\lambda^{(0)}$.
\item Let $k=0,\ldots,S$ for some sufficiently large integer $S$.
\item Sample $\theta^{(k+1)}$ from $p(\theta \given\lambda^{(k)}, y_{1:n})$
\item Sample $\lambda^{(k+1)}$ from $p(\lambda \given\theta^{(k+1)}, y_{1:n})$
\item Set $\phi^{(k+1)}=(\theta^{(k+1)},\lambda^{(k+1)}).$
\item If $k<S$, set $k=k+1$ and go back to step (3).
\eenum

And so, we have built a two-variable Gibbs Sampling (GS) algorithm, where the densities $p(\theta \given\lambda^{(k)}, y_{1:n})$ and $p(\lambda^{(k+1)} \given\theta^{(k+1)}, y_{1:n})$, are known as the full conditional densities for $\btheta$ and $\blambda$, respectively.  After running the GS, the outcome is the sequence $$\lrb{\phi^{(1)},\phi^{(2)},\ldots,\phi^{(S)}}$$

The code for the GS algorithm of the Normal likelihood problem with semi-conjugate priors is provided below for the midge wing-length problem.  Before coding the problem, note that to simplify calculations we can re-express the term $\sum_{i=1}^n(y_i-\theta)^2$ in the scale parameter of the full-conditional for $\lambda$, as follows
\bean
\sum_{i=1}^n(y_i-\theta)^2&=& \sum_{i=1}^n (y_i-\bar{y} + \bar{y} - \theta)^2\\
 &=& \sum_{i=1}^n (y_i-\bar{y})^2 + 2\sum_{i=1}^n(y_i-\bar{y}) (\bar{y} - \theta) + \sum_{i=1}^n(\bar{y} - \theta)^2\\
&=& \sum_{i=1}^n (y_i-\bar{y})^2 + 2(\bar{y} - \theta)\underbrace{\sum_{i=1}^n(y_i-\bar{y})}_{n \bar{y} - n \bar{y}=0}  + n(\bar{y} - \theta)^2\\
&=& \sum_{i=1}^n (y_i-\bar{y})^2 +  n(\bar{y} - \theta)^2\\
&=& (n-1) S_n^2 +  n(\bar{y} - \theta)^2,
\eean
where $S_n^2=\frac{\sum_{i=1}^n (y_i-\bar{y})^2}{n-1}$. Since $\bar{y}$ and $S_n^2$ are fixed, this simple calculation reduces $n$ sums of squares to the simple sum above, where the only component that needs updating is $n(\bar{y} - \theta)^2$.

<<echo=T, message=F, warning=F>>=
set.seed(1)
S<-2000
PHI<-matrix(nrow=S,ncol=2)
PHI[1,]<-phi<-c( mean.y, 1/var.y)
mu0<-1.9  ; t20<-0.95^2
lambda0 <- 1/t20
beta<-.01 ; alpha<-1

### Gibbs sampling
for(s in 2:S) {
  
  # generate a new theta value from its full conditional
  mu.n <-  ( mu0*lambda0 + n*mean.y*phi[2] ) / ( lambda0 + n*phi[2] )
  lambda.n <- ( lambda0 + n*phi[2] )
  phi[1] <- rnorm(1, mu.n, sqrt(1/lambda.n) )
  
  # generate a new lambda value from its full conditional
  alpha.n<- alpha+n
  beta.n<- (beta + (n-1)*var.y + n*(mean.y-phi[1])^2 )
  phi[2]<- rgamma(1, alpha.n/2, beta.n/2)
  
  PHI[s,]<-phi         
}
###
@

After running the algorithm for the midge wing length problem, we can track the GS  draws in the three panels in Figure \ref{fig:GibbsDrawsNormal}, which display the Gibbs Sampler moves after 5, 15 and 100 iterations. 

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=12, fig.height=4, fig.align='center'>>=
par(mfrow=c(1,3),mar=c(2.75,2.75,.5,.5),mgp=c(1.70,.70,0))
m1<-5
plot( PHI[1:m1,],type="l",xlim=range(PHI[1:100,1]), ylim=range(PHI[1:100,2]),
      lty=1,col="gray",xlab=expression(theta),ylab=expression(tilde(sigma)^2))
text(  PHI[1:m1,1], PHI[1:m1,2], c(1:m1) )

m1<-15
plot( PHI[1:m1,],type="l",xlim=range(PHI[1:100,1]), ylim=range(PHI[1:100,2]),
      lty=1,col="gray",xlab=expression(theta),ylab=expression(tilde(sigma)^2))
text(  PHI[1:m1,1], PHI[1:m1,2], c(1:m1) )

m1<-100
plot( PHI[1:m1,],type="l",xlim=range(PHI[1:100,1]), ylim=range(PHI[1:100,2]),
      lty=1,col="gray",xlab=expression(theta),ylab=expression(tilde(sigma)^2))
text(  PHI[1:m1,1], PHI[1:m1,2], c(1:m1) )

sseq<-1:1000
sseq2<-1000:2000#c(200,1000,4)

### Gibbs based credible intervals
CI.gibbstheta <- round(quantile(PHI[sseq2,1],probs=c(0.025,0.975)),4)
CI.gibbslambda <- round(quantile(PHI[sseq2,2],probs=c(0.025,0.975)),4)

### t-test based confidence interval
n<-length(y) ; ybar<-mean(y) ; s2<-var(y)
t.CI <- ybar+qt( c(.025,.975), n-1) *sqrt(s2/n)
@
\caption{Sequence of Gibbs sampling draws}
\label{fig:GibbsDrawsNormal}
\end{figure}

Now, from the samples obtained using the Gibbs sampler in figure \ref{fig:Gibbsnormal} we overlayed the points sampled over the discrete approximation we obtained earlier for the joint and they match quite accurately.  Furthermore, we obtain the 95\% credible interval estimate for $\btheta$ %and for $\blambda$ 
is $(\theta_{2.5},\theta_{97.5})_{gibbs}=(\Sexpr{CI.gibbstheta})$.  Compare these values to those derived from frequentist confidence interval built using the $t$ distribution, which is given by $(\theta_{2.5},\theta_{97.5})_{freq}=(\Sexpr{round(t.CI,4)})$. % and $(\lambda_{2.5},\lambda_{97.5})_{gibbs}=(\Sexpr{CI.gibbslambda})$, respectively.
Although we summarize information with the credible intervals, notice that after drawing the samples we have access to the entire joint and marginal posterior densities for both parameters; which provides considerably more information about the parameters than what is available from the classical approaches.

{\scriptsize
\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=12, fig.height=4, fig.align='center'>>=

par(mfrow=c(1,3),mar=c(2.75,2.75,.5,.5),mgp=c(1.70,.70,0))
image( mean.grid,prec.grid,post.grid,col=gray( (10:0)/10 ),
       xlab=expression(theta), ylab=expression(tilde(sigma)^2) ,
       xlim=range(PHI[,1]),ylim=range(PHI[,2]) )
points(PHI[sseq,1],PHI[sseq,2],pch=".",cex=1.25 )

plot(density(PHI[,1],adj=2),  xlab=expression(theta),main="",
     xlim=c(1.55,2.05),
     ylab=expression( paste(italic("p("),
                            theta,"|",italic(y[1]),"...",italic(y[n]),")",sep="")))

abline(v=CI.theta,lwd=2,col="gray")
abline( v= t.CI, col="black",lwd=1)

plot(density(PHI[,2],adj=2), xlab=expression(tilde(sigma)^2),main="",
     ylab=expression( paste(italic("p("),tilde(sigma)^2,"|",italic(y[1]),
                            "...",italic(y[n]),")",sep=""))) 


@
\caption{Joint posterior draws and estimated marginal posterior densities}
\label{fig:Gibbsnormal}
\end{figure}
}

Some useful visual tools to assess the behavior of the algorithm are:
\bdes
\item[Scatterplot.] The scatterplot shows us what the posterior distribution looks like. 
\item[Traceplots.] A traceplot simply shows the sequence of samples, for instance $\phi^{(1)}\phi^{(2)},\ldots,\phi^{(S)}$. Traceplots are a simple but very useful way to visualize how the sampler is behaving. If the traceplot indicates that the sampler is getting stuck on or near particular values, it is said to have poor mixing. %The traceplots in Figure 2(a) look very healthy—the sampler doesn’t appear to be getting stuck anywhere.
\item[Running averages.] Running averages display the value of the sample mean up to the $k$th draw for each $k=1,2,\ldots,S$. This plot provides a useful heuristic for visually assessing the convergence of the Markov chain. If the running average eventually flattens out and stays that way, this is an indication that the estimate has converged.
\edes

{\scriptsize
\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=8, fig.height=7, fig.align='center'>>=

par(mfrow=c(2,2),mar=c(2.75,2.75,.5,.5),mgp=c(1.70,.70,0))
plot(PHI[,1],  ylab=expression(theta^{(k)}),main="",pch=20,cex=0.7,
     xlab="Gibbs iteration (k)")
plot(PHI[,2],  ylab=expression(lambda^{(k)}),main="",pch=20,cex=0.7,
     xlab="Gibbs iteration (k)")
plot(cumsum(PHI[,1])/(1:S),  ylab=expression(theta^{(k)}),main="",
     type="l",col="cornflowerblue",lwd=2,xlab="Gibbs iteration (k)")
plot(cumsum(PHI[,2])/(1:S),  ylab=expression(lambda^{(k)}),main="",
     type="l",col="cornflowerblue",lwd=2,xlab="Gibbs iteration (k)")


@
\caption{Traceplots and runnning average plots for Gibbs draws $\theta^{(k)}$ and $\lambda^{(k)}$.}
\label{fig:dxnormal}
\end{figure}
}


\subsection{General Gibbs Sampling algorithm for more than two variables}

We saw how to use Gibbs sampling for distributions with two variables, e.g., $p(x,y)$. The generalization to more than two variables is completely straightforward, in brief, we cycle through the variables, sampling each from its conditional distribution given all the rest.
For instance, for a distribution with three random variables, say, $p(x,y,z)$, we set $x, y$, and $z$ to some initial values, and sample $x|y,z$, then $y|x,z$, then $z|x, y$, then $x|y,z$, and so on. This procedure can be summarized as

\benum
\item[0.] Set $(x^{(0)}, y^{(0)}, z^{(0)})$ to some starting value.
\item[1.] Sample $x^{(1)} \sim p(x|y^{(0)}, z^{(0)})$.

Sample $y^{(1)} \sim p(y|x^{(1)}, z^{(0)})$.

Sample $z^{(1)} \sim p(z|x^{(1)}, y^{(1)})$.

\item[2.] Sample $x^{(2)} \sim p(x|y^{(1)}, z^{(1)})$.

Sample $y^{(2)} \sim p(y|x^{(2)}, z^{(1)})$.

Sample $z^{(2)} \sim p(z|x^{(2)}, y^{(2)})$.

\item[3.] $\qquad\qquad\qquad\vdots$
\eenum
In general, to approximate with Gibbs Sampling the distribution with $m$ random variables, say, $p(\nu_1,\ldots , \nu_m)$; at each iteration of the algorithm we sample from
\bean
\nu_1\given\nu_2,\nu_3,&\ldots&,\nu_m\\
\nu_2 \given \nu_1,\nu_3,&\ldots&,\nu_m \\
&\vdots&\\
\nu_m \given \nu_1,\nu_2,&\ldots&,\nu_{m-1} 
\eean
conditioning always on the most recent values of all the other variables. The conditional distribution of a variable given all of the others is again the full conditional in this context, and for brevity this is sometimes denoted $\nu_i\given \cdots$.

\subsection{Example: Censored Data}

More frequently than we would like, in real-world problems some of the data is either missing altogether or is partially obscured. Gibbs sampling provides a coherent fully Bayesian strategy to deal with these situations. In these cases the missing or distorted variables are sampled along with the parameters, providing information about the actual values of the missing/obscured data.

A way in which data is usually partially obscured is by \emph{censoring}, meaning that we only know that a data point lies in some particular interval, but we don’t observe the value exactly. Censored data is typically found in medical research (since for instance, the researchers may lose contact with some of the patients), and also in engineering (since some measurements may exceed the lower or upper limits of the measurement instrument used).

Let's work through a censoring example. Consider a study regarding the length of life (lifetime) following a specific medical intervention (e.g., a new drug to treat heart disease). Suppose a study of 12 patients was conducted, recording the number of years after the intervention and before death for each is.  The observed values are
$$3.4, 2.9, 1.2+, 1.4, 3.2, 1.8, 4.6, 1.7+, 2.0+, 1.4+, 2.8, 0.6+$$
where $x+$ indicates that the patient was alive after $x$ years but contact with the patient stopped at $x$. In this type of study, other factors (e.g., patient age at baseline) are important, and obviously, there will always also be a control group, but let’s focus on one group and ignore baseline characteristics to keep things simple. Consider the following model:

\bean
\btheta&\sim&\text{Gamma}(a,b)\\
Z_1,\ldots,Z_n \given \theta &\overset{iid}{\sim}& \text{Gamma}(r,\theta)\\
Y_i&=&\left\{\bmat Z_i&\text{if }Z_i\leq c_i \\ *&\text{if }Z_i> c_i \emat\right.,
\eean
where ``*'' is a special character used to denote that censoring has occurred.  Notice that $c_i$ is known  only if censoring occurs. Notice that the $Z_i$'s are the actual responses, but instead of observing these directly we only have access to the obscured version $Y_i$.

The goal of this problem is to conduct inference on $\btheta$, the rate parameter for the lifetime distribution.  Tackling this problem directly is hard, we would need to find $p(\theta\given y_{1:n})\propto p(y_{1:n}\given \theta)p(\theta)$, which implies integrating out $z_{1:n}$.  For the censored observations (i.e., where $y_i=*$), the likelihood is $$p(y_i\given \theta)=\Pr(Y_i=*\given \theta)=\Pr(Z_i>c_i\given \theta)=1-F_{Z_i}(c_i),$$
where $F_{Z_i}$ is the cdf for the Gamma$(r,\theta)$ random variable.  And so $p(y_i\given \theta)$ has a very complicated form.

So, instead of having to jump through hoops trying to get this done, let's build a Gibbs Sampler, which is is easy for this challenging problem.

\bsh
\vspace{10cm}
\esh

\bsh
\vspace{22cm}
\esh

{\small
<<echo=T>>=
y <- c(3.4, 2.9, NA, 1.4, 3.2, 1.8, 4.6, NA, NA, NA, 2.8, NA)
n <- length(y)

c.vals <- c(1.2, 1.7, 2.0, 1.4, 0.6)
nc <- length(c.vals) #number of censored obs

#define censored set
J.set <- which(is.na(y))

#def hyperparameters
a <- b <- 1; r <- 2

#initialize z's and theta
z <- y
z[J.set] <- c.vals
theta <- rgamma(1,shape=(a+n*r),rate=(b+sum(z)))

#cdf evaluated at c.i
Fz.nc <- pgamma(c.vals,shape=r,rate=theta)
uvec <- rep(NA,length(J.set))


#Gibbs Sampelr variables
S <- 2000
gibbs.mat <- matrix(NA,ncol=(nc+1),nrow=S)

#Gibbs Sampler
for(k in 1:S){
  
  ###sample z's
  uvec <- runif(nc, min = Fz.nc, max = rep(1,nc))
  z[J.set] <- qgamma(uvec,shape=r,rate=theta)
  
  ###sample theta
  theta <-  rgamma(1,shape=(a+n*r),rate=(b+sum(z)))
  
  ###store draws
  gibbs.mat[k,] <- c(z[J.set],theta) 
}


@
}

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=6, fig.height=8, fig.align='center'>>=
burnin <- 1:(S/2)

par(mfrow=c(3,2),mar=c(2.75,2.75,.5,.5),mgp=c(1.70,.70,0))
plot(gibbs.mat[-burnin,2],  ylab=expression(z[8]^{(k)}),main="",pch=20,cex=0.7,
     xlab="Gibbs iteration (k)")
plot(gibbs.mat[-burnin,(nc+1)],  ylab=expression(theta^{(k)}),main="",pch=20,cex=0.7,
     xlab="Gibbs iteration (k)")
plot(cumsum(gibbs.mat[-burnin,2])/(1:(S/2)),  ylab=expression(E(Z[8])),main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
plot(cumsum(gibbs.mat[-burnin,(nc+1)])/(1:(S/2)),  ylab=expression(E(theta)),main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
hist(gibbs.mat[-burnin,2],  xlab=expression(paste("est. density for ",z[8])),
     main="",col="cornflowerblue")
hist(gibbs.mat[-burnin,(nc+1)],  xlab=expression(paste("est. density for ",theta)),
     main="",col="cornflowerblue")
abline(v=quantile(gibbs.mat[-burnin,(nc+1)],c(0.025,0.975)),col="red",lwd=2)
@
\caption{Traceplots, runnning average plots, and histograms for Gibbs draws of $\btheta$ and $Z_{8}$.}
\label{fig:dxcensored}
\end{figure}

\subsection{Example: data Augmentation (on your own)}

A commonly-used technique for designing MCMC samplers is to use data augmentation, also known as auxiliary variables. The idea is to introduce a new variable (or variables) $Z$ that depends on the distribution of the existing variables in such a way that the resulting conditional distributions, with $Z$ included, are easier to sample from and/or result in better mixing (faster convergence). So, the $Z$’s are essentially latent/hidden variables that are introduced for the purpose of simplifying/improving the sampler. For instance, suppose we want to sample from $p(x,y)$, but $p(x|y)$ and/or $p(y|x)$ are complicated. If we can choose some $p(z|x, y)$ such that $p(x|y, z)$, $p(y|x, z)$, and $p(z|x, y)$ are easy to sample from, then we can construct a Gibbs sampler to sample all three variables $(X,Y,Z)$ from $p(x,y,z)$ and then just throw away the $Z$’s and we will have samples $(X, Y )$ from $p(x, y)$.

To illustrate, consider the data set consisting of the heights of 695 Dutch women and 562 Dutch men. Suppose we have the list of heights, but we don't know which datapoints are from women and which are from men. Can we still infer the distribution of female heights and male heights, e.g., the mean for males and the mean for females? Perhaps surprisingly, the answer is yes. The reason is that this is a two-component mixture of Normals, and there is an (essentially) unique set of mixture parameters corresponding to any such distribution.

To construct a Gibbs sampler for a mixture model such as this, it is common to introduce an auxiliary variable $Z_i$ for each datapoint, indicating which mixture component it is drawn from. For instance, in this example, $Z_i$ would indicate whether subject $i$ is female or male. This results in a Gibbs sampler that is quite easy to derive and implement.

To keep things as simple as possible, let's assume that both mixture components (female and male) have the same precision, say $\lambda$, and that $\lambda$ is fixed and known. Then the usual two-component Normal mixture model is:
\bean
\mu_f, \mu_m &\sim& \tN(\mu_0,\lambda_0^{-1})\\
\pi &\sim&\text{Beta}(a,b)\\
Y_1,\ldots,Y_n\given \mu_f, \mu_m,\pi &\overset{iid}{\sim}& F(\mu_f, \mu_m,\pi),
\eean
where $F(\mu_f, \mu_m,\pi)$ is the distribution with pdf $$f(\mu_f, \mu_m,\pi)=(1-\pi) \tN(\mu_f,\lambda^{-1})+\pi \tN(\mu_m,\lambda^{-1})$$

Let's derive the Gibbs Sampler for this problem

\bsh
\vspace{2cm}
\esh

\bsh
\vspace{20cm}
\esh



\section{MCMC Introduction to Diagnostics}

As we know by now, Monte Carlo (MC) and Markov Chain Monte Carlo (MCMC) methods use sequences of random variable $\bphi^{(1)},\bphi^{(2)},\ldots,\bphi^{(S)}$ to approximate quantities of the form $$E(g(\bphi))=\int g(\phi) p(\phi)d\phi,\quad\text{using}\quad \frac{1}{S}\sum_{k=1}^S g(\bphi^{(k)}).$$

When using MC, the random variables $\bphi^{(1)},\bphi^{(2)},\ldots,\bphi^{(S)}$ are independent and identically distributed from the distribution of interest $p(\phi)$.  As such, these are automatically representative of $p(\phi)$, in the sense that $$\Pr(\bphi^{(k)}\in A)=\int_A p(\phi)d\phi.$$

Conversely, when using MCMC $\bphi^{(1)},\bphi^{(2)},\ldots,\bphi^{(S)}$ are dependent, so that we have instead
$$\Pr(\bphi^{(k)}\in A)\overset{k\rightarrow \infty}\longrightarrow \int_A p(\phi)d\phi.$$

To appreciate the differences between the outcome derived from a MC algorithm and a MCMC one, lt's consider the following example.

\bsh
\bexa {\bemph{Mixture of Normals:}} Suppose that we want to sample from the joint distribution of $\delta\in\lrb{1,2,3}$ and $\theta\in\mathbb{R}$, where the pmf for $\bdelta$ is
\bean
\Pr(\bdelta=r)\,=\,p(r)&=&\left\{\bmat 0.45 & \text{if }r=1\\ 0.1 & \text{if }r=2\\0.45 & \text{if }r=3 \emat \right.,
\eean
and the conditional density for $\btheta$ is $p(\theta\given \delta)=\tN(\mu_{\delta},\sigma_{\delta}^2)$, where
\begin{center}
\begin{tabular}{r | c c}
 & $\mu_\delta$ & $\sigma_\delta^2$\\
\hline
$\delta=1$ & $-3$ & $1/9$ \\
$\delta=2$ & $0$ & $1/9$ \\
$\delta=3$ & $3$ & $1/9$
\end{tabular}
\end{center}

This model implies that the marginal density for $\btheta$ is given by $$p(\btheta)=\sum_{r=1}^3 p(\bdelta=r)\tN(\theta\given \mu_r,\sigma_r^2),$$ and is commonly known as a three component mixture of normal densities, with $\delta$ representing the group membership.

\bdes
\item[MC:] because $p(\theta,\delta)=p(\theta\given \delta)p(\delta)$ and we can sample directly from both $p(\theta\given \delta)$ and from $p(\delta)$ in this case we can use MC to obtain draws from the joint distribution.
\benum
\item Sample $\bdelta^{(1)}, \bdelta^{(2)},\ldots, \bdelta^{(S)}\overset{iid}{\sim} p(\delta)$
\item Sample $\btheta^{(k)} \sim  p(\theta \given \delta^{(k)})$ for $k=1,\ldots,S$
\item The pairs $(\theta^{(1)},\delta^{(1)}),(\theta^{(2)},\delta^{(2)}),\ldots,(\theta^{(S)},\delta^{(S)})$ correspond to draws from $p(\theta,\delta)$.
\eenum

\item[MCMC:] for MCMC we need to derive the full conditional posteriors for each parameter, which are given by
 \benum
 \item Derive the full conditionals:
\bean
p(\bdelta=r \given \theta) &=& \frac{p(\bdelta=r,\theta)}{\sum_{h=1}^3 p(\bdelta=h,\theta)}\\
 &=& \frac{p(\bdelta=r)p(\theta\given \bdelta=r)}{\sum_{h=1}^3 p(\bdelta=h)p(\theta\given \bdelta=h)}\\
 &=& \frac{p(\bdelta=r)\tN (\theta\given \mu_r,\sigma_r^2)}{\sum_{h=1}^3 p(\bdelta=h)\tN (\theta\given \mu_h,\sigma_h^2)}\\
 &&\\
 p(\theta\given \bdelta=r ) &=& \tN (\theta\given \mu_r,\sigma_r^2)
 \eean
 \item Choose a starting value $\delta^{(0)}\in\lrb{1,2,3}$, and for $k=0,1,\ldots,S$
 \item Sample $\theta^{(k+1)}$ from $p(\theta\given \delta^{(k)})$
 \item Sample $\delta^{(k+1)}$ from $p(\delta\given \theta^{(k+1)})$
 \item The pairs $(\theta^{(1)},\delta^{(1)}),(\theta^{(2)},\delta^{(2)}),\ldots,(\theta^{(S)},\delta^{(S)})$ correspond to draws approximately from $p(\theta,\delta)$.
 \eenum
\edes
Now, let's code up and run both algorithms in R.  
\eexa
\esh



\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=8, fig.height=4, fig.align='center'>>=
###### Intro to MCMC diagnostics
mu<-c(-3,0,3)
s2<-c(.33,.33,.33)
w<-c(.45,.1,.45)

ths<-seq(-5,5,length=100)

#### MC Sampling
set.seed(1)
S<-1000
d<-sample(1:3,S, prob=w,replace=TRUE)
th<-rnorm(S,mu[d],sqrt(s2[d]))
THD.MC<-cbind(th,d)
####

### Plot MC marginal density for theta and compare to true marginal
par(mfrow=c(1,1),mar=c(3,3,1,1),mgp=c(1.75,.75,0))
ths<-seq(-6,6,length=1000)
plot(ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
       w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
       w[3]*dnorm(ths,mu[3],sqrt(s2[3])) ,type="l" , xlab=expression(theta),ylab=
       expression( paste( italic("p("),theta,")",sep="") ),lwd=2 ,ylim=c(0,.40))
hist(THD.MC[,1],add=TRUE,prob=TRUE,nclass=20,col="gray")
lines( ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
         w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
         w[3]*dnorm(ths,mu[3],sqrt(s2[3])),lwd=2 )
@
\caption{Marginal density approximated using MC draws}
\label{fig:MCres}
\end{figure}

\bsh
As you can see in Figure \ref{fig:MCres}, the approximation to the marginal density of $\btheta$ obtained using MC with $S=1000$ is quite accurate, closely approximating the true marginal pdf. 
\esh

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=8, fig.height=4, fig.align='center'>>=

#### MCMC sampling
th<-0
THD.MCMC<-NULL
S<-10000
set.seed(1)
for(s in 1:S) {
  d<-sample(1:3 ,1,prob= w*dnorm(th,mu,sqrt(s2))   )
  th<-rnorm(1,mu[d],sqrt(s2[d]) )
  THD.MCMC<-rbind(THD.MCMC,c(th,d) )
}

par(mfrow=c(1,2))

Smax=1000
plot(THD.MCMC[1:Smax,1],cex=0.5,main="",ylab="theta values")
lines( mu[THD.MCMC[1:Smax,2]])
plot(ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
       w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
       w[3]*dnorm(ths,mu[3],sqrt(s2[3])) ,type="l" , xlab=expression(theta),ylab=
       expression( paste( italic("p("),theta,")",sep="") ),lwd=2 ,ylim=c(0,.40))
hist(THD.MCMC[1:Smax,1],add=TRUE,prob=TRUE,nclass=20,col="gray")
lines( ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
         w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
         w[3]*dnorm(ths,mu[3],sqrt(s2[3])),lwd=2 )
#lines( mu[THD.MCMC[,2]])

###
@
\caption{Results MCMC with $S=1000$}
\label{fig:MCMCres1K}
\end{figure}

\bsh
Conversely, when using MCMC, also with $S=1000$, it is clear that the algorithm has not been able to capture representatively the local modes.  Thsi is an indication that it was not run long enough. If we now increase $S\in\lrb{2000, 5000}$, we observe how the approximation to the marginal density of $\btheta$ improves dramatically.
\esh

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=8, fig.height=7, fig.align='center'>>=

par(mfrow=c(2,2))
Smax=c(2000,5000)
plot(THD.MCMC[1:Smax[1],1],cex=0.5,main="",ylab="theta values")
lines( mu[THD.MCMC[1:Smax[1],2]])
plot(ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
       w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
       w[3]*dnorm(ths,mu[3],sqrt(s2[3])) ,type="l" , xlab=expression(theta),ylab=
       expression( paste( italic("p("),theta,")",sep="") ),lwd=2 ,ylim=c(0,.40))
hist(THD.MCMC[1:Smax[1],1],add=TRUE,prob=TRUE,nclass=20,col="gray")
lines( ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
         w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
         w[3]*dnorm(ths,mu[3],sqrt(s2[3])),lwd=2 )

plot(THD.MCMC[1:Smax[2],1],cex=0.5,main="",ylab="theta values")
lines( mu[THD.MCMC[1:Smax[2],2]])
plot(ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
       w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
       w[3]*dnorm(ths,mu[3],sqrt(s2[3])) ,type="l" , xlab=expression(theta),ylab=
       expression( paste( italic("p("),theta,")",sep="") ),lwd=2 ,ylim=c(0,.40))
hist(THD.MCMC[1:Smax[2],1],add=TRUE,prob=TRUE,nclass=20,col="gray")
lines( ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
         w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
         w[3]*dnorm(ths,mu[3],sqrt(s2[3])),lwd=2 )

# plot(THD.MCMC[1:Smax[3],1],cex=0.5,main="",ylab="theta values")
# lines( mu[THD.MCMC[1:Smax[3],2]])
# plot(ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
#        w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
#        w[3]*dnorm(ths,mu[3],sqrt(s2[3])) ,type="l" , xlab=expression(theta),ylab=
#        expression( paste( italic("p("),theta,")",sep="") ),lwd=2 ,ylim=c(0,.40))
# hist(THD.MCMC[1:Smax[3],1],add=TRUE,prob=TRUE,nclass=20,col="gray")
# lines( ths, w[1]*dnorm(ths,mu[1],sqrt(s2[1])) +
#          w[2]*dnorm(ths,mu[2],sqrt(s2[2])) +
#          w[3]*dnorm(ths,mu[3],sqrt(s2[3])),lwd=2 )
#lines( mu[THD.MCMC[,2]])

###
@
\caption{Results MCMC with $S\in\lrb{2000,5000}$}
\label{fig:MCMCres10K}
\end{figure}

\bsh
The reason for this behavior corresponds to (i) the fact that the draws from the MCMC algorithm are correlated, and (ii) the distribution that we are sampling from is not the exact distribution we are interested in but an approximation that becomes better and better as we sample more and more.
\esh

\bsh
\note Some take home messages from this problem:
\bit
\item[1.] Choose good starting values when using MCMC
\item[2.] $S$ needs to be suitably large to ensure that the MCMC has converged to the target (stationary) distribution $p(\theta,\delta)$. Having large $S$ large enough guarantees that:
\bit
\item[a.] even regions with low probability are visited
\item[b.] all high probability regions  (i.e., regions surrounding local modes) are visited representatively.
\eit
\eit
\esh

It is easy see more formally why having dependent draws affects convergence speed.  Suppose that we were interested im estimating $E(\bphi)=\phi_0$, and that, as usual, we approximate this value with $\bar{\bphi}_S=\frac{1}{S}\sum_{k=1}^S \bphi^{(k)}$.

\bdes
\item[MC:] Recall that when we use the MC strategy
$$\text{var}_{MC}(\bar{\bphi}_S)=\frac{1}{S}\text{var}(\bphi)$$

\item[MCMC:] Now with MCMC, we have
\bean
\text{var}_{MCMC}(\bar{\bphi}_S)&=& E\lrp{(\bar{\bphi}_S-\phi_0)^2}\\
&=& E\lrp{\lrp{\frac{1}{S}\sum_{k=1}^S \bphi^{(k)}-\phi_0}^2}\\
&=& E\lrp{\frac{1}{S^2}\lrp{\sum_{k=1}^S \bphi^{(k)}-S\phi_0}^2}\\
&=& \frac{1}{S^2} E\lrp{\lrp{\sum_{k=1}^S (\bphi^{(k)}-\phi_0)}^2}\\
&=& \frac{1}{S^2} \lrp{\sum_{k=1}^S E\lrp{(\bphi^{(k)}-\phi_0)^2}+\underset{\ell\not=h}{\sum\sum} E\lrp{(\bphi^{(\ell)}-\phi_0)(\bphi^{(h)}-\phi_0)} }\\
&=&  \frac{1}{S} \text{var}(\bphi) + \frac{1}{S^2}\underset{\ell\not=h}{\sum\sum} E\lrp{(\bphi^{(\ell)}-\phi_0)(\bphi^{(h)}-\phi_0)}\\
&=&\text{var}_{MC}(\bphi)+ \frac{1}{S^2}\underset{\ell\not=h}{\sum\sum} \text{cov}(\bphi^{(\ell)},\bphi^{(h)})
\eean
\edes

Given that the second term in the last line above is almost always positive, the variance for the estimator obtained from MCMC algorithms is larger than that from MC, so it is no surprise that the results from MCMC we obtained for the same $S$ as with the MC agorithm were worse.  The higher the autocorrelation in the chain, the larger the MCMC variance and so the worse the approximation will be.

The degree of correlation between the samples can be assessed with the autocorrelation function (acf).  For a sequence of of numbers $\phi^{(1)}, \phi^{(2)}, \ldots, \phi^{(S)}$ the lag-$t$ autocorrelation function provides a measure of how correlated observations $t$ steps apart are.  The lag-$t$ acf is given by $$\text{acf}_t(\phi)=\frac{\frac{1}{S-t}\sum_{k=1}^{S-t}(\phi^{(k)}-\bar{\phi})(\phi^{(k+t)}-\bar{\phi}) }{\frac{1}{S-1}\sum_{k=1}^{S}(\phi^{(k)}-\bar{\phi})^2}.$$

In the three component mixture problem, we have a 10-lag and a 50-lag autocorrelation for $\btheta$ of \Sexpr{round(acf(THD.MCMC[,1],lag.max=10)$acf[10],4)} and \Sexpr{round(acf(THD.MCMC[,1],lag.max=50)$acf[50],4)}, respectively.  The acf can take values between -1 and 1, with values close to 1 indicating high correlation between the elements of the sequence, and that the movements across the parameter space are happening slowly (slow mixing of the chain), and so more MCMC draws will be required to get a good approximation. 

An important notion that helps us assess if we have sufficient samples to obtain a good approximation is the {\bemph{effective sample size}}.  In the extreme, consider the situation in which all values are exactly the same (i.e., we have a an acf eual to 1).  Because there is perfect correlation, this is equivalent to having a single observation (i.e., an effective sample size of 1).  Conversely, whenever the acf is close to 0 (independent draws), this will imply that each one is providing a new piece of information regarding the distribution of the variable.

So, in summary, the effective sample size tells us to how many MC draws $S$ MCMC draws would correspond to.  The effective sample size is given by $$\text{ESS}=\frac{S}{1+2\sum_{t=1}^\infty \text{acf}_t(\phi)}.$$

We can obtain this value using the function \textsf{effectiveSize} from the \textsf{coda} package in \textsf{R}.  For the 3 component normal mixture problem with $S=5000$ we have an ESS$_\theta=\Sexpr{round(coda::effectiveSize(THD.MCMC[,1]),4)}$ for $\btheta$.  This is telling us that $S=5000$ MCMC draws from this algorithm yield a precision equivalent to that from $S\approx 18$ from a MC algorithm, which clearly is far from optimal.

\end{document}