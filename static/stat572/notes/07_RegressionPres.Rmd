---
title: 'STAT 572: Bayesian Statistics'
author: "Prof. Taylor-Rodriguez"
subtitle: Regression Analysis
output:
  beamer_presentation:
    includes:
      in_header: "preamble.tex"
  ioslides_presentation: default
  slidy_presentation: default
---

## Announcements

\benum
\item Please take 5 minutes to fill out the course evaluation survey.

\bigskip
\item Three more lectures to go including this one!

\bigskip
\item We'll cover regression in two sessions and we have one more lecture.  We can either do: 

\bit
\item a small part of Hierarchical Modeling, 
\item more on prior specification, 
\item or cover in detail Bayesian Variable Selection.
\eit
YOUR PICK
\eenum


## Regression Modeling

\begin{block}{Connects generating distribution of a rv $Y$ (support in $\mathbb{R}$) to one or more \emph{predictors} $\bx = (x_1, \ldots,x_p)\in \mathbb{R}^p$.}

\bit
\item Let $p(y \given \bx)$ denote the generating distribution of the rv $Y$ conditional on predictors $\bx$

\bigskip
\item Consider realizations $y_1,y_2, \ldots, y_n$ of the rv's $$Y_1\given \bx_1, Y_2\given \bx_2, \ldots, Y_n \given \bx_n$$

\bigskip
\item \textcolor{blue}{GOAL: Estimate $p(y \given \bx)$}
\eit

\end{block}


## Regression Modeling

Connection between $Y$ and $\bx = (x_{1},\ldots,x_{p})\in\mathbb{R}^p$:

\begin{itemize}
\item Assume $p(y \given \bx)$ varies smoothly with $\bx$, that is $$g:\mathbb{R}^p\rightarrow \mathbb{R},\;\text{with}\quad E[Y\given \bx] = g(\bx).$$

\item Most commonly, we assume that $g$ is a the linear function
\end{itemize}

\begin{eqnarray*}
E[Y\given \bx, \bbeta] &=& g(\bx; \bbeta) \;=\;\int_{y\in \mathbb{R}} y p(y\given \bx) dy \\
&=&\bbeta' \bx = \beta_1 x_1 + \beta_2 x_2 +\cdots \beta_p x_p.
\end{eqnarray*}

\textcolor{blue}{NOTE: this model is linear in $\bbeta$, the $x$'s can be anything!}


## Example: Oxygen Uptake

\bit
\item Sample of $n=12$ healthy men who DO NOT exercise regularly

\bigskip
\item Want to assess effect of oxygen uptake of 2 exercise regiments

\bigskip
\item 6 men assigned at random to 12-week flat-terrain running

\bigskip
\item 6 remaining, assigned to 12-week step aerobics

\eit

## Example: Oxygen Uptake

\bit
\item Response ($y$): change in max oxygen uptake (lt$/$min) measured on treadmill before and after trt.

\bigskip
\item Predictors ($\bx$):
\bdes
\item[$x_1=1$:\hfill] Intercept

\smallskip
\item[$x_2$:\hfill] $x_2=1$ if trt$\equiv$aerobics, $x_2=0$ o.w.

\smallskip
\item[$x_3$:\hfill] age (treated as continuous)

\smallskip
\item[$x_4=x_2*x_3$:\hfill] interaction of treatment and age
\edes
\eit

## Example: Oxygen Uptake

For $i=1,\ldots,12$, the model is

\vspace{-0.7cm}
\begin{eqnarray*}
Y_i &=& g(\bx_i;\bbeta)+\varepsilon_i\\
&=&\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3+\beta_4 x_4+\varepsilon_i\\
\text{with}&&\varepsilon_1,\varepsilon_2,\ldots,\varepsilon_n \stackrel{iid}{\sim}N(0,\sigma^2).
\end{eqnarray*}
Note that:
\begin{eqnarray*}
E[Y_i\given x_{1i}, x_{2i}=0,x_{3i}, x_{4i}=0, \bbeta]&=&\beta_1+\beta_3 x_{3i},\quad\text{and}\\
E[Y_i\given x_{1i}, x_{2i}=1,x_{3i}, x_{4i}=x_{3i}, \bbeta]&=&\beta_1+\beta_2+\beta_3 x_{3i}+\beta_4 x_{3i}\\
&=&\beta_1+\beta_2+(\beta_3+\beta_4) x_{3i}.
\end{eqnarray*}


## Example: Oxygen Uptake

\textcolor{blue}{The randomness in $Y_i$ is entirely induced by $\varepsilon_i$!!}, implying

$$Y_i\given \bx_i,\bbeta,\sigma^2 \sim N(g(\bx_i; \bbeta), \sigma^2)=N(\bbeta' \bx_i, \sigma^2),$$
with $Y_1,\ldots, Y_n$ independent... 

\centering
\textcolor{blue}{BUT NOT IDENTICALLY DISTRIBUTED!}


## Likelihood for the regression problem

As such, we have that
\small{
\begin{eqnarray*}
p(y_{1:n}\given \bx_{1:n},\bbeta,\sigma^2)&=&\prod_{i=1}^{n} (2\pi\sigma^2)^{-1/2} e^{-\frac{1}{2 \sigma^2}(y_{i}-\bbeta' \bx)^2}\\
&=&(2\pi\sigma^2)^{-n/2} e^{-\frac{1}{2 \sigma^2}\sum_{i=1}^{n} (y_{i}-\bbeta' \bx)^2}
\end{eqnarray*}
}
Letting $\by=(y_1, y_2, \ldots, y_n)'$ and $X = (\bx_1,\bx_2,\ldots,\bx_n)'$, we can represent this likelihood as 
\small{
\begin{eqnarray*}
p(y_{1:n}\given \bx_{1:n},\bbeta,\sigma^2)&=& p(\by \given X,\bbeta,\sigma^2)\\
&=&(2\pi)^{-n/2} |\sigma^2\text{I}_n|^{-1/2} e^{-\frac{1}{2 \sigma^2}(\by-X \bbeta)' (\sigma^2\text{I}_n)^{-1}(\by-X \bbeta)}
\end{eqnarray*}
}

Look familiar?

## Likelihood for the regression problem

It should, this is the density for the $n$-variate normal distribution $$\bY\given X, \bbeta,\sigma^2\sim N_n(X\bbeta,\sigma^2 \text{I}_n),$$ since 
$$E[\bY\given X,\bbeta]=X\bbeta = \lrsqb{\bmat \bbeta'\bx_1 \\ \bbeta'\bx_2 \\ \vdots \\ \bbeta'\bx_n \emat}= \lrsqb{\bmat E[Y_1 \given \bx_1,\bbeta] \\ E[Y_2 \given \bx_2,\bbeta] \\ \vdots \\ E[Y_n \given \bx_n,\bbeta] \emat}$$


## Likelihood for the regression problem

The likelihood $p(y_{1:n}\given \bx_{1:n},\bbeta,\sigma^2)$ is maximized over $\bbeta$ when
\small{
\begin{eqnarray*}
SSR(\bbeta) &=&\sum_{i=1}^n (y_i -\bbeta' \bx_i)^2\\
&=& (\by-X\bbeta)'(\by-X\bbeta)\\
&=& \by'\by-2\bbeta' X' \by + \bbeta' (X'X)\bbeta
\end{eqnarray*}
is minimized.  Deriving w.r.t. $\bbeta$ and setting to 0, we have
\begin{eqnarray*}
\frac{d SSR(\bbeta)}{d\bbeta} &=& -2 X' \by + (X'X)\bbeta \;\stackrel{set}{=}\;0\\
\frac{d SSR(\bbeta)}{d\bbeta} &=& 0 \Longleftrightarrow \hat{\bbeta}_{OLS}\;=\; (X' X)^{-1} X' \by. 
\end{eqnarray*}
\textcolor{blue}{This is the OLS estimate for $\bbeta$} (unique if $(X'X)^{-1}$ exists).
}

## Bayesian inference for the Regression problem
### Semiconjugate priors for $\bbeta$ and $\sigma^2$

As a function of $\bbeta$, we have that
$$p(\by \given X,\bbeta,\sigma^2)\propto_{\bbeta} e^{-\frac{1}{2\sigma^2}\lrp{\bbeta' (X'X)\bbeta - 2\bbeta' X' \by }},$$ so assuming $\bbeta\sim N_p(\bbeta_0,\Sigma_0)$, the full conditional posterior is
\begin{eqnarray*}
p(\bbeta\given X,\sigma^2,\by) &\propto_{\bbeta}& p(\by\given X,\bbeta,\sigma^2) p(\bbeta)\\
&\propto_{\bbeta}&  \lrp{e^{-\frac{1}{2\sigma^2}\lrp{\bbeta' (X'X)\bbeta - 2\bbeta' X' \by }}} \lrp{e^{-\frac{1}{2}\lrp{\bbeta' \Sigma_0^{-1}\bbeta - 2\bbeta' \Sigma_0^{-1}\bbeta_0 }}}\\
&\propto_{\bbeta}& \lrp{e^{-\frac{1}{2}\lrp{\bbeta' \lrp{\frac{1}{\sigma^2}(X'X) + \Sigma_0^{-1}}\bbeta - 2\bbeta'\lrp{\frac{1}{\sigma^2}X' \by+\Sigma_0^{-1}\bbeta_0} }  }}
\end{eqnarray*}


## Bayesian inference for the Regression problem
### Semiconjugate priors for $\bbeta$ and $\sigma^2$
So, letting 
\begin{eqnarray*}
\Sigma_{\bbeta}&=&\lrp{\frac{1}{\sigma^2}(X'X) + \Sigma_0^{-1}}^{-1}\;\text{and}\\
\bmu_{\bbeta} &=&  \frac{1}{\sigma^2}(X' \by)+\Sigma_0^{-1}\bbeta_0,
\end{eqnarray*}

we have that 

$$\bbeta\given X,\sigma^2,\by \sim N_p( \Sigma_{\bbeta} \bmu_{\bbeta},\Sigma_{\bbeta}).$$
\vspace{2cm}

## Bayesian inference for the Regression problem
### Semiconjugate priors for $\bbeta$ and $\sigma^2$

Assuming that a priori $\sigma^2\sim \text{IG}\lrp{\frac{\nu_0}{2},\frac{\nu_0\sigma_0^2}{2}}$, we have
\small{
\begin{eqnarray*}
p(\sigma^2\given X,\bbeta,\by) &\propto_{\sigma^2}& p(\by\given X,\bbeta,\sigma^2) p(\sigma^2)\\
&\propto_{\sigma^2}& \lrp{(\sigma^2)^{-\frac{n}{2}} e^{-\frac{1}{2\sigma^2}(\by-X\bbeta)'(\by-X\bbeta)}} \times \\
&& \qquad\lrp{(\sigma^2)^{-\frac{\nu_0}{2}-1}e^{-\frac{1}{\sigma^2} \frac{\nu_0\sigma_0^2}{2} }}\\
&\propto_{\sigma^2}& (\sigma^2)^{-\frac{n+\nu_0}{2}-1}\times \\
&&\qquad \text{exp}\lrb{-\frac{1}{\sigma^2}
\frac{(\by-X\bbeta)'(\by-X\bbeta)+\nu_0\sigma_0^2}{2}},
\end{eqnarray*}}

Implying that $$\sigma^2\given X,\bbeta,\by\sim \text{IG}\lrp{\frac{n+\nu_0}{2},\frac{1}{2}\lrp{(\by-X\bbeta)'(\by-X\bbeta)+\nu_0\sigma_0^2} }$$

## Gibbs Sampler for Bayesian Regression

Init. $(\sigma^{2})^{(0)}$, calculate fixed quantities: $X'X$, $(X'X)^{-1}$, $X' \by$, $\Sigma_0^{-1}.$

\begin{block}{For $k=1,\ldots,S$}
\begin{enumerate}
\item Update $\bbeta=\bbeta^{(k)}$
\begin{itemize}
\item Update $\Sigma_{\bbeta}^{(k)}=\lrp{\frac{1}{(\sigma^2)^{(k-1)}}(X'X) + \Sigma_0^{-1}}^{-1}$
\item Update $\frac{1}{(\sigma^2)^{(k-1)}} (X' \by) +\Sigma_0^{-1}\bbeta_0$
\item Sample $\bbeta^{(k)}$ from $N_p( \Sigma_{\bbeta} \bmu_{\bbeta},\Sigma_{\bbeta})$
\end{itemize}

\item Update $\sigma^2=(\sigma^2)^{(k)}$
\begin{itemize}
\item Sample $(\sigma^2)^{(k)}$ from $\text{IG}\lrp{\frac{n+\nu_0}{2},\frac{1}{2}\lrp{(\by-X\bbeta^{(k)})'(\by-X\bbeta^{(k)})+\nu_0\sigma_0^2} }$
\end{itemize}

\end{enumerate}
\end{block}


## Example: Oxygen Uptake

```{r echo=F,fig.height=3.5, fig.width=4}
o2uptake<-data.frame(dget("yX.o2uptake.txt"))
with(o2uptake,plot(x=age,y=uptake,col=c("black","grey")[aerobic+1],pch=20)) 
legend("topleft",legend=c("running","aerobic"),cex=0.8,
       col=c("black","grey"),pch=20,bty="n")
```


## Example: Oxygen Uptake

To analyze this problem with the framework we developed we need $$(\bbeta_0,\Sigma_0)\quad\text{and}\quad(\nu_0,\sigma_0^2).$$
Specification of these parameters can be hard! To illustrate:

\begin{itemize}
\item A scan of the literature reveals that males in their 20's is $\sim 150$  lt/min
\item With sd 15 lt/min, so
\end{itemize}
$$150\pm2\times 15=(120,180)$$
\begin{itemize}
\item Changes in oxygen uptake should be somewhere in $(-60,60)$ with high prob.
\end{itemize}

## Example: Oxygen Uptake
\small{
\begin{itemize}
\item So, to start we at least need $$\beta_1+\beta_3 x_{3}\in (-60,60)$$ with high prob.
\item Some algebra reveals that $$-300< \beta_1 <300\quad\text{and}\quad -12< \beta_3 <12$$
\end{itemize}
$$150\pm2\times 15=(120,180)$$
\begin{itemize}
\item We can induce this behavior by setting $$\text{var}(\beta_1)=150^2\quad\text{and}\quad \text{var}(\beta_3)=6^2$$
\end{itemize}
\textcolor{blue}{But what about the variances for the other parameters and all covariances?}
}

## Default and weakly informative priors

\begin{itemize}
\item Ideally, prior should provide meaningful prior information.

\bigskip
\item In many cases there is none, or not enough.

\bigskip
\item If so, better to not contaminate data with arbitrary priors

\bigskip
\item Let the data speak as loud as possible
\end{itemize}


## Default and weakly informative priors

\begin{block}{Three examples of such priors:}
\begin{itemize}
\item Unit information prior

\bigskip
\item Jeffreys' priors 

\bigskip
\item $g$-priors
\end{itemize}
\end{block}

These all strive to minimize the influence of the prior over posterior inference


## Unit Information Prior

\begin{block}{IDEA: have prior contain the strength of one observation.}
\scriptsize{
\begin{eqnarray*}
\ell(\bbeta, \sigma^2|\by)&=& \text{ln}\lrp{(2\pi \sigma^2)^{-n/2} e^{-\frac{1}{2\sigma^2}\|\by - X\bbeta \|^2 }}\\
&\propto_{\beta,\sigma^2}& -\frac{n}{2}\ln{\sigma^2} -\frac{1}{2\sigma^2}\lrp{\bbeta' (X'X)\bbeta - 2\bbeta' X' \by + \by'\by}\\
&=& -\frac{n}{2}\ln{\sigma^2} -\frac{1}{2\sigma^2}\lrp{(\bbeta-(X' X)^{-1} X' \by)'(X'X)(\bbeta-(X' X)^{-1} X' \by) }\\
&&{}\qquad\qquad -\frac{1}{2\sigma^2}\lrp{\by' \by - \by' X (X' X)^{-1} X' \by}\\
&=& -\frac{n}{2}\ln{\sigma^2} -\frac{1}{2\sigma^2}(\bbeta-\hat{\bbeta}_{OLS})'(X'X)(\bbeta-\hat{\bbeta}_{OLS}) \\
&&{}\qquad\qquad -\frac{1}{2\sigma^2}\lrp{\by'(I_n - H_x)\by} \hspace{2.5cm}(*)
\end{eqnarray*}
}
\end{block}

\normalsize{
$(*)$ represents info of $n$ observations.  Now rescale to emulate influence of only one observation
}

## Unit Information Prior

\begin{block}{First, exponentiate $(*)$}
\small{
\begin{eqnarray*}
L(\bbeta, \sigma^2|\by) &\propto_{\beta,\sigma^2}& (\sigma^2)^{-\frac{n}{2}} \text{exp}\lrb{-\frac{1}{2\sigma^2}(\bbeta-\hat{\bbeta}_{OLS})'(X'X)(\bbeta-\hat{\bbeta}_{OLS}) }\\
&&{}\qquad\qquad \text{exp}\lrb{-\frac{1}{2\sigma^2}\by'(I_n - H_x)\by}\\
&=& (\sigma^2)^{-\frac{p}{2}} \text{exp}\lrb{-\frac{1}{2\sigma^2}(\bbeta-\hat{\bbeta}_{OLS})' (X'X) (\bbeta-\hat{\bbeta}_{OLS}) }\\
&&{}\qquad\qquad (\sigma^2)^{-\frac{n-p-2}{2}-1}\text{exp}\lrb{-\frac{1}{2\sigma^2}\by'(I_n - H_x)\by}\\
&\propto_{\beta,\sigma^2}& N_p\lrp{\bbeta\given \hat{\bbeta}_{OLS}, \sigma^2(X'X)^{-1}}\\
&&{} \times\; \text{IG}\lrp{\sigma^2\given \frac{n-p-2}{2}, \frac{1}{2}\text{SSR}( \hat{\bbeta}_{OLS})}.
\end{eqnarray*}
}
\end{block}

## Unit Information Prior

\begin{block}{Second, re-scale to unit information}
\footnotesize{
\begin{eqnarray*}
p(\bbeta, \sigma^2) &=& p(\bbeta \given \sigma^2)p(\sigma^2)\\
&=& N_p\lrp{\bbeta\given \hat{\bbeta}_{OLS}, n \sigma^2(X'X)^{-1}} \\
&&\qquad \times\; \text{IG}\lrp{\sigma^2\given \frac{1}{2}, \frac{1}{n}\lrp{\frac{1}{2}\text{SSR}( \hat{\bbeta}_{OLS})}}.
\end{eqnarray*}
}
\end{block}
\textcolor{red}{Warning: double use of data -- cannot represent real prior beliefs}

\begin{block}{This prior leads to the posterior (convince yourself)}
\small{
\begin{eqnarray*}
p(\bbeta, \sigma^2|\by) &=&  N_p\lrp{\bbeta\given \hat{\bbeta}_{OLS}, \frac{n}{n+1} \sigma^2(X'X)^{-1}} \\
&&\qquad \times\; \text{IG}\lrp{\sigma^2\given \frac{n+1}{2}, \frac{n+1}{n}\lrp{\frac{1}{2}\text{SSR}( \hat{\bbeta}_{OLS})}}.
\end{eqnarray*}
}
\end{block}

## Jeffreys' Prior

\begin{block}{IDEA: build prior invariant to paramerization.}
\begin{itemize}
\item Treat $\bbeta$ and $\sigma^2=\phi^{-1}$ independently (orthogonal parametrization)

\bigskip
\item For ease of derivation, let's use $\phi=1/\sigma^2$ 

\bigskip
\item Uses the Expected Fisher Information matrix $$\mathcal{J}(\btheta)=- E\lrsqb{\frac{d^2 \ell(\btheta \given \by)}{d\btheta d\btheta'} },\;\text{where}\; \btheta=(\bbeta',\phi)'$$ 

\bigskip
\item The priors are given by
$$p_J(\bbeta)\propto \sqrt{\text{det}(\mathcal{J}(\bbeta))}\;\text{and}\;p_J(\phi)\propto \sqrt{\text{det}(\mathcal{J}(\phi))}$$

\end{itemize}

\end{block}



## Jeffreys' Prior

\begin{block}{IDEA: build prior invariant to paramerization.}
\small{
\begin{eqnarray*}
\ell(\bbeta, \phi|\by)&=& \frac{n}{2}\ln{\phi} -\frac{\phi}{2}(\bbeta-\hat{\bbeta}_{OLS})'(X'X)(\bbeta-\hat{\bbeta}_{OLS}) \\
&&{}\qquad\qquad -\frac{\phi}{2}\text{SSR}(\hat{\bbeta}_{OLS})
\end{eqnarray*}
\begin{eqnarray*}
\frac{d^2 \ell(\btheta \given \by)}{d\btheta d\btheta'}&=& \lrsqb{\bmat -\phi (X' X)& - (X' X)(\bbeta -\hat{\bbeta}_{OLS}) \\
- (\bbeta -\hat{\bbeta}_{OLS})'(X' X) & -\frac{n}{2}\phi^{-2}\emat}
\end{eqnarray*}
}
\end{block}

## Jeffreys' Prior

\begin{block}{IDEA: build prior invariant to paramerization.}

\begin{eqnarray*}
\frac{d^2 \ell(\btheta \given \by)}{d\btheta d\btheta'}&=& \lrsqb{\bmat -\phi (X' X)& - (X' X)(\bbeta -\hat{\bbeta}_{OLS}) \\
- (\bbeta -\hat{\bbeta}_{OLS})'(X' X) & -\frac{n}{2}\phi^{-2}\emat}
\end{eqnarray*}

\bigskip
Therefore:
$$\mathcal{J}(\btheta) = \lrsqb{\bmat \phi (X' X)& \0_p \\
\0_p' & \frac{n}{2}\phi^{-2}\emat}$$

\end{block}
Hence, $$\mathcal{J}(\bbeta)=\phi (X' X)\;\text{and}\;\mathcal{J}(\phi)=\frac{n}{2}\phi^{-2}.$$




## Jeffreys' Prior

\begin{block}{Therefore:}

\begin{eqnarray*}
p_J(\bbeta)&\propto_\beta& \sqrt{\text{det}(\phi X' X)} \;\propto_{\beta}\; 1,\;\text{and}\\
p_J(\phi)&\propto_{\phi}& \sqrt{\text{det}(n\phi^{-2}/2)} \;\propto_{\phi}\; \phi^{-1},
\end{eqnarray*}
\end{block}

\begin{block}{Implying that:}
$$p_{J}(\bbeta,\phi)=p_{J}(\bbeta)p_{J}(\phi)\propto_{\bbeta,\phi} \;\phi^{-1}.$$
\end{block}


## Jeffreys' Prior

\begin{block}{Hence, the posterior densities are}
\begin{eqnarray*}
\bbeta \given \phi ,\by &\sim& N_p(\hat{\bbeta}_{OLS}, \phi^{-1}(X'X)^{-1})\\
\bphi \given \by &\sim& \text{Gamma}\lrp{\frac{n-p}{2}, \frac{1}{2}\|\by-X\hat{\bbeta}_{OLS} \|^2}
\end{eqnarray*}
\end{block}

\begin{block}{Note that marginally, we have}
$$\bbeta \given \by \sim t_{n-p}(\hat{\bbeta}_{OLS}, \hat{\sigma}^{2}(X'X)^{-1})$$
\end{block}

Bayesian CI's match frequentist confidence regions, but these priors cannot be used for model selection.

<!-- ## Jeffreys' Prior -->

<!-- \begin{block}{Some comments about these priors} -->
<!-- \begin{itemize} -->
<!-- \item Invariant to transformations -->

<!-- \bigskip -->
<!-- \item It's optimal from the information theoretic standpoint (minimum description length) -->

<!-- \bigskip -->
<!-- \item Violate strong likelihood principle -->
<!-- \end{itemize} -->
<!-- \end{block} -->




## Zellner's $g$-prior

Another popular regression prior is the $g$-prior: $$\bbeta\given \sigma^2 \sim N_p(\bb_0,g \sigma^2 (X'X)^{-1}),$$ which leads to posteriors $$\bbeta\given \sigma^2, \by \sim N_p\lrp{\frac{1}{1+g}\bb_0+\frac{g}{1+g}\hat{\bbeta}_{OLS},\frac{g}{g+1} \sigma^2 (X'X)^{-1}}.$$

Letting $\bb_0=0_p$ leads to a transformation invariant prior. 

## Zellner's $g$-prior

If $\sigma^2\sim \text{IG}(\frac{\nu_0}{2},\frac{\nu_0\sigma_0^2}{2})$, then 
$$\sigma^2\given \by \sim \text{IG}(\frac{\nu_0+n}{2},\frac{\nu_0\sigma_0^2 + SSR_g}{2}),$$ where 
\begin{eqnarray*}
SSR_g &=& \by' \lrp{I_n -\frac{g}{g+1}X (X' X)^{-1}X'}\by \\
\stackrel{g\rightarrow\infty}{\longrightarrow}&&\by' \lrp{I_n -X (X' X)^{-1}X'}\by = SSR(\hat{\bbeta}_{OLS})
\end{eqnarray*}

\vspace{0.5cm}
\centering
\textcolor{blue}{Because we can sample directly from $p(\sigma^2\given \by)$ and $p(\bbeta\given \sigma^2, \by)$ a direct MC sampler can be built!}

## Example: Oxygen Uptake

\bit
\item Response ($y$): change in max oxygen uptake (lt$/$min) measured on treadmill before and after trt.

\bigskip
\item Predictors ($\bx$):
\bdes
\item[$x_1=1$:\hfill] Intercept

\smallskip
\item[$x_2$:\hfill] $x_2=1$ if trt$\equiv$aerobics, $x_2=0$ o.w.

\smallskip
\item[$x_3$:\hfill] age (treated as continuous)

\smallskip
\item[$x_4=x_2*x_3$:\hfill] interaction of treatment and age
\edes
\eit

## Example: Oxygen Uptake


```{r echo=F, message=F, eval=T, warning=F, fig.width=3.5, fig.height=3, fig.align='center'}
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))

x1<-c(0,0,0,0,0,0,1,1,1,1,1,1)
x2<-c(23,22,22,25,27,20,31,23,27,28,22,24)
y<-c(-0.87,-10.74,-3.27,-1.97,7.50,-7.25,17.05,4.96,10.40,11.05,0.26,2.51)

par(mfrow=c(1,1))
plot(y~x2,pch=16,xlab="age",ylab="change in maximal oxygen uptake", 
     col=c("black","gray")[x1+1],cex.lab=0.7)
legend(27,0,legend=c("aerobic","running"),pch=c(16,16),col=c("gray","black"))

```

