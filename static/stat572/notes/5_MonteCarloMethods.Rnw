\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espaÃ±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{float}

\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbs}[1]{\boldsymbol{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\renewcommand{\d}{\text{d}}
\newcommand{\by}{\mbf{y}}
\newcommand{\mts}{\tilde{Y}}
\newcommand{\mtsv}{\tilde{\bv}}
\newcommand{\btw}{\tilde{\bw}}
\newcommand{\bhw}{\hat{\bw}}
\newcommand{\btx}{\tilde{\bx}}
\newcommand{\pt}{\tilde{p}}
\newcommand{\ba}{\mbf{a}}
\newcommand{\bb}{\mbf{b}}
\newcommand{\bc}{\mbf{c}}
\newcommand{\bd}{\mbf{d}}
\newcommand{\boe}{\mbf{e}}
\newcommand{\bk}{\mbf{k}}
\newcommand{\bq}{\mbf{q}}
\newcommand{\br}{\mbf{r}}
\newcommand{\bs}{\mbf{s}}
\newcommand{\bh}{\mbf{h}}
\newcommand{\bff}{\mbf{f}}
\newcommand{\bt}{\mbf{t}}
\newcommand{\bu}{\mbf{u}}
\newcommand{\bm}{\mbf{m}}
\newcommand{\bv}{\mbf{v}}
\newcommand{\bx}{\mbf{x}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\bz}{\mbf{z}}
\newcommand{\tby}{\tilde{\mbf{y}}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\bp}{\mbs{p}}
\newcommand{\bA}{\mbf{A}}
\newcommand{\tbA}{\tilde{\bA}}
\newcommand{\bB}{\mbf{B}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\tc}{\tilde{c}}
\newcommand{\tC}{\tilde{C}}
\newcommand{\tbC}{\tilde{\bC}}
\newcommand{\bF}{\mbf{F}}
%\newcommand{\bBs}{\mbf{B}^\star}
\newcommand{\bD}{\mbf{D}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bK}{\mbf{K}}
\newcommand{\bQ}{\mbf{Q}}
\newcommand{\bV}{\mbf{V}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\hN}{\hat{N}}
\newcommand{\tbc}{\tilde{\mbf{c}}}
\newcommand{\tba}{\tilde{\mbf{a}}}
\newcommand{\tbX}{\tilde{\mbf{X}}}
\newcommand{\tbW}{\tilde{\mbf{W}}}
\newcommand{\bSigma}{\mbs{\Sigma}}
\newcommand{\bGamma}{\mbs{\Gamma}}
\newcommand{\bUps}{\mbs{\Upsilon}}
\newcommand{\bPsi}{\mbs{\Psi}}
\newcommand{\vs}{v^\star}
\newcommand{\Vs}{V^\star}
\newcommand{\bvs}{\bv_i^\star}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bP}{\mbf{P}}
\newcommand{\bBs}{\bB^\star}
\newcommand{\bXs}{\bX^\star}
\newcommand{\bxs}{\bx^\star}
\newcommand{\bWs}{\bW^\star}
\newcommand{\bws}{\bw_i^\star}
\newcommand{\bwsp}{\left.\bw_i^\star\right.^\prime}
\newcommand{\bBsp}{\left.\bB^\star\right.^\prime}
\newcommand{\bVs}{\bV^\star}
\newcommand{\bpsi}{\mbs{\psi}}
\newcommand{\bphi}{\mbs{\phi}}
\newcommand{\bmu}{\mbs{\mu}}
\newcommand{\bbeta}{\mbs{\beta}}
\newcommand{\bxi}{\mbs{\xi}}
\newcommand{\bchi}{\mbs{\chi}}
\newcommand{\blambda}{\mbs{\lambda}}
\newcommand{\bLambda}{\mbs{\Lambda}}
\newcommand{\LamT}{\tilde{\Lambda}}
\newcommand{\GamT}{\tilde{\Gamma}}
\newcommand{\WT}{\tilde{W}}
\newcommand{\balpha}{{\mbs{\alpha}}}
\newcommand{\bepsilon}{{\mbs{\varepsilon}}}
\newcommand{\bgamma}{{\mbs{\gamma}}}
\newcommand{\btheta}{{\mbs{\theta}}}
\newcommand{\bseta}{{\mbs{\eta}}}
\newcommand{\bpi}{{\mbs{\pi}}}
\newcommand{\bI}{\mbf{I}}
\newcommand{\bH}{\mbf{H}}
\newcommand{\tbH}{\tilde{\mbf{H}}}
\newcommand{\1}{\mbs{1}}
\newcommand{\0}{\mbs{0}}
\newcommand{\detstar}[1]{\text{det}^+\left(#1\right)}
\renewcommand{\det}[1]{\text{det}\left(#1\right)}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\renewcommand{\exp}[1]{\text{exp}\left[#1\right]}
\newcommand{\M}{{M}}
\newcommand{\K}{{K}}
\newcommand{\peq}{{p}}
\newcommand{\MB}{{M_B}}
\newcommand{\MF}{{M_F}}
\newcommand{\MT}{{M_T}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\graph}{\Gamma}
\newcommand{\kset}{\Upsilon}
\newcommand{\order}{order}
\newcommand{\parents}{\mcal{P}}
\newcommand{\children}{\mcal{C}}
\newcommand{\extreme}{\mcal{E}}
\newcommand{\combined}{\mcal{A}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left\{#1\right\}}
\newcommand{\lrno}[1]{\left.#1\right.}
\newcommand{\lrsqb}[1]{\left[#1\right]}
\newcommand{\G}[1]{\Gamma_{#1}}
\newcommand{\N}{\mcal{N}}
\renewcommand{\d}{\text{d}}
\newcommand{\Ps}[1]{\Pr{\left(#1\right)}}
\newcommand{\BF}[2]{{BF}_{#1,#2}(Y)}
\newcommand{\BFd}[3]{{BF}_{#1,#2}(Y,#3)}
\newcommand{\xmark}{\ding{55}}
\newcommand{\ben}{\begin{equation*}}
\newcommand{\een}{\end{equation*}}
\newcommand{\bean}{\begin{eqnarray*}}
\newcommand{\eean}{\end{eqnarray*}}
\newcommand{\bsm}{\begin{smallmatrix}}
\newcommand{\esm}{\end{smallmatrix}}
\newcommand{\bmat}{\begin{matrix}}
\newcommand{\emat}{\end{matrix}}
\newcommand{\tI}{\text{I}}
\newcommand{\tN}{\text{N}}
\newcommand{\trN}{\text{trunc.N}}
\newcommand{\nl}[1]{\text{log}{\lrp{#1}}}
\newcommand{\e}[1]{\text{exp}{\lrb{#1}}}
\newcommand{\indf}[1]{\tI_{\left\{#1\right\}}}
\newcommand{\parent}{\mcal{P}}
\newcommand{\gp}{\text{GP}{\lrp{\0,\mcal{C}(\cdot\given\bphi)}}}
\newcommand{\gpd}[3]{\text{GP}_{#1}{\lrp{\0,\mcal{C}_{#2}(\cdot\given\phi_{#3})}}}
\newcommand{\mnngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mbs{\mcal{C}}}_{#2}(\cdot,\cdot;\bphi_{#3})}}}
\newcommand{\nngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mcal{C}}_{#2}(\cdot,\cdot;\phi_{#3})}}}
\newcommand{\nngpw}[4]{\text{NNGP}_{#1}^{#2}{\lrp{0,\tilde{\mcal{C}}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpw}[4]{\text{GP}_{#1}^{#2}{\lrp{0,\mcal{C}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpq}{\text{GP}_q{\lrp{\0,\mbs{\mcal{C}}(\cdot\given\bphi)}}}
\newcommand{\gpk}{\text{GP}{\lrp{0,\mcal{C}(\cdot\given\tilde{\phi}_k)}}}
\newcommand{\nngp}{\text{NNGP}{\lrp{\0,\tilde{\mcal{C}}(\cdot\given\phi)}}}
\newcommand{\refset}{\mcal{T}}
\newcommand{\uset}{\mcal{U}}
\newcommand{\oset}{\mcal{T}}
\newcommand{\Xall}{\mbb{X}}
\newcommand{\given}{\,|\,}
\newcommand{\dtr}[1]{\textcolor{blue}{(#1)}}
\newcommand{\bemph}[1]{\bf \emph{#1}}
\newcommand{\var}[1]{\text{var}{\left(#1\right)}}

%-------------------------------
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{exa}{Example}
\newtheorem{note}{Note}
\newcommand{\bex}{\begin{exer}}
\newcommand{\eex}{\end{exer}}
\newcommand{\bexa}{\begin{exa}}
\newcommand{\eexa}{\end{exa}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\bdes}{\begin{description}}
\newcommand{\edes}{\end{description}}

\newcommand{\bsh}{\begin{shaded}}
\newcommand{\esh}{\end{shaded}}
%-------------------------------

%-------------------------------
%hiding proof solutions
%-------------------------------


\begin{document}


\setcounter{section}{0}
\title{Monte Carlo Methods}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf Monte Carlo Methods}\\
{\large STAT 572: Bayesian Statistics}\\
Fall 2018
\end{center}
\section{Motivation}

As we saw in the Pygmalion example in the \emph{Univariate Normal Distribution} notes, samples from a distribution of interest can be used to compute Bayes estimators; in fact, using samples often is the only viable alternative. 
For example, in the Pygmalion problem, what do you think was the challenge in calculating $\Pr(\bmu_p>\bmu_c \given x_{1:n_p}, y_{1:n_c})$ directly?
%For example, we are often interested in calculating $\Pr(\btheta \in A \given y_{1:n})$ for an arbitrary region $A\in\Theta$, or we might want to estimate the mean, variance and other moments for some function of $\btheta$.  Exact calculation of many of these posterior quantities is impossible.


The popularity of sampling based approaches is owed to their flexibility and easy of use.  These methods are devised to approximate solutions to the problem of calculating expressions of the form 
$$E(g(Y))=\int g(y) p(y) dy\quad\text{ or }\quad E(g(Y))=\sum_y g(y) p(y),$$
where $p(y)$ corresponds to a pdf for continuous $Y$, and a pmf for discrete $Y$ --to ease the notation, for now we omit the explicit dependence on parameters. The idea at play is that if one obtains enough samples, say $Y_1,Y_2,\ldots,Y_S\overset{iid}{\sim} p(\cdot)$ the expectation above can be approximated from 
$$E(g(Y))\approx \frac{1}{S}\sum_{k=1}^S g(Y_k).$$

This expression may at a first glance have the deceptive appearance of being limited, but as we will see, it's applicability is surprisingly broad. In fact, the majority of the statistical inferential problems we are interested in can be expressed in a form similar to the one above. 

For many of the most computationally challenging problems we face in statistics, sampling based approaches are often the only pragmatic solution.  All of these problems correspond to intractable integrals or sums. Sampling methods are actually quite general, reliable and work in very complex high-dimensional settings. This is not to say that sampling methods are without fault. We will discuss some challenges associated to them as we move forward in the discussion.%The main drawbacks of these approaches are that it may be difficult to obtain actual samples from the distributions we are interested in, the accuracy of the estimates in some situations may be difficut to evaluate, and they may be slow to converge.

Throughout this section, we will explore other examples of objects that can be approximated using samples, such as: \emph{posterior means}, \emph{variances}, \emph{posterior densities}, \emph{fit statistics}, etc.  First let's lay down some of the theoretical notions that underlie Monte Carlo.

\section{Monte Carlo Sampling}

In the simplest possible situation, consider the random variable $Y$ with pdf or pmf $p$, and say we are interested in estimating its expectation $E(Y)$.  As you have seen in other statistics courses, regardless of the distribution, if we have a random sample $Y_1,Y_2,\ldots,Y_S \sim p$ a suitable estimator for the mean is simply $$\frac{1}{S}\sum_{k=1}^S Y_k.$$

This simple notion lays the foundation for the more general situation where we approximate 
$$E\lrp{g(Y)\given Z=z}\approx \frac{1}{S}\sum_{k=1}^S g(Y_k),$$
where $Y_1,\ldots,Y_S \overset{iid}{\sim}p(\cdot \given Z=z)$ with $p(\cdot \given Z=z)$ representing the conditional distribution of $Y_i\given Z=z$. 

\bsh
\note The simpler case of approximating $E(Y)$ is equivalent to this last situation, where our interest lies in approximating $E(g(Y))$.  This is because we can always define a random variable $X$ such that $X=g(Y)\given Z=z$ (i.e., has the same distribution of $g(Y)\given Z=z$) and then approximate $E(X)$ as before.
\esh


\subsection{Some basic properties of Monte Carlo estimators}

\bit
\item If $E(|Y|)<\infty$, then $\frac{1}{S}\sum_{k=1}^S Y_k$ is consistent for $E(Y)$, or in other words $$\frac{1}{S}\sum_{k=1}^S Y_k  \longrightarrow E(Y)\text{ w.p. }1.$$

\item $\frac{1}{S}\sum_{k=1}^S Y_k$ is unbiased for $E(Y)$, that is, $$E\lrp{\frac{1}{S}\sum Y_k}=\frac{1}{S}\sum E(Y_k)\overset{iid}{=} \frac{1}{S}\cdot S\cdot E(Y)=E(Y)$$

\item The variance of $\frac{1}{S}\sum Y_k$ is $$\var{\frac{1}{S}\sum Y_k}\overset{indep}{=} \frac{1}{S^2}\sum \var{Y_k}\overset{id\, dist}{=} \frac{1}{S^2}\cdot \sum \var{Y} = \frac{\var{Y}}{S}$$

\item Finally, the root mean squared error (RMSE) is 
\bean
RMSE&=&\sqrt{E\lrp{\lrsqb{\frac{1}{S}\sum Y_k-E(Y)}^2}},\\
&=&\sqrt{\var{\frac{1}{S}\sum Y_k}},\\
&=&\sqrt{\frac{\var{Y}}{S}}\quad=\quad \sigma(Y)/\sqrt{S},
\eean
where the last line arises from the fact that $\frac{1}{S}\sum Y_k$ is unbiased for $E(Y)$.
\eit

The RMSE enables us to determine how close our estimate is from the true value we are trying to estimate depending on the nuber of samples $S$ that we generate.  In particular, the rate of converegence of $\sum Y_k / S$ is of order $(S)^{-1/2}$.



% \subsection{Some quantities that can be obtained with Monte Carlo methods}
% 
% The Strong Law of Large Numbers (LLN) is the theoretical argument fueling the Monte Carlo approach.  From the Bayesian perspective we are often concerned with functions of our parameters $g(\btheta)$ after having observed the data.  That is, if $\btheta^{(1)}, \btheta^{(2)},\ldots,\btheta^{(S)}$ are iid samples from $p(\theta\given y_{1:n})$, the LLN guarantees that as $S\rightarrow \infty$, with (almost) any function $g(\btheta)$
% $$\frac{1}{S}\sum_k g(\btheta^{(k)}) \overset{S\rightarrow\infty}{\longrightarrow} E(g(\btheta)\given y_{1:n}),$$
% 
% which implies that the following approximations can be made
% \begin{center}
% \begin{tabular}{c  c  c}
% Approximation && Quantity \\
% \hline
%  $\bar{\btheta}=\frac{1}{S}\sum_k \btheta^{(k)}$ &$\rightarrow$ & $E(\btheta\given y_{1:n})$ \\
%  $\hat{\sigma}^2(\btheta)=\frac{1}{S}\sum_k (\btheta^{(k)}-\bar{\btheta})^2$ &$\rightarrow$& $\var{\btheta\given y_{1:n}}$
% \end{tabular}
% \end{center}

\section{Examples}

\subsection{Back to the Pygmalion effect problem}

Recall the Pygmalion experiment, if we want to calculate $\Pr(\bmu_p>\bu_c\given x_{1:n_p},x_{1:n_c})$, letting $B=\lrb{(a,b)\in \mathbb{R}^2: a>b}$ we would need to solve the double integral 
\bean
\Pr(\bmu_p>\bu_c\given x_{1:n_p},x_{1:n_c})&=&\underset{(\mu_p,\mu_c)\in B}{\int\int} p(\mu_p,\mu_c\given x_{1:n_p}, y_{1:n_c}) d\mu_p d\mu_c\\
&=&\int_{-\infty}^\infty\int_{-\infty}^{\infty} \1_{\lrb{(\mu_p,\mu_c)\in B}}p(\mu_p,\mu_c\given x_{1:n_p}, y_{1:n_c}) d\mu_p d\mu_c\\
&=&\int_{-\infty}^\infty\int_{-\infty}^{\infty} \1_{\lrb{\mu_p>\mu_c}}p(\mu_p,\mu_c\given x_{1:n_p}, y_{1:n_c}) d\mu_p d\mu_c\\
&=& E\lrp{\1_{\lrb{\bmu_p>\bmu_c}}\given x_{1:n_p}, y_{1:n_c}}.
\eean
\bsh \note in the original statement of the Pygmalion problem the precision parameters $\lambda_p$ and $\lambda_c$ (or equivalently, the variances $\sigma_p^2$ and $\sigma_c^2$) were unknown.  As such, we would have had to integrate these out to obtain the posterior $p(\mu_p,\mu_c\given x_{1:n_p}, y_{1:n_c})$ used above, so in reality the double integral is a four dimensional integral.
\esh

Because of this last expression, we may instead choose estimate it with:
\bean
\Pr(\bmu_p>\bu_c\given x_{1:n_p},y_{1:n_c})&\approx& \frac{1}{S}\sum_{k=1}^S \1_{\lrb{\bmu_p^{(k)}>\bmu_c^{(k)}}}.
\eean
with the pairs $(\bmu_p^{(k)},\bmu_c^{(k)}) \sim p(\cdot,\cdot \given x_{1:n_p}, y_{1:n_c})$, which bypasses the need to evaluate the complicated bivariate integral directly. 

<<echo=F, message=F, warning=F>>=

#get data and define prior and posterior parameters
pyg.data <- Sleuth3::ex1321
summ.pyg <- with(pyg.data,
                 by(data = Gain, INDICES = Treatment,
                    FUN = function(x){
                      data.frame(n=round(length(x),0),mean=mean(x),sd=sd(x))
                      }))

mu0 <- 0
nu <- 1
alpha <- 1/2
beta <- 100*alpha

#data
x <- pyg.data$Gain[pyg.data$Treatment=="pygmalion"]
y <- pyg.data$Gain[pyg.data$Treatment=="control"]

#data summaries
np <- summ.pyg$pygmalion$n
x.bar <- summ.pyg$pygmalion$mean

nc <- summ.pyg$control$n
y.bar <- summ.pyg$control$mean

#posterior parameters
mu.p.s <- (np/(nu+np))*x.bar
mu.c.s <- (nc/(nu+nc))*y.bar

nu.p <- nu+np
nu.c <- nu+nc

alpha.p <- alpha+np/2
alpha.c <- alpha+nc/2

beta.p <- 0.5*(sum(x^2)+2*beta-nu.p*mu.p.s^2)
beta.c <- 0.5*(sum(y^2)+2*beta-nu.c*mu.c.s^2)

S <- 10^4
rnormgamma <- function(S,a,b,mu,nu){
  #a: scale paramter for gamma component
  #b: rate paramter for gamma component
  #mu: mean parameter for normal component
  #nu: scaling factor for precision of normal component
  
  lambda <- rgamma(S,shape=a,rate=b)
  theta <- rnorm(S,mean=mu,sd=1/sqrt(nu*lambda))
  return(list(lambda=lambda,theta=theta))
}

sample.p.1 <- rnormgamma(S=S,a=alpha.p,b=beta.p,mu=mu.p.s,nu=nu.p)
sample.c.1 <- rnormgamma(S=S,a=alpha.c,b=beta.c,mu=mu.c.s,nu=nu.c)

psi <- mean((sample.p.1$theta>sample.c.1$theta)[-(1:1000)])

@

To visualize the theoretical rate of convergence in this problem, let's proceed step by step:
\bit
\item define $X=g(\bmu_p,\bmu_c)=\1_{\lrb{\bmu_p>\bmu_c}}$, 
\item denote $\psi=\Pr(\bmu_p>\bu_c\given x_{1:n_p},y_{1:n_c})\approx \Sexpr{round(psi,4)}$.  
\eit
With these definitions, note that theoretically we have that $$X\sim \text{Bernoulli}(\psi)\Longrightarrow \var{X}=\psi(1-\psi)\approx \Sexpr{round(psi*(1-psi),3)},$$ and so we can build an approximate Monte Carlo confidence interval for the posterior mean of $X$.  Denoting $\hat{\psi}_S=\frac{1}{S}\sum_{k=1}^S X_i$, then $\var{\hat{\psi}_S}=\var{X}/S=\psi(1-\psi)/S= \Sexpr{round(psi*(1-psi),3)}/S$, so the approximate 95\% Monte Carlo CI for $\psi$ is given by $$\hat{\psi}_S \pm 2 \sqrt{\Sexpr{round(psi*(1-psi),3)}/S}.$$

Figure \ref{fig:runave} illustrates how the confidence interval for $\hat{\psi}_S$ varies as $S$ grows.

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=6, fig.height=4, fig.align='center'>>=
runnav <- cumsum(sample.p.1$theta>sample.c.1$theta)/(1:S)
ll <- runnav - 2*sqrt(psi*(1-psi)/(1:S))
ul <- runnav + 2*sqrt(psi*(1-psi)/(1:S))
plot(x=1:S,y=runnav,type="l",
     ylim=range(c(ll[-(1:100)],ul[-(1:100)])),
     xlab="number of samples (S)",
     ylab=expression(psi[S]))
abline(h=psi,col="red",lwd=2)
lines(ll,col="cornflowerblue",lty=3)
lines(ul,col="cornflowerblue",lty=3)
#legend()
@
\label{fig:runave}\caption{Mean and 95\% CI for $\psi$ as $S$ grows.}
\end{figure}



\subsection{For general functions $g(Y)$}

When we have a general function of $Y$, say $g(Y)$, letting $X=g(Y)$, if the distribution of $X$ can be sampled from directly easily, all needed is to draw $X_1,\ldots,X_S$ and operate as before to obtain the quantities of interest.  However, often it is easier to draw samples $Y_1,Y_2,\ldots,Y_S$ iid from the same distribution as $Y$, from which we then calculate $X_1=g(Y_1), X_2=g(Y_2),\ldots,X_S=g(Y_S)$, (i.e., a sample from the same distribution as $X$), where the estimators using the calculated $X$'s have analogous properties as those described in section 2.1. 

The Strong Law of Large Numbers guarantees that, as $S\rightarrow\infty$ 
$$\bar{g}_S=\frac{1}{S}\sum_k g(Y_{k}) \longrightarrow E(g(Y)),$$ 
and because $$\var{\bar{g}_S}=\frac{1}{S}\var{g(Y)}=E\lrp{\frac{1}{S} \lrsqb{g(Y)-E(g(Y))}^2 },$$ letting $h(Y)=\frac{1}{S} \lrsqb{g(Y)-E(g(Y))}^2$, we may estimate $\var{\bar{g}_S}$ using the Monte Carlo approach, as follows
\bean
\var{\bar{g}_S}\,=\, E(h(Y)) &\approx& \frac{1}{S} \sum_{k=1}^S \lrp{\frac{1}{S}\lrsqb{g(Y_k)-E(g(Y))}^2}\\
&\approx& \frac{1}{S^2} \sum_{k=1}^S \lrp{g(Y_k)-\bar{g}_S}^2
\eean

\bsh
\note In the Hoff book, $\var{\bar{g}_S}$ is instead approximated with
$$\var{\bar{g}_S}\approx \frac{1}{S}\lrp{\frac{1}{S-1} \sum_{k=1}^S \lrp{g(Y_k)-\bar{g}_S}^2}$$
\esh

\subsection{Approximating and sampling from the posterior predictive density}

As we have discussed before, a predictive density is a probability distribution for a new random variable $\tilde{Y}$ that is conditional on observed quantities (e.g., the data $y_{1:n}$ used in fitting) and where the unknowns (the parameters) have been integrated out; this distribution is essentially used to generate predictions. Let's see how we can use the Monte Carlo method to approximate a posterior predictive density.

Recall that the posterior predictive density for a new observation $\tilde{Y}$ is defined as (under the assumption of conditional independence, i.e., $\tilde{Y}\given \theta, y_{1:n} = \tilde{Y}\given \theta$)
$$p(\tilde{y}\given y_{1:n})= \int_{\theta\in \Theta} p(\tilde{y}\given \theta) p(\theta\given y_{1:n}) d\theta.$$

Because of it's integral form, the posterior predictive density can be setup as the following expectation
\bean
p(\tilde{y}\given y_{1:n}) &=& E\lrp{p(\tilde{y}\given \btheta) \given y_{1:n}}.
\eean
As such, for a particular value of $\tilde{y}$, we can approximate the posterior predictive density at $\tilde{y}$ with the Monte Carlo method as follows
\bean
p(\tilde{y}\given y_{1:n})&\approx& \frac{1}{S} \sum_{k=1}^S p(\tilde{y}\given \btheta_k),
\eean
where in the last expressions $\btheta_1,\ldots,\btheta_S \overset{iid}{\sim}p(\theta\given y_{1:n})$.  

Now, it is often the case that the posterior predictive distribution is hard to sample from directly, but using the Monte Carlo procedure we can generate samples from $p(\tilde{y}\given y_{1:n})$, as follows. For $k=1,2,\ldots,S$:
\benum
\item Sample $\theta_k$ from $p(\theta\given y_{1:n})$.
\item Sample $\tilde{y}_k$ from $p(\tilde{y}\given \theta_k)$.
\eenum
This yields the sequence of $S$ independent bivariate samples $\lrb{(\theta_1,\tilde{y}_1),\ldots,(\theta_S,\tilde{y}_S)}$ drawn from the joint posterior distribution of $(\btheta,\tilde{Y})$ given $Y_{1:n}=y_{1:n}$.  Dropping the $\theta_k$'s, the values $\lrb{\tilde{y}_1,\ldots,\tilde{y}_S}$ constitute $S$ independent draws from the  (marginal) posterior predictive $p(\tilde{y}\given y_{1:n})$.  This is equivalent to integrating out $\btheta$.

\subsection{Using the posterior predictive density to assess model fit}

The posterior predictictive distribution has been found to have other uses, in particular, it can provide guidance regarding model suitability (Guttman, 1964; Rubin, 1985). Let's illustrate how this is done using an example from the book.

Throughout the 90's the General Social Survey gathered data about 155 women of age 40 at the time of the survey.  These women were in their 20's during the 70's, which is considered a period of historically low fertility in the US.  Assume that $Y_1,\ldots,Y_{n}\overset{iid}{\sim}\theta\sim \text{Poisson}(\theta)$ denote the number of children that each of the $n$ women have.

Recall that for $n$ observations assumed to come from a $\text{Poisson}(\theta)$ generating model, the likelihood is
$$p(y_{1:n}\given \theta) =  \theta^{\sum y_i} e^{- n\theta } \prod_i \lrp{1/y_i!}\prod_{i=1}^n\1_{\lrb{y_i}\in\mathbb{N}_0},$$
which when assuming a Gamma$(a,b)$ prior on $\btheta$ yields the posterior
$$p(\theta\given y_{1:n}) \propto  \theta^{\sum y_i + a -1} e^{- \theta(n+b) }\1_{\lrb{\theta>0}}\Longrightarrow \btheta\given y_{1:n}\sim\text{Gamma}(\sum y_i + a,n+b).$$

It turns out that the posterior predictive distribution in this case for a new observation is $$\text{NegBinom}(\sum y_i + a, (b+n)/(b+n+1)).$$ 

However, as shown in the previous section, we may as well use Monte Carlo methods to sample values $\theta_k$ from $p(\theta\given y_{1:n})$ and then sample $\tilde{y}_k$ from $p(\tilde{y}\given \theta_k)$ and approximate the posterior predictive using the samples $\tilde{y}_1,\ldots,\tilde{y}_S$.  

<<echo=F>>=
load("alldata")
w40 <- Y[(Y$YEAR>=1990)&(Y$FEMALE==1)&(Y$AGE==40),#&(Y$DEGREE<3),
         c("YEAR","CHILDS","AGE","DEGREE")]
w40 <- na.exclude(w40)

#empirical distribution
ct.child <- rep(0,11)
ct.child[1:length(unique(w40$CHILDS))] <- as.vector(table(w40$CHILDS))
pr.child <- ct.child/sum(ct.child)
names(pr.child) <- names(ct.child) <- 0:10

#posterior predictive NegBinom(sum(y) + a, (b+n)/(b+n+1))
a <- 2; b <- 1
n <- nrow(w40)
sum.y <- sum(w40$CHILDS)
p <- (b+n)/(b+n+1)

post.pred <- dnbinom(0:10,size=(a+sum.y),prob=p)#mu=((a+sum.y)/(b+n)))#
@


Regardless, this is not the focus of our current discussion.  As we mentioned above, we may use the posterior predictive and Monte Carlo methods to evaluate the quality of the fit of a particular model with the empirical distribution of the data.  Figure \ref{fig:barpw40} shows the empirical and the posterior predictive pmf's. Notice that the odds $\mathcal{O}_{2,1}=\Pr(Y=2)/\Pr(Y=1)$ is $\Sexpr{round(pr.child[3]/pr.child[2],2)}$ and $\Sexpr{round(post.pred[3]/post.pred[2],2)}$ under the empirical and posterior predictive distributions, respectively. This disagreement between the empirical and the posterior predictive distributions seems large for it to be product merely of the sampling variability, and might be an indication that the poisson as a generating model for this data could be flawed.  

The whole point of this exercise is to evaluate this claim numerically using Monte Carlo methods, with three simple steps:

For each $k\in\lrb{1,2,\ldots,S}$,
\benum
\item sample a value $\theta_k$ from $\text{Gamma}(a+\sum y_i,b+n)$\\
\item sample $\tilde{y}_{1:n}^{(k)}=(\tilde{y}_{1}^{(k)},\ldots,\tilde{y}_{n}^{(k)})\sim \text{Poisson}(\theta_k)$
\item with each $\tilde{y}_{1:n}^{(k)}$ drawn, calculate the odds of having 2 vs one child: $$\mathcal{O}_{2,1}=\frac{\sum_{i=1}^n \1_{\lrb{\tilde{y}_{i}^{(k)}=2}}}{\sum_{i=1}^n \1_{\lrb{\tilde{y}_{i}^{(k)}=1}}}.$$
\eenum
This last step provides Monte Carlo samples from the posterior predictive distribution for $\mathcal{O}_{2,1}$, that is, the odds of having 2 vs 1 children.

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=8, fig.height=4, fig.align='center'>>=

par(mfrow=c(1,2))
#plot empty graph device (using the option type="n")
dev <- 0.1
plot( x=c(-dev,10+dev), y=c(0,.4),xlab="number of children",
      ylab=expression(p(Y[i]==y[i])),type="n",main="")

#add barplots for theta=0.05, 0.1 and 0.2
points(0:10-dev,pr.child,type="h",col="black",lwd=4)
points(0:10+dev,post.pred,type="h",col=gray(.75),lwd=4)
legend(4,0.35,legend=c("empirical",
                       "predictive"),
       bty="n", lwd=c(2,2),col=c("black",gray(.75)))

S <- 10000
O21 <- rep(NA,S)
theta.k <- ytilde.ks <- NULL

for(k in 1:S){
  theta.k <- rgamma(1,a+sum.y,b+n)
  ytilde.ks <- rpois(n,theta.k)
  O21[k] <- sum(ytilde.ks==2)/sum(ytilde.ks==1)
}
hist(x=O21,main="",xlab="posterior predictive odds for 2 vs 1 children",freq=F)
abline(v=pr.child[3]/pr.child[2],lwd=3,col="red")
@
\label{fig:barpw40}\caption{Empirical vs posterior predictive distribution (left). Posterior predictive density for $\mathcal{O}_{2,1}$ (right).}
\end{figure}

Looking at the right panel in Figure \ref{fig:barpw40}, which depicts the histogram for the Monte Carlo samples of $\mathcal{O}_{2,1}$, it is clear that the value observed in the actual data for these odds (red vertical line) is extremely unlikely.



\section{Importance Sampling}

As we saw above, the direct Monte Carlo (MC) approach to approximate integrals and sums is really useful. Nevertheless: 
\bit
\item its direct applicability is limited to cases where we can obtain or calculate samples from the distribution of interest
\item when we are trying to compute $E(g(Y))$, but it turns out that most of the action of $g(Y)$ takes place in a region $A$ where $\Pr(Y\in A)$ is small, MC will be slow to converge. The problem here is that too many samples are required to get enough of them falling within $A$ (e.g., when calculating probabilities in the tails of a distribution).
\eit

\subsection{Example: estimating a Cauchy tail probability}

Consider the following example.  Assume that we want to approximate the probability $\psi=\Pr(X> 2)$, where $X\sim\text{Cauchy}(0,1)$, or in other words
$$\psi=\int_2^{\infty} \frac{1}{\pi(1+x^2)} dx.$$

This probability in R can be obtained with the function \textsf{pcauchy}, and is $\psi=\Sexpr{(psi=round(pcauchy(2,lower.tail=F),4))}$.  But for the sake of argument, imagine that we could not calculate it directly. When we approximate $\psi$ directly using MC sampling $X_1,\ldots,X_n\overset{iid}{\sim}\text{Cauchy}(0,1)$, remember (from the Pygmalion example) that the theoretical standard error from the MC estimator $\frac{1}{S}\sum \1_{\lrb{X_k>2}}$  would be $\sqrt{\psi(1-\psi)/S}=\Sexpr{psi=pcauchy(2,lower.tail=F);round(psi*(1-psi),4)}/S$. 

This is actually quite a large variation, this is because when sampling from the Cauchy$(0,1)$ many samples will fall outside of $[2,\infty)$, and so getting the estimator to converge will take a while. 

Can you think of any way to reduce the variance and therefore improve the speed of covergence of the estimator?

Notice that the number of samples needed for the estimates to converge could be reduced by making use of the symmetry of the Cauchy$(0,1)$ distribution about 0. Specifically, we know that $\Pr(X>0)=0.5$, and so 
\bean
\Pr(X>2)&=&\Pr(X>0)-\Pr(0\leq X < 2)\\
&=& \frac{1}{2}-\int_{0}^2 \frac{1}{\pi(1+x^2)} dx\\
&=& \frac{1}{2}-\int_{0}^2 \frac{2}{\pi(1+y^2)} \lrp{\frac{1}{2} \1_{\lrb{y\in(0,2)}}}dy\\
&=& \frac{1}{2}- E(g(Y))\eean
where $g(Y)=\frac{2}{\pi(1+Y^2)}$ and $Y\sim U(0,2)$ (i.e., has pdf $p(y)=\frac{1}{2} \1_{\lrb{y\in(0,2)}}$).  And so, this probability can also be estimated using
$$\Pr(X>2)\approx  \frac{1}{2}- \frac{1}{S}\sum_{k=1}^S \lrp{\frac{2}{\pi(1+Y_k^2)}},$$ with $Y_1,\ldots,Y_S\overset{iid}{\sim} U(0,2)$.  This last estimator has variance $0.0285/S$ (this is obtained with integration by parts), making the variance of the new estimator is about $\Sexpr{round(0.1258/0.0285,0)}$ times smaller than of the direct MC one.  

This is all to say that there are many alternative ways in which integrals or sums can be represented as expectations, it's just a matter of finding good ones among them. 

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=6, fig.height=4, fig.align='center'>>=
S <- 5000
psi <- pcauchy(2,lower.tail = F)
th.sd <- sqrt(psi*(1-psi)/(1:S))

#direct MC
x.k <- rcauchy(S)
ra.x <- cumsum(x.k>2)/(1:S)
ll.x <- ra.x - 2*th.sd
ul.x <- ra.x + 2*th.sd

#using uniforms MC
y.k <- runif(S,0,2)
z.k <- 2/(pi*(1+y.k^2))
sd.z <- sqrt(0.0285/(1:S))
ra.y <- 0.5-cumsum(z.k)/(1:S)
ll.y <- ra.y - 2*sd.z
ul.y <- ra.y + 2*sd.z

plot(x=1:S,y=ra.x,type="l",
     ylim=c(0.1,0.2),
     xlab="number of samples (S)",
     ylab=expression(psi[S]),
     col="blue",lwd=2)
lines(ll.x,col="blue",lty=2)
lines(ul.x,col="blue",lty=2)

lines(ra.y,col="orange",lwd=2)
lines(ll.y,col="orange",lty=2)
lines(ul.y,col="orange",lty=2)
abline(h=psi,col="black",lwd=2)
legend("topright",bty="n",legend=c("true","MC Cauchy","MC Unif"),col=c("black","blue","orange"),lty=1)
@
\label{fig:runaveCauchy}\caption{MC estimate for $\Pr(X>2)$ with $X\sim\text{Cauchy}(0,1)$ vs indirect MC using $Y\sim U(0,2)$ .}
\end{figure}

\subsection{The idea behind Importance Sampling}

In such situations, a powerful alternative is using {\bemph{Importance Sampling}} (IS), which is a versatile twist to how the MC approach can be used. With IS one may target the \emph{important} regions, thus reducing the variance and speeding up convergence (because the variance can be made smaller). However, it's is also crucial to disclose that much care is required to implement this method properly.

Suppose $Y \sim p$ is continuous (it also works in the discrete case), and consider a pdf $q$ from a distribution we can easily sample from, $q$ is referred to as the proposal distribution. Assume that  $q(y) > 0$ whenever $p(y) > 0$. Then, to approximate $E(g(Y))$ using IS, we have that
\bean
E_p(g(Y))&=& \int g(y) p(y) dy\\
&=& \int g(y)\frac{q(y)}{q(y)} p(y) dy\\
&=& \int \lrp{\frac{g(y)  p(y)}{q(y)}}q(y) dy\\
&=& E_q\lrp{g(Z)\frac{p(Z)}{q(Z)}} \\
&\approx&  \frac{1}{S}\sum_{k=1}^S g(Z_k)\frac{p(Z_k)}{q(Z_k)},
\eean
where $Z_1,\ldots, Z_S \overset{iid}{\sim} q$. This is called the {\bemph{importance sampling approximation}}, which is a Monte Carlo approximation, adjusting the importance of different values to account for the fact that they are not being sampled directly from the actual distribution of interest.  In particular, the ratio $p(Z) / q(Z)$ is known as the {\bemph{importance weight}}.

These estimators are:
\bit
\item consistent if $E_q\lrp{\left| g(Z)\frac{p(Z)}{q(Z)}\right|}< \infty$ (where $Z\sim q$),
\item unbiased, since 
\bean
E_q\lrp{\frac{1}{S}\sum_{k=1}^S g(Z_k)\frac{p(Z_k)}{q(Z_k)}}&=& \frac{1}{S}\sum_{k=1}^S E_q\lrp{g(Z_k)\frac{p(Z_k)}{q(Z_k)}}\\
&=&\frac{1}{S}\sum_{k=1}^S E_q\lrp{g(Z_k)\frac{p(Z_k)}{q(Z_k)}}\\
&=&\frac{1}{S}\cdot S \cdot E_q\lrp{g(Z)\frac{p(Z)}{q(Z)}}\\
&=& E_p\lrp{g(Y)}
\eean
\item $\text{var}_q\lrp{\sum_{k=1}^S g(Z_k)\frac{p(Z_k)}{q(Z_k)}}=\frac{1}{S}\text{var}_q\lrp{g(Z)\frac{p(Z)}{q(Z)}}$
\item due to unbiasedness, its RMSE is $$\sqrt{\text{var}_q\lrp{g(Z)\frac{p(Z)}{q(Z)}}\Big/S},$$
\eit
so its convergence rate is again of order $S^{-1/2}$, but $\text{var}_q\lrp{g(Z)\frac{p(Z)}{q(Z)}}\not=\text{var}_p\lrp{g(Y)}$, so the gains that can be obtained with IS depend on finding a density $q$ such that $\text{var}_q\lrp{g(Z)\frac{p(Z)}{q(Z)}}<\text{var}_p\lrp{g(Y)}$.


\subsection{Choosing the proposal $q$}

To minimize the RMSE, we want $q(y)$ to look as much like $g(y)p(y)$ as possible, up to a constant of proportionality. In fact, if we could choose $q(y)$ to be exactly proportional to $g(y)p(y)$, then we would have $g(y)p(y)/q(y) = c$ for some $c$, and this would mean that $\var{g(Y)p(Y)/q(Y)}=\var{c}=0$, in other words, the error would be zero after only one sample! In this situation, however, $E\lrp{g(Y)p(Y)/q(Y)}=c$, and so there would be no need for sampling, since in this case, $E(h(Y))=E\lrp{g(Y)p(Y)/q(Y)}=c$.

In spite of this, this also hints that to minimize the approximation error, we want $q(y)$ to be as close as possible to being proportional to $g(y)p(y)$, and it shows that there can be a substantial reduction in the error if a good choice of $q(y)$ is made. A word of caution when choosing $q$, it is better to be cautious and have $q$ be a little more ``spread out'', so that the area where $g(y)p(y)$ is large is adequately covered.  Not covering some of this area may result in occasionally having very large importance weights that would increase the RMSE of the estimator.

Also, to avoid having to come up with a new proposal $q$ when approximating different functions $g$ with the same $p$, it is common to choose $q(y)$ to be as close as possible to $p(y)$ (rather than to $g(y)p(y)$).  This allows reusing the same samples and the same importance weights, and still obtain reasonably good estimators for all of these $g$'s, rather than specializing for each individual $g$.

\bexa {\bemph{Small tail probabilies from a normal}}

The direct approach to estimate probabilities may break down whenever we go too far out in the tail of a distribution.  Consider for example the random variable $X\sim \N(0,1)$ and suppose we needed a good estimate $\Pr(X>4.5)$. Of course we know this probability is VERY small, so although we could sample from a $\N(0,1)$ and calculate the corresponding MC estimator, we can easily find ourselves in the situation where even for $S=10,000$ we would not sample any value in the region of interest (i.e., $(4.5,\infty)$).

Because this is a probability really far out in the tail of the normal, we need a distribution that places most of its mass in the region $[4.5,\infty)$.  So far we have only seen continuous distributions that are defined on $\mathbb{R}$ or on $\mathbb{R}^+$, so perhaps a density defined only over positive values should work well.  However, thsi will still have some room for improvement since $\1_{\lrb{X>4.5}}=0$ if the value sampled falls in $(0,4.5]$.

This being the case, we can take a distribution defined over $\mathbb{R}^+$ and truncate it so that it is always greater than 4.5 Let's assume that $Z\sim\text{Exp}(1)$ and condition it on the event that $\lrsqb{Z>4.5}$, so that, if $Z^\star = Z \given Z>4.5$, we have that the cdf for $Z^\star$ is 
\bean
\Pr(Z^\star\leq z)&=&\frac{\Pr(\lrb{Z\leq z} \cap \lrb{Z>4.5)}}{\Pr(Z>4.5)}\\
&=& \frac{\Pr(4.5\leq Z\leq z)}{\Pr(Z>4.5)} \1_{\lrb{z > 4.5}}\\
&=& \frac{\Pr(Z\leq z)-\Pr(Z \leq 4.5)}{\Pr(Z>4.5)} \1_{\lrb{z > 4.5}}\\
&=&\frac{(1-e^{-z})-(1-e^{-4.5})}{1-(1-e^{-4.5})} \1_{\lrb{z>4.5}}\\
&=& \frac{e^{-4.5}-e^{-z}}{e^{-4.5}}\1_{\lrb{z>4.5}}\\
&=& 1-e^{-(z-4.5)} \1_{\lrb{z^\star>4.5}},
\eean
as such, $Z^\star\sim q(z)$, where, denoting by $F_{Z^\star}(z)= \Pr(Z^\star\leq z)$, the pdf $q(z)$ is given by
\bean
q(z)&=& \frac{d F_{Z^\star}(z)}{dz} \\
&=&e^{-(z-4.5)}\1_{\lrb{z^\star>4.5}}.
\eean

To sample from this distribution we may simply draw values $z_k$ from an Exp$(1)$ and let $z_k^\star=z_k+4.5$.  Therefore, the IS estimator for this problem using this particular $q$ and $g(X)=\1_{\lrb{X>4.5}}$ is
\bean
\Pr(X>4.5) &\approx& \frac{1}{S}\sum_{k=1}^S \1_{\lrb{Z_k^\star >4.5}} \frac{(2\pi)^{-1/2}\e{-\frac{1}{2}{Z_k^\star}^2} }{\e{-(Z_k^\star-4.5)}}=3.38\times 10^{-6}.
\eean
which is pretty close compared to the true value $\Sexpr{pnorm(4.5,lower.tail=F)}$.
\eexa

\bexa
Consider the random variable $Z=g(Y)=10\, e^{-2|Y-5|}$ where $Y\sim U(0,10)$. Propose a reasonable Importance Sampling algorithm to approximate $E(g(Y))$, and justify your choice.

%\vspace{-2cm}
\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=8, fig.height=4, fig.align='center'>>=
S <- 10000
Y <- runif(S,0,10)
gY <- 10*exp(-2*abs(Y-5))
g <- function(x){10*exp(-2*abs(x-5))}

par(mfrow=c(1,2))
plot(density(gY,from=0, to=10),type="l",
      main="",xlab=expression(z),ylab=expression(p(z)))
curve(expr=g,from=0.001,to=10,xlab="y",ylab="g(y)")
@
\label{fig:gY}\caption{Density function for the random variable $Z=g(Y)$.}
\end{figure}

We know that sampling from the uniform will mot be a great alternative since we will get many samples of $Y$ in regions where $g(y)$ is very close to zero.  Looking at the right panel in the figure above it seems like a good idea to consider a distribution that concentrates it's mass around values close to 5 and dissipates the further the values are from 5.  To this end we have several different alternatives.

The first thing to notice is that the function is symmetric about 5, as such it is advisable to use a symmetric distribution.  A reasonable family of distributions to consider is $$\lrb{\tN(y\given 5, \sigma^2):\text{ with }\sigma^2\in \lrp{0,2}}.$$

The second thing to notice is how spiked the function becomes at 5, so perhaps another family that has heavier tails and is able to capture the spike at 5 somewhat better than the normal is the $$\lrb{\text{Cauchy}(5,\gamma):\text{ with }\gamma\in \lrp{0,1} }.$$

To compare the efficacy of each these distributions, let's calculate the estimators for $E_p(g(Y))$ and their corresponding variance using IS with both the normal and Cauchy families assuming $\sigma, \gamma \in \lrp{0.1,0.5,1,2}$.  Additionally, let's obtain the plain MC estimator and it's variance, obtained with draws from the $U(0,10)$ distribution. Below is the code and the results obtained from this comparison.

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=5, fig.height=4, fig.align='center'>>=
g <- function(x) 10*exp(-2*abs(x-5))

dispvals <- c(0.1,0.5,1,2)
yvals <- seq(0,10,by=0.1)
plot(yvals,dcauchy(yvals,location = 5,scale=0.1)/3,lty=2,col=2,
     type="n",ylab="rescaled g(y) and q(y)",xlab="y")
lines(yvals,g(yvals)/10,type="l",lwd=2)
for(dd in dispvals){
  lines(yvals,dcauchy(yvals,location = 5,scale=dd)/3,lty=1,col="cornflowerblue",lwd=1)
  lines(yvals,dnorm(yvals,mean = 5,sd=dd)/3,lty=1,col="orange",lwd=1)
}
legend("topright",lty=rep(1,2),col=c("cornflowerblue","orange","black"),legend=c(expression(paste("Cauchy(",5,",",gamma,")")),expression(paste("Normal(",5,",",sigma^2,")")),"g(y)"),bty="n")
@
\label{fig:qfam}\caption{Different densities considered as proposals to estimate $E_p(g(Y))$ using Importance Sampling.}
\end{figure}

{\small
<<echo=TRUE,results="asis",warning=FALSE>>=
IS.fn <- function(X,dq){
  g(X)*dunif(X,0,10)/dq(X)
} 

S <- 10000

results.normal <- results.cauchy <-list()
results.unif <- g(runif(S,0,10))
uu.res <- c(estimate=mean(results.unif),var=var(results.unif))

k <- 1
for(dd in dispvals){
  dqn <- function(X){dnorm(X,mean=5,sd=dd)}
  dqc <- function(X){dcauchy(X,location=5,scale=dd)}
  
  results.normal[[k]] <- IS.fn(X=rnorm(S,mean=5,sd=dd),dq=dqn)
  results.cauchy[[k]] <- IS.fn(X=rcauchy(S,location=5,scale=dd),dq=dqc)
  k <- k+1
}

results.all <- data.frame(rbind(uu.res,
                     do.call(rbind,lapply(results.normal,
                                          function(xx)c(estimate=mean(xx),var=var(xx)))),
                     do.call(rbind,lapply(results.cauchy,
                                          function(xx)c(estimate=mean(xx),var=var(xx))))))
results.all <- cbind(family=c("Uniform",rep("Normal",4),rep("Cauchy",4)),
                     disp=c("",rep(dispvals,2)),
                     results.all)

  
library(xtable)
print(xtable(results.all,
             caption="Importance sampling estimate and estimator variance 
             for different proposal distributions.",
             label="tab:ISestvar"),
      include.rownames = FALSE,booktabs = T)
@
}

Obviously the MC estimate based on the Uniform is not a great alternative, and it turns out that IS using as the proposal density either the $\tN(5,1)$ or the Cauchy$(5,0.5)$ work much better in terms of the variance of the estimator.

\eexa



\bexa
Now, let's try our hand at a modified version of the problem above. Consider the same function as before $Z=g(Y)=10\, e^{-2|Y-5|}$ but now $Y\sim U(0,5)$. Propose a good IS algorithm to approximate $E(g(Y))$.


\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=8, fig.height=4, fig.align='center'>>=
S <- 10000
Y <- runif(S,0,5)
hY <- 10*exp(-2*abs(Y-5))
g <- function(x){10*exp(-2*abs(x-5))}

par(mfrow=c(1,2))
plot(density(hY,from=0, to=10),type="l",
      main="",xlab=expression(z),ylab=expression(p(z)))
curve(expr=g,from=0.001,to=5,xlab="y",ylab="h(y)")
@
\label{fig:hY2}\caption{Density function for the random variable $Z=g(Y)$.}
\end{figure}

Note that the function $g(y)$ in this case corresponds to the portion to the left of 5 for the function we had earlier.  We saw above that the proposals $\tN(5,1)$ and the Cauchy$(5,0.5)$ worked well.  This being the case, it stands to reason that we may be able to use these distributions again for importance sampling.

Let's try two approaches.  The first one consists of simply using draws from the $\tN(5,1)$ and the Cauchy$(5,0.5)$ distributions directly. The second approach, results from taking into account the fact that we only need draws of $Y$ that are less than 5.  In extent, because we know that those density functions are symmetric about 5, by considering only values less than 5 we need to rescale the densities to integrate to 1, and this is achieved by multiplying the density values by 2.  Draws from this latter alternative can be obtained by drawing values $y$ from the $\tN(5,1)$ and the Cauchy$(5,0.5)$ distributions, respectively, and then reflecting observations greater than 5  across the vertical line at $y=5$.

{\scriptsize
<<echo=TRUE,results="asis",warning=FALSE>>=
g.U <- function(x){10*exp(-2*abs(x-5))}

S <- 10000

IS.weight <- function(X,q){
  dunif(X,0,5)/q(X)
}


X.unif <- runif(S,0,5)
g.unif <- g.U(X.unif)

mu <- 5; sigma <- 1
qnormal <- function(x){dnorm(x,mean=mu,sd=sigma)}
X.normal <- rnorm(S,mean=mu,sd=sigma)
g.normal <- g.U(X.normal)*IS.weight(X.normal,qnormal)

Y.half <-  X.normal
Y.half[Y.half>5] <- 10-Y.half[Y.half>5]
qhalf <- function(x){2*dnorm(x,mean=mu,sd=sigma)}
g.half <- g.U(Y.half)*IS.weight(Y.half,qhalf)

mu <- 5; sigma <- 1
qcauchy <- function(x){dcauchy(x,location=mu,scale=sigma)}
X.cauchy <- rcauchy(S,location = mu, scale=sigma)
g.cauchy <- g.U(X.cauchy)*IS.weight(X.cauchy,qcauchy)

Y.half2 <-  X.cauchy
Y.half2[Y.half2>5] <- 10-Y.half2[Y.half2>5]
qhalf2 <- function(x){2*dcauchy(x,location=mu)}
g.half2 <- g.U(Y.half2)*IS.weight(Y.half2,qhalf2)


results.all2 <- round(rbind(MC.unif = c(estimate=mean(g.unif),var=var(g.unif)),
      IS.nor = c(estimate=mean(g.normal),var=var(g.normal)),
      IS.half = c(estimate=mean(g.half),var=var(g.half)),
      IS.cauchy = c(estimate=mean(g.cauchy),var=var(g.cauchy)),
      IS.half2 = c(estimate=mean(g.half2),var=var(g.half2))),4)
results.all2 <- cbind(family=c("Uniform","Normal","Half.Normal",
                               "Cauchy","Half.Cauchy"),
                      disp=c(NA,1,1,1,1),location=c(NA,rep(5,4)),
                      results.all2)

library(xtable)
print(xtable(results.all2,
             caption="Importance sampling estimate and estimator variance
             for different proposal distributions.",
             label="tab:ISestvar2"),
      include.rownames = FALSE,booktabs = T)
@
}

\eexa

\subsection{Finite Variance Estimators}

Although $q$ can be almost any density, for the estimator $\sum_{k=1}^S g(Y_k)\frac{p(Y_k)}{q(Y_k)}$ to converge and the speed at which it does will depend on the actual choices made, and so it makes sense to be able to compare between different option of $q$.

Although $$\sum_{k=1}^S g(Z_k)\frac{p(Z_k)}{q(Z_k)}\overset{a.s.}{\longrightarrow} E_p(g(Y))$$
if $Z_1,\ldots,Z_S \overset{iid}{\sim} q$, note that the variance of the estimator is only finite if
\bean
E_q\lrp{\lrp{g(Y)\frac{p(Y)}{q(Y)}}^2}&=& \int \lrp{g(y)\frac{p(y)}{q(y)}}^2 q(y) dy\\
&=& \int g^2(y)\frac{p(y)}{q(y)}p(y) dy< \infty %  \\
%&=&E_p\lrp{g^2(Y)\frac{p(Y)}{q(Y)}}< \infty 
\eean
So, the last line above implies that proposal distributions $q$ with tails lighter than those of $p$ (i.e., those with unbounded ratios $p/q$) are not appropriate for IS, since for $q$'s of this form with many different functions $g$, the estimator have infinite variance. 

{\bemph{All this to say that it is generally desirable to consider densities $q$ that have heavier tails than those of $p$.}} %Geweke (1989) proposes two types of sufficient conditions:
% \benum
% \item[(a)] $p(y)/q(y)<M$ for all $y\in\mathcal{Y}$ and $\text{var}_p(g(Y))<\infty$
% \item[(b)] $\mathcal{Y}$ is compact, $p(y)<F$
% \eenum



\end{document}