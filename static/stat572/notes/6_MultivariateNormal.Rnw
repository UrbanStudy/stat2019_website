\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espaÃ±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{float}
\usepackage{url}

\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbs}[1]{\boldsymbol{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\renewcommand{\d}{\text{d}}
\newcommand{\by}{\mbf{y}}
\newcommand{\mts}{\tilde{Y}}
\newcommand{\mtsv}{\tilde{\bv}}
\newcommand{\btw}{\tilde{\bw}}
\newcommand{\bhw}{\hat{\bw}}
\newcommand{\btx}{\tilde{\bx}}
\newcommand{\pt}{\tilde{p}}
\newcommand{\ba}{\mbf{a}}
\newcommand{\bb}{\mbf{b}}
\newcommand{\bc}{\mbf{c}}
\newcommand{\bd}{\mbf{d}}
\newcommand{\boe}{\mbf{e}}
\newcommand{\bk}{\mbf{k}}
\newcommand{\bq}{\mbf{q}}
\newcommand{\br}{\mbf{r}}
\newcommand{\bs}{\mbf{s}}
\newcommand{\bh}{\mbf{h}}
\newcommand{\bff}{\mbf{f}}
\newcommand{\bt}{\mbf{t}}
\newcommand{\bu}{\mbf{u}}
\newcommand{\bm}{\mbf{m}}
\newcommand{\bv}{\mbf{v}}
\newcommand{\bx}{\mbf{x}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\bz}{\mbf{z}}
\newcommand{\tby}{\tilde{\mbf{y}}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\bp}{\mbs{p}}
\newcommand{\bA}{\mbf{A}}
\newcommand{\tbA}{\tilde{\bA}}
\newcommand{\bB}{\mbf{B}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\tc}{\tilde{c}}
\newcommand{\tC}{\tilde{C}}
\newcommand{\tbC}{\tilde{\bC}}
\newcommand{\bF}{\mbf{F}}
%\newcommand{\bBs}{\mbf{B}^\star}
\newcommand{\bD}{\mbf{D}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bK}{\mbf{K}}
\newcommand{\bQ}{\mbf{Q}}
\newcommand{\bV}{\mbf{V}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\hN}{\hat{N}}
\newcommand{\tbc}{\tilde{\mbf{c}}}
\newcommand{\tba}{\tilde{\mbf{a}}}
\newcommand{\tbX}{\tilde{\mbf{X}}}
\newcommand{\tbW}{\tilde{\mbf{W}}}
\newcommand{\bSigma}{\mbs{\Sigma}}
\newcommand{\bGamma}{\mbs{\Gamma}}
\newcommand{\bUps}{\mbs{\Upsilon}}
\newcommand{\bPsi}{\mbs{\Psi}}
\newcommand{\vs}{v^\star}
\newcommand{\Vs}{V^\star}
\newcommand{\bvs}{\bv_i^\star}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bP}{\mbf{P}}
\newcommand{\bBs}{\bB^\star}
\newcommand{\bXs}{\bX^\star}
\newcommand{\bxs}{\bx^\star}
\newcommand{\bWs}{\bW^\star}
\newcommand{\bws}{\bw_i^\star}
\newcommand{\bwsp}{\left.\bw_i^\star\right.^\prime}
\newcommand{\bBsp}{\left.\bB^\star\right.^\prime}
\newcommand{\bVs}{\bV^\star}
\newcommand{\bpsi}{\mbs{\psi}}
\newcommand{\bphi}{\mbs{\phi}}
\newcommand{\bmu}{\mbs{\mu}}
\newcommand{\bdelta}{\mbs{\delta}}
\newcommand{\bbeta}{\mbs{\beta}}
\newcommand{\bxi}{\mbs{\xi}}
\newcommand{\bchi}{\mbs{\chi}}
\newcommand{\blambda}{\mbs{\lambda}}
\newcommand{\bLambda}{\mbs{\Lambda}}
\newcommand{\LamT}{\tilde{\Lambda}}
\newcommand{\GamT}{\tilde{\Gamma}}
\newcommand{\WT}{\tilde{W}}
\newcommand{\balpha}{{\mbs{\alpha}}}
\newcommand{\bepsilon}{{\mbs{\varepsilon}}}
\newcommand{\bgamma}{{\mbs{\gamma}}}
\newcommand{\btheta}{{\mbs{\theta}}}
\newcommand{\bseta}{{\mbs{\eta}}}
\newcommand{\bpi}{{\mbs{\pi}}}
\newcommand{\bI}{\mbf{I}}
\newcommand{\bH}{\mbf{H}}
\newcommand{\tbH}{\tilde{\mbf{H}}}
\newcommand{\1}{\mbs{1}}
\newcommand{\0}{\mbs{0}}
\newcommand{\detstar}[1]{\text{det}^+\left(#1\right)}
\renewcommand{\det}[1]{\text{det}\left(#1\right)}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\renewcommand{\exp}[1]{\text{exp}\left[#1\right]}
\newcommand{\M}{{M}}
\newcommand{\K}{{K}}
\newcommand{\peq}{{p}}
\newcommand{\MB}{{M_B}}
\newcommand{\MF}{{M_F}}
\newcommand{\MT}{{M_T}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\graph}{\Gamma}
\newcommand{\kset}{\Upsilon}
\newcommand{\order}{order}
\newcommand{\parents}{\mcal{P}}
\newcommand{\children}{\mcal{C}}
\newcommand{\extreme}{\mcal{E}}
\newcommand{\combined}{\mcal{A}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left\{#1\right\}}
\newcommand{\lrno}[1]{\left.#1\right.}
\newcommand{\lrsqb}[1]{\left[#1\right]}
\newcommand{\G}[1]{\Gamma_{#1}}
\newcommand{\N}{\mcal{N}}
\renewcommand{\d}{\text{d}}
\newcommand{\Ps}[1]{\Pr{\left(#1\right)}}
\newcommand{\BF}[2]{{BF}_{#1,#2}(Y)}
\newcommand{\BFd}[3]{{BF}_{#1,#2}(Y,#3)}
\newcommand{\xmark}{\ding{55}}
\newcommand{\ben}{\begin{equation*}}
\newcommand{\een}{\end{equation*}}
\newcommand{\bean}{\begin{eqnarray*}}
\newcommand{\eean}{\end{eqnarray*}}
\newcommand{\bsm}{\begin{smallmatrix}}
\newcommand{\esm}{\end{smallmatrix}}
\newcommand{\bmat}{\begin{matrix}}
\newcommand{\emat}{\end{matrix}}
\newcommand{\tI}{\text{I}}
\newcommand{\tN}{\text{N}}
\newcommand{\trN}{\text{trunc.N}}
\newcommand{\nl}[1]{\text{log}{\lrp{#1}}}
\newcommand{\e}[1]{\text{exp}{\lrb{#1}}}
\newcommand{\indf}[1]{\tI_{\left\{#1\right\}}}
\newcommand{\parent}{\mcal{P}}
\newcommand{\gp}{\text{GP}{\lrp{\0,\mcal{C}(\cdot\given\bphi)}}}
\newcommand{\gpd}[3]{\text{GP}_{#1}{\lrp{\0,\mcal{C}_{#2}(\cdot\given\phi_{#3})}}}
\newcommand{\mnngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mbs{\mcal{C}}}_{#2}(\cdot,\cdot;\bphi_{#3})}}}
\newcommand{\nngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mcal{C}}_{#2}(\cdot,\cdot;\phi_{#3})}}}
\newcommand{\nngpw}[4]{\text{NNGP}_{#1}^{#2}{\lrp{0,\tilde{\mcal{C}}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpw}[4]{\text{GP}_{#1}^{#2}{\lrp{0,\mcal{C}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpq}{\text{GP}_q{\lrp{\0,\mbs{\mcal{C}}(\cdot\given\bphi)}}}
\newcommand{\gpk}{\text{GP}{\lrp{0,\mcal{C}(\cdot\given\tilde{\phi}_k)}}}
\newcommand{\nngp}{\text{NNGP}{\lrp{\0,\tilde{\mcal{C}}(\cdot\given\phi)}}}
\newcommand{\refset}{\mcal{T}}
\newcommand{\uset}{\mcal{U}}
\newcommand{\oset}{\mcal{T}}
\newcommand{\Xall}{\mbb{X}}
\newcommand{\given}{\,|\,}
\newcommand{\dtr}[1]{\textcolor{blue}{(#1)}}
\newcommand{\bemph}[1]{\bf \emph{#1}}
\newcommand{\var}[1]{\text{var}{\left(#1\right)}}

%-------------------------------
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{exa}{Example}
\newtheorem{note}{Note}
\newcommand{\bex}{\begin{exer}}
\newcommand{\eex}{\end{exer}}
\newcommand{\bexa}{\begin{exa}}
\newcommand{\eexa}{\end{exa}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\bdes}{\begin{description}}
\newcommand{\edes}{\end{description}}

\newcommand{\bsh}{\begin{shaded}}
\newcommand{\esh}{\end{shaded}}
%-------------------------------

%-------------------------------
%hiding proof solutions
%-------------------------------


\begin{document}


\setcounter{section}{0}
\title{Gibbs Sampling}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf Bayesian Inference for the Multivariate Normal}\\
{\large STAT 572: Bayesian Statistics}\\
Fall 2019
\end{center}
\section{Motivation}

Many real problems involve multiple measurements from the same experimental unit or individual.  The multivariate normal distribution is by far the most imprortant model for multiuvariate data, and serves as a foundational building block for many non-normal response problems.

Let's consider an example to bring the idea home.

\bexa \textbf{Reading Comprehension}
A reading comprehension test is given to $n=22$ children before and after implementing a particular instruction method in order to compare its efficacy. 

As such, each student's response consists of a two-dimensional vector, $\bY_i=\lrp{\bsm Y_{i1}// Y_{i2} \esm}$ for $i=1,2,\ldots,22$, where $Y_{i1}$ and $Y_{i2}$ denote the tests scores before and after, respectively.

Regardless of the distribution, assuming that $E\lrsqb{Y_{ij}}=\theta_j$, var$(Y_{ij})=\sigma_j^2$, $E\lrsqb{Y_{i1} Y_{i2}}=\sigma_{12}$, we have that
$$E\lrsqb{\bY_i}= \lrp{\bmat E\lrsqb{Y_{i1}} \\ E\lrsqb{Y_{i2}} \emat} = \lrp{\bmat \theta_1 \\ \theta_2 \emat},\quad\text{and}$$

\bean
\text{cov}(\bY_i)&=& \lrp{\bmat \text{var}(Y_{i1}) & \text{cov}(Y_{i1}, Y_{i2}) \\ \text{cov}(Y_{i1}, Y_{i2}) & \text{var}(Y_{i2}) \emat} \\
&=&  \lrp{\bmat \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \emat}
\eean
So the mean and variance-covariance of $\bY_i$ are functions of the first and second order moments of $Y_{i1}$ and $Y_{i2}$. Inference for the multivariate normal problem focuses on the mean vector and the covariance matrix.
\eexa


\section{A very short vector and matrix algebra review}

Before moving forward, a few things to remember:

Let $$\bx=\lrp{\bmat x_1\\ x_2\\ \vdots \\ x_p\emat},\quad \by=\lrp{\bmat y_1\\ y_2\\ \vdots \\ y_p\emat}\quad \text{and}\quad A=\lrp{\bmat a_{11}&a_{12}&\cdots& a_{1p}\\ a_{21}&a_{22}&\cdots& a_{2p} \\ \vdots & \vdots &\ddots & \vdots \\ a_{p1}&a_{p2}&\cdots& a_{pp} \emat},$$
where $x_i, y_j \in\mathbb{R}$.  Then we have that
\benum
\item $\bx' \by = \sum_{i=1}^p x_i y_i $\\

\item $\bx \by'= \lrp{\bmat x_1 y_1 & x_1 y_2 &\cdots& x_1 y_p \\ x_2 y_1 & x_2 y_2 &\cdots& x_2 y_p \\ \vdots & \vdots &\ddots & \vdots \\ x_p y_1 & x_p y_2 &\cdots& x_p y_p \emat}$

\item $\bx' A =\lrp{\bmat \sum_{i=1}^p x_i a_{i1} & \sum_{i=1}^p x_i a_{i2} & \vdots & \sum_{i=1}^p x_i a_{ip}\emat}$ and $A \bx =\lrp{\bmat \sum_{i=1}^p a_{1i} x_i  \\ \sum_{i=1}^p a_{i2} x_i  \\ \vdots \\ \sum_{i=1}^p a_{ip} x_i \emat}$

\bigskip
\item $A$ is said to be positive definite (pd) iff $A$ is symmetric and $\bx' A \bx >0$ for all $\bx \not= \mbs{0}$.

\bigskip
\item If $A$ is pd, then $$A^{-1}=\lrb{B\in \mathbb{R}^{p\times p}: B A = I_p,\; AB' = I_p\;\text{and}\;B=B'}.$$

\bigskip
\item $A^{-1}$ is called the inverse of $A$

\bigskip
\item $|A|$ is called the determinant of $A$

\item
\bean
\bx' A \by \;=\; (\bx' A ) \by &=& \lrp{\bmat \sum_{i=1}^p x_i a_{i1} & \sum_{i=1}^p x_i a_{i2} & \vdots & \sum_{i=1}^p x_i a_{ip}\emat} \lrp{\bmat y_1\\ y_2\\ \vdots \\ y_p\emat}\\
&=& \sum_{i=1}^p x_i a_{i1} y_{i1} + \sum_{i=1}^p x_i a_{i2} y_{i2} + \vdots + \sum_{i=1}^p x_i a_{ip} y_{ip}\\
&=& \sum_{j=1}^p\sum_{i=1}^p x_i a_{i1} y_{ij}.
\eean
\eenum

\section{The Multivariate Normal distribution}

Assume $$\bY=(Y_1, Y_2, \cdots, Y_p)' \sim N_p(\btheta, \Sigma),$$ or in words, we say that $\bY$ is a $p$-variate multivariate normal random vector, with $p$-variate mean vector  and $p\times p$ covariance matrix $$\btheta = \lrp{\theta_1 \\ \theta_2 \\ \cdots \\ \theta_p}\quad\text{and}\quad\Sigma=\lrp{\bmat \sigma_{1}^2&\sigma_{12}&\cdots& \sigma_{1p}\\ \sigma_{21}&\sigma_{2}^2&\cdots& \sigma_{2p} \\ \vdots & \vdots &\ddots & \vdots \\ \sigma_{p1}&\sigma_{p2}&\cdots& \sigma_{p}^2 \emat},$$ respectively, if and only if
$$p(\by given \btheta,\Sigma) = (2\pi)^{-p/2} |\Sigma|^{-1/2} \e{-\frac{1}{2} (\by -\btheta)'\Sigma^{-1} (\by -\btheta)} \1_{\lrb{\by\in \mathbb{R}^p}},$$ where $\Sigma^{-1}$ is often referred to as the \textsf{precision matrix}.

Again, here we are interested in conducting inference on $\btheta$ and $\Sigma$, or functions of them of the form $g_1(\btheta)$, $g_2(\Sigma)$ or $g_3(\btheta,\Sigma)$.  In the following link, play with the correlation and number of observations to see how the distribution of $\bY$ changes as a function of the correlation \url{https://shiny.rit.albany.edu/stat/rectangles/}.  Recall that $$\text{corr}(Y_{1},Y_2)= \frac{\text{cov}(Y_{1},Y_2)}{\sqrt{\text{var}(Y_{1})\text{var}(Y_2)}} = \frac{\sigma_{12}}{\sigma_1 \sigma_2}.$$

\section{Properties of the Multivariate Normal model}

Assume that $\bY=(Y_1, Y_2, \cdots, Y_p)' \sim N_p(\btheta, \Sigma),$ with $\btheta$ and $\Sigma$ as defined before, then we have that
\benum
\item Marginally $Y_j\sim N(\theta_j, \sigma_j^2)$ for all $j=1,2,\ldots,p$.

\item If $\bY^{(1)}=\lrp{Y_1, Y_2, \ldots, Y_k}'$ and $\bY^{(1)}=\lrp{Y_{k+1}, Y_{k+2}, \ldots, Y_p}'$, such that $\bY=\lrp{\bmat \bY^{(1)} \\ \bY^{(2)} \emat}$, we then have that marginally
$\bY^{(1)} \sim N_k(\btheta^{(1)}, \Sigma^{(1)}),$ where 
$$\btheta^{(1)} = \lrp{\theta_1 \\ \theta_2 \\ \cdots \\ \theta_k}'\quad\text{and}\quad\Sigma^{(1)}=\lrp{\bmat \sigma_{1}^2&\sigma_{12}&\cdots& \sigma_{1k}\\ \sigma_{21}&\sigma_{2}^2&\cdots& \sigma_{2k} \\ \vdots & \vdots &\ddots & \vdots \\ \sigma_{k1}&\sigma_{k2}&\cdots& \sigma_{k}^2 \emat},$$
and similarly $\bY^{(2)} \sim N_{p-k}(\btheta^{(2)}, \Sigma^{(2)}),$ with $\btheta^{(2)}$ and $\Sigma^{(2)}$ defined analogously.

Note that since $\bY=\lrp{\bmat \bY^{(1)} \\ \bY^{(2)} \emat}$ we can similarly partition  $\btheta$ and $\Sigma$ as 
$$\btheta = \lrp{\bmat \btheta^{(1)} \\ \btheta^{(2)} \emat}\quad\text{and} \Sigma^{(1)}=\lrp{\bmat \Sigma^{(1)}&\Sigma^{(12)}\\ \Sigma^{(21)}&\Sigma^{(2)} \emat}.$$

\item Conditionally $ \bY^{(1)} \given \bY^{(2)}=\by_2 \sim N_k(\btheta_\star^{(1)}, \Sigma_\star^{(1)}), $ where $$\btheta_\star^{(1)} =  \btheta^{(1)} + \Sigma^{(12)}\lrp{\Sigma^{(2)}}^{-1}(\by_2-\btheta^{(2)})\quad\text{and}\quad \Sigma_\star^{(1)} =  \Sigma^{(1)} + \Sigma^{(12)}\lrp{\Sigma^{(2)}}^{-1}\Sigma^{(21)}.$$
\eenum

\bsh \textbf{Importantly, note that}
$$\left\{\bmat X_1 \sim N(\theta_1, \sigma_1^2) \\ X_2 \sim N(\theta_2, \sigma_2^2) \emat \right. \not \Rightarrow \bX = \lrp{\bmat X_1 \\ X_2 \emat} \sim N_2\lrp{ \lrp{\bmat \theta_1 \\ \theta_2 \emat}, \lrp{\bmat \sigma_1^2  & \sigma_{12}\\ \sigma_{12} & \sigma_2^2 \emat}}.$$
\esh

\section{Inference for the Mutivariate Normal parameters}

So, how do we go about doing inference for $\btheta$ and $\Sigma$ for $\bY\sim N_p(\btheta, Sigma)$? As before, let's start simple and assume that $\Sigma$ is fixed and know and first focus on the mean parameter $\btheta$.

\subsection{Semiconjugate prior for the mean $\btheta$}

As in the univariate case, a convenient prior for the mean $\btheta=(\theta_1, \theta_2, \ldots, \theta_p)'$ is the $N_p(\bmu_0, \Omega_0),$ which implies that
\bean
p(\btheta) &=& (2\pi)^{-p/2} |\Omega|^{-1/2} \e{-\frac{1}{2} (\btheta -\bmu_0)'\Omega_0^{-1} (\btheta -\bmu_0)} \1_{\lrb{\btheta\in \mathbb{R}^p}}\\
&=& (2\pi)^{-p/2} |\Omega|^{-1/2} \e{-\frac{1}{2} (\btheta'\Omega_0^{-1} \btheta  -2 \btheta' \Omega_0^{-1}\bmu_0 + \bmu_0' \Omega_0^{-1}\bmu_0 )}\\
&\propto_{\btheta}&  \e{-\frac{1}{2} (\btheta'\Omega_0^{-1} \btheta  -2 \btheta' \Omega_0^{-1}\bmu_0)}\\
&\propto_{\btheta}&  \e{-\frac{1}{2} (\btheta'A_0 \btheta  -2 \btheta' \bb_0)},\qquad (\star)
\eean
where $A_0= \Omega_0^{-1}$ and $\bb_0 = \Omega_0^{-1}\bmu_0$.

In other words, this implies that if a random vector $\btheta\in \mathbb{R}^p$ has pdf proportional to $$\e{-\frac{1}{2} (\btheta'A_0 \btheta  -2 \btheta' \bb_0)},$$ then $\btheta \given A, \bb \sim N_p(A_0^{-1} \bb_0, A_0^{-1})$.  This fact will come in handy when obtaining the posterior density for $\btheta$.

\subsection{Likelihood for the Multivariate Normal problem}

Assume that we have a sample of random vectors $\bY_1, \bY_2, \ldots, \bY_n \stackrel{iid}{\sim} N_p(\btheta, \Sigma),$ then the joint generating density (or the likelihood, if seen as a function of the parameters) evaluated at $\by_1, \by_2, \ldots, \by_n $ is given by
\bean
p(\by_1, \ldots, \by_n\given \btheta, \Sigma) &=& \prod_{i=1}^n (2\pi)^{-p/2} |\Sigma|^{-1/2} \e{-\frac{1}{2} (\by_i -\btheta)'\Sigma^{-1} (\by_i -\btheta)}\\
&=&  (2\pi)^{-n p/2} |\Sigma|^{-n/2} \e{-\frac{1}{2} \sum_{i=1}^n (\by_i -\btheta)'\Sigma^{-1} (\by_i -\btheta)}.
\eean
Notice that
\bean
\sum_{i=1}^n (\by_i -\btheta)'\Sigma^{-1} (\by_i -\btheta)&=& \sum_{i=1}^n\lrp{\by_i'\Sigma^{-1} \by_i - 2 \by_i'\Sigma^{-1} \btheta + \btheta' \Sigma^{-1} \btheta}\\
&=& \sum_{i=1}^n\by_i'\Sigma^{-1} \by_i - 2 \btheta'\Sigma^{-1} \sum_{i=1}^n \by_i + n \btheta' \Sigma^{-1} \btheta\\
&=& n \btheta' \Sigma^{-1} \btheta - 2 \btheta'\Sigma^{-1} n \bar{\by} + \sum_{i=1}^n\by_i'\Sigma^{-1} \by_i.
\eean
Replacing this last result in the likelihood, we have that
\bean
p(\by_1, \ldots, \by_n\given \btheta, \Sigma) &=&(2\pi)^{-n p/2} |\Sigma|^{-n/2} \e{-\frac{1}{2} \lrp{n \btheta' \Sigma^{-1} \btheta - 2 \btheta'\Sigma^{-1} n \bar{\by} + \sum_{i=1}^n\by_i'\Sigma^{-1} \by_i}}.\qquad (\star\star)
\eean

\subsection{Deriving the full conditional density for $\btheta$}

Using the expressions in $(\star)$ and $(\star\star)$, we have that the posterior density (in this case the full conditional posterior) for $\btheta$ is given by
\bean
p( \btheta \given \by_1, \ldots, \by_n, \Sigma) &\propto& p(\btheta \given \Sigma) p(\by_1, \ldots, \by_n\given \btheta, \Sigma)\\
&\propto_{\btheta}& \e{-\frac{1}{2} (\btheta'A_0 \btheta  -2 \btheta' \bb_0)} \times \\
&&\quad\quad \e{-\frac{1}{2} \lrp{n \btheta' \Sigma^{-1} \btheta - 2 \btheta'\Sigma^{-1} n \bar{\by} + \sum_{i=1}^n\by_i'\Sigma^{-1} \by_i}}
\eean
Letting $A_1= n \Sigma^{-1}$, and $\bb_1= A_1 \bar{\by}$, we have that
\bean
p( \btheta \given \by_1, \ldots, \by_n, \Sigma) &\propto_{\btheta}& \e{-\frac{1}{2} (\btheta'A_0 \btheta  -2 \btheta' \bb_0)} \times \\
&&\quad\quad \e{-\frac{1}{2} \lrp{ \btheta' A_n \btheta - 2 \btheta'A_n \bar{\by} + \sum_{i=1}^n\by_i'\Sigma^{-1} \by_i}}\\
&\propto_{\btheta}& \e{-\frac{1}{2} (\btheta'(A_0 + A_n) \btheta  -2 \btheta' (\bb_0+\bb_n)}\\
&=& \e{-\frac{1}{2} (\btheta'A_\star \btheta  -2 \btheta' \bb_\star},
\eean
where $A_\star=A_0+A_n$ and $\bb_\star=\bb_0+\bb_n$, implying from a previous result implies that
$$\btheta \given \Sigma, \by_{1:n} \sim N_p(A_\star^{-1} \bb_\star, A_\star^{-1}),$$ or in words, $\btheta\given \Sigma, \by_{1:n}$ is $p$-variate normal with mean and covariance
\bean
  \bmu_n\;=\;E[\btheta\given \Sigma, \by_{1:n}]&=&A_\star^{-1} \bb_\star\\
&=&(A_0+A_1)^{-1} (\bb_0+\bb_n)   \\
&=&(\Omega_0^{-1}+n \Sigma^{-1})^{-1} (\Omega_0^{-1}\bmu_0 + n \Sigma^{-1} \bar{\by}).\\
&=&\\
\Omega_n\;=\;\text{var}(\btheta\given \Sigma, \by_{1:n}) &=& (A_0+A_1)^{-1}\\
&=& (\Omega_0^{-1}+n \Sigma^{-1})^{-1}.
\eean



\end{document}