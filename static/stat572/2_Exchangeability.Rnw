\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espaÃ±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbs}[1]{\boldsymbol{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\N}{\mbb{N}_0}
\renewcommand{\d}{\text{d}}
\newcommand{\by}{\mbf{y}}
\newcommand{\mts}{\tilde{Y}}
\newcommand{\mtsv}{\tilde{\bv}}
\newcommand{\btw}{\tilde{\bw}}
\newcommand{\bhw}{\hat{\bw}}
\newcommand{\btx}{\tilde{\bx}}
\newcommand{\pt}{\tilde{p}}
\newcommand{\ba}{\mbf{a}}
\newcommand{\bb}{\mbf{b}}
\newcommand{\bc}{\mbf{c}}
\newcommand{\bd}{\mbf{d}}
\newcommand{\boe}{\mbf{e}}
\newcommand{\bk}{\mbf{k}}
\newcommand{\bq}{\mbf{q}}
\newcommand{\br}{\mbf{r}}
\newcommand{\bs}{\mbf{s}}
\newcommand{\bh}{\mbf{h}}
\newcommand{\bff}{\mbf{f}}
\newcommand{\bt}{\mbf{t}}
\newcommand{\bu}{\mbf{u}}
\newcommand{\bm}{\mbf{m}}
\newcommand{\bv}{\mbf{v}}
\newcommand{\bx}{\mbf{x}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\bz}{\mbf{z}}
\newcommand{\tby}{\tilde{\mbf{y}}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\bp}{\mbs{p}}
\newcommand{\bA}{\mbf{A}}
\newcommand{\tbA}{\tilde{\bA}}
\newcommand{\bB}{\mbf{B}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\tc}{\tilde{c}}
\newcommand{\tC}{\tilde{C}}
\newcommand{\tbC}{\tilde{\bC}}
\newcommand{\bF}{\mbf{F}}
%\newcommand{\bBs}{\mbf{B}^\star}
\newcommand{\bD}{\mbf{D}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bK}{\mbf{K}}
\newcommand{\bQ}{\mbf{Q}}
\newcommand{\bV}{\mbf{V}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\hN}{\hat{N}}
\newcommand{\tbc}{\tilde{\mbf{c}}}
\newcommand{\tba}{\tilde{\mbf{a}}}
\newcommand{\tbX}{\tilde{\mbf{X}}}
\newcommand{\tbW}{\tilde{\mbf{W}}}
\newcommand{\bSigma}{\mbs{\Sigma}}
\newcommand{\bGamma}{\mbs{\Gamma}}
\newcommand{\bUps}{\mbs{\Upsilon}}
\newcommand{\bPsi}{\mbs{\Psi}}
\newcommand{\vs}{v^\star}
\newcommand{\Vs}{V^\star}
\newcommand{\bvs}{\bv_i^\star}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bP}{\mbf{P}}
\newcommand{\bBs}{\bB^\star}
\newcommand{\bXs}{\bX^\star}
\newcommand{\bxs}{\bx^\star}
\newcommand{\bWs}{\bW^\star}
\newcommand{\bws}{\bw_i^\star}
\newcommand{\bwsp}{\left.\bw_i^\star\right.^\prime}
\newcommand{\bBsp}{\left.\bB^\star\right.^\prime}
\newcommand{\bVs}{\bV^\star}
\newcommand{\bpsi}{\mbs{\psi}}
\newcommand{\bphi}{\mbs{\phi}}
\newcommand{\bmu}{\mbs{\mu}}
\newcommand{\bbeta}{\mbs{\beta}}
\newcommand{\bxi}{\mbs{\xi}}
\newcommand{\bchi}{\mbs{\chi}}
\newcommand{\blambda}{\mbs{\lambda}}
\newcommand{\bLambda}{\mbs{\Lambda}}
\newcommand{\LamT}{\tilde{\Lambda}}
\newcommand{\GamT}{\tilde{\Gamma}}
\newcommand{\WT}{\tilde{W}}
\newcommand{\balpha}{{\mbs{\alpha}}}
\newcommand{\bepsilon}{{\mbs{\varepsilon}}}
\newcommand{\bgamma}{{\mbs{\gamma}}}
\newcommand{\btheta}{{\mbs{\theta}}}
\newcommand{\bseta}{{\mbs{\eta}}}
\newcommand{\bpi}{{\mbs{\pi}}}
\newcommand{\bI}{\mbf{I}}
\newcommand{\bH}{\mbf{H}}
\newcommand{\tbH}{\tilde{\mbf{H}}}
\newcommand{\1}{\mbs{1}}
\newcommand{\0}{\mbs{0}}
\newcommand{\detstar}[1]{\text{det}^+\left(#1\right)}
\renewcommand{\det}[1]{\text{det}\left(#1\right)}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\renewcommand{\exp}[1]{\text{exp}\left[#1\right]}
\newcommand{\M}{{M}}
\newcommand{\K}{{K}}
\newcommand{\peq}{{p}}
\newcommand{\MB}{{M_B}}
\newcommand{\MF}{{M_F}}
\newcommand{\MT}{{M_T}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\graph}{\Gamma}
\newcommand{\kset}{\Upsilon}
\newcommand{\order}{order}
\newcommand{\parents}{\mcal{P}}
\newcommand{\children}{\mcal{C}}
\newcommand{\extreme}{\mcal{E}}
\newcommand{\combined}{\mcal{A}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left\{#1\right\}}
\newcommand{\lrno}[1]{\left.#1\right.}
\newcommand{\lrsqb}[1]{\left[#1\right]}
\newcommand{\G}[1]{\Gamma_{#1}}
\renewcommand{\d}{\text{d}}
\newcommand{\Ps}[1]{\Pr{\left(#1\right)}}
\newcommand{\BF}[2]{{BF}_{#1,#2}(Y)}
\newcommand{\BFd}[3]{{BF}_{#1,#2}(Y,#3)}
\newcommand{\xmark}{\ding{55}}
\newcommand{\ben}{\begin{equation*}}
\newcommand{\een}{\end{equation*}}
\newcommand{\bean}{\begin{eqnarray*}}
\newcommand{\eean}{\end{eqnarray*}}
\newcommand{\bsm}{\begin{smallmatrix}}
\newcommand{\esm}{\end{smallmatrix}}
\newcommand{\bmat}{\begin{matrix}}
\newcommand{\emat}{\end{matrix}}
\newcommand{\tI}{\text{I}}
\newcommand{\tN}{\text{N}}
\newcommand{\trN}{\text{trunc.N}}
\newcommand{\nl}[1]{\text{log}{\lrp{#1}}}
\newcommand{\e}[1]{\text{exp}{\lrb{#1}}}
\newcommand{\indf}[1]{\tI_{\left\{#1\right\}}}
\newcommand{\parent}{\mcal{P}}
\newcommand{\gp}{\text{GP}{\lrp{\0,\mcal{C}(\cdot\given\bphi)}}}
\newcommand{\gpd}[3]{\text{GP}_{#1}{\lrp{\0,\mcal{C}_{#2}(\cdot\given\phi_{#3})}}}
\newcommand{\mnngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mbs{\mcal{C}}}_{#2}(\cdot,\cdot;\bphi_{#3})}}}
\newcommand{\nngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mcal{C}}_{#2}(\cdot,\cdot;\phi_{#3})}}}
\newcommand{\nngpw}[4]{\text{NNGP}_{#1}^{#2}{\lrp{0,\tilde{\mcal{C}}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpw}[4]{\text{GP}_{#1}^{#2}{\lrp{0,\mcal{C}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpq}{\text{GP}_q{\lrp{\0,\mbs{\mcal{C}}(\cdot\given\bphi)}}}
\newcommand{\gpk}{\text{GP}{\lrp{0,\mcal{C}(\cdot\given\tilde{\phi}_k)}}}
\newcommand{\nngp}{\text{NNGP}{\lrp{\0,\tilde{\mcal{C}}(\cdot\given\phi)}}}
\newcommand{\refset}{\mcal{T}}
\newcommand{\uset}{\mcal{U}}
\newcommand{\oset}{\mcal{T}}
\newcommand{\Xall}{\mbb{X}}
\newcommand{\given}{\,|\,}
\newcommand{\dtr}[1]{\textcolor{blue}{(#1)}}
\newcommand{\bemph}[1]{\bf \emph{#1}}
\newcommand{\var}[1]{\text{var}{(#1)}}

%-------------------------------
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{exa}{Example}
\newtheorem{note}{Note}
\newcommand{\bex}{\begin{exer}}
\newcommand{\eex}{\end{exer}}
\newcommand{\bexa}{\begin{exa}}
\newcommand{\eexa}{\end{exa}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\bdes}{\begin{description}}
\newcommand{\edes}{\end{description}}

\newcommand{\bsh}{\begin{shaded}}
\newcommand{\esh}{\end{shaded}}
%-------------------------------

%-------------------------------
%hiding proof solutions
%-------------------------------
\newif\ifhideproofs
\hideproofstrue %uncomment to hide proofs

\ifhideproofs
\usepackage{environ}
\NewEnviron{hide}{}
\let\proof\hide
\let\endproof\endhide
\fi
%-------------------------------


\begin{document}


\setcounter{section}{0}
\title{Independence, Exchangeability and de Finetti's}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf Independence, Exchangeability and de Finetti's}\\
{\large STAT 572: Bayesian Statistics}\\
Fall 2019
\end{center}
\section{Independence}

\bsh
Indepenence, and specially \emph{conditional independence} plays a huge role in statistics.  If we have a set of random variables $Y_1, Y_2, \ldots, Y_n$, and the parameter $\theta$ describes the mechanism through which values from these random variables are generated, then the random variables are conditionally independent given $\theta$, if for every collection of sets $\lrb{A_1,\ldots,A_n}$, we have
\bean
\Pr(Y_1\in A_1, Y_2\in A_2, \ldots, Y_n\in A_n \given \theta)&=& \Pr(Y_1\in A_1 \given \theta)\times \Pr(Y_2\in A_2 \given \theta)\times \cdots \times \Pr(Y_n\in A_n \given \theta).
\eean
\esh

The expression above implies that \emph{once we know and condition on the parameter $\theta$}, there is no additional information about $Y_i$ contained in any of the other random variables $Y_j$ ($j\not= i$).  
\bsh
This fact provides the form of the joint generating density, which under independence (i.e., conditional on $\theta$) is 
$$p(y_1,y_2,\ldots,y_n \given \theta) = \prod_{i=1}^n p_{Y_i}(y_i \given \theta),$$ or in other words, the joint density is the product of the marginal densities.
\esh

If in addition, the random variables are thought to come from the same population or the same data generating mechanism, then we say that they are both indepepndent and identically distributed (iid), meaning that $p_{Y_i}(y_i \given \theta) = p(y_i \given \theta)$ for all $i=1,2,\ldots, n$, such that $$p(y_1,y_2,\ldots,y_n \given \theta) = \prod_{i=1}^n p(y_i \given \theta).$$ We denote this by writting $Y_1, Y_2, \ldots, Y_n\given \theta \stackrel{iid}{\sim} p(y\given \theta)$.

\section{Exchangeability and de Finetti's Theorem}

The concept of \emph{exchangeability} helps motivate the use of prior distributions on the parameters $\btheta$. To illustrate the idea behind identifiability consider the following example.

\bexa (Happiness)
In the 1998 the General Social Survey, $n=1272$ women were asked the question: \emph{Are you generally happy?} Denote by $Y_i\sim\text{Bernoulli}(\theta)$ the response of each respondant, with $Y_i=1$ if the $i$th woman reports being \emph{generally happy} and $Y_i=0$ otherwise. Let's consider the joint structure of the beliefs for the first 10 randomly chosen participants $(Y_1, Y_2, \ldots,Y_{10})$.  Let $$p(y_1,y_2,\ldots,y_{10})=\Pr(Y_1=y_1,Y_2=y_2,\ldots,Y_{10}=y_{10})$$ denote the joint density for these 10 random variables, where each $y_i\in\lrb{0,1}$.

If asked to assign probabilities to the following three events (note that there is no conditioning on a parameter!!!)
\bean
p(1,1,1,1,1,1,0,0,0,0)&=&?\\
p(1,0,1,0,1,0,1,0,1,1)&=&?\\
p(0,0,0,0,1,1,1,1,1,1)&=&?,
\eean
all of which contains 6 ones and 4 zeros. Is there an argument to assign them the same probability?
\eexa

\bsh \textbf{Def: Exchangeability}

Let $p(y_1,y_2,\ldots,y_{n})$ denote the joint density for $Y_1, Y_2, \ldots, Y_n$.  Then if $$p(y_1,y_2,\ldots,y_{n})=p(y_{\pi_1},y_{\pi_2},\ldots,y_{\pi_n}),$$
for ALL permutations $\bpi=\lrb{\pi_1,\pi_2,\ldots,\pi_n}$ of $\lrb{1,2,\ldots,n}$, we say that $$Y_1, Y_2, \ldots, Y_n\quad\text{\bf are exchangeable}.$$
\esh

\emph{Exchangeability simply conveys the notion that the subscript does not contain information about the outcomes!!!}

\bexa (Happiness continued)
Let's go back to the Happiness problem, assume that we are additionally told that the value $\theta$ provides the rate of happiness of all respondants to the survey.  
% If this is the case, would you consider the statements
% \bean
% \Pr(Y_{10}=1\given \theta)&\stackrel{?}{=}&\theta\\
% \Pr(Y_{10}=1\given Y_{1}=y_{1},\ldots,Y_{9}=y_{9}, \theta)&\stackrel{?}{=}&\theta\\
% \Pr(Y_{9}=1\given Y_{1}=y_{1},\ldots,Y_{8}=y_{8},Y_{10}=y_{10}, \theta)&\stackrel{?}{=}&\theta
% \eean
% valid? If so, we can think of the $Y_i$'s iid \emph{conditionally on $\theta$}! 

If we assume iid random variables conditional on $\theta$, then the Bernoulli model is a suitable data generating distribution, such that for $y_i\in\lrb{0,1}$
\bean
\Pr(Y_{i}=y_i\given \theta)&=&\theta^{y_i}(1-\theta)^{y_i}\\
\Pr(Y_{1}=y_1, Y_{2}=y_{2},\ldots,Y_{10}=y_{10} \given \theta)&=& \prod_{i=1}^{10}\theta^{y_i}(1-\theta)^{y_i}\\
&=& \theta^{\sum_{i} y_i}(1-\theta)^{10-\sum_i y_i}.
\eean

Now, if the rate of happiness $\theta$ is uncertain, we may describe our prior beliefs about $\theta$ using the prior $p(\theta)$, allowing us to obtain the marginal disribution for $Y_1, Y_2, \ldots, Y_{10}$, given by
\bean
p(y_1,y_2,\ldots,y_n) &=& \int_{\theta\in (0,1)} \lrp{\theta^{\sum_{i} y_i}(1-\theta)^{10-\sum_i y_i}} p(\theta) d\theta.
\eean
For the three events we considered before, this implies that
\bean
p(1,1,1,1,1,1,0,0,0,0)&=& \int_{\theta\in (0,1)} \lrp{\theta^{6}(1-\theta)^{10-6}} p(\theta) d\theta\\
p(1,0,1,0,1,0,1,0,1,1)&=&\int_{\theta\in (0,1)} \lrp{\theta^{6}(1-\theta)^{10-6}} p(\theta) d\theta\\
p(0,0,0,0,1,1,1,1,1,1)&=&\int_{\theta\in (0,1)} \lrp{\theta^{6}(1-\theta)^{10-6}} p(\theta) d\theta.
\eean
So, the random variables for this problem are exchangeable.  Can we generalize this result?
\eexa

\bsh
A fundamental assumption in many statistical analyses is that the random variables are iid (conditionally on $\theta$), such that
\bean
p(y_1,y_2,\ldots,y_n \given \theta) &=& \prod_{i=1}^n p(y_i \given \theta)\\
&=&\prod_{i=1}^n p(y_{\pi_i} \given \theta)\\
&=&p(y_{\pi_1},y_{\pi_2},\ldots,y_{\pi_n} \given \theta),
\eean
for any permutation $\bpi$.  Now, putting the result above together with the prior $p(\theta)$ and integrating out $\theta$, implies that
\bean
p(y_1,y_2,\ldots,y_n) &=& \int_{\theta\in\Theta} p(y_1,y_2 \ldots,y_n \given \theta) p(\theta) d\theta\\
&\stackrel{iid}{=}&\int_{\theta\in\Theta} \lrp{\prod_{i=1}^n p(y_i \given \theta)} p(\theta) d\theta\\
&=&\int_{\theta\in\Theta} \lrp{\prod_{i=1}^n p(y_{\pi_i} \given \theta)}p(\theta) d\theta\quad\text{order does not affect product}\\
&=&\int_{\theta\in\Theta} p(y_{\pi_1},y_{\pi_2},\ldots,y_{\pi_n} \given \theta)p(\theta) d\theta\\
&=&p(y_{\pi_1},y_{\pi_2},\ldots,y_{\pi_n} ).
\eean
And so, in general $$\left. \bmat Y_1, Y_2,\ldots, Y_n\given \theta \text{ iid}\\ \btheta\sim p(\theta) \emat\right\} \Longrightarrow Y_1, Y_2,\ldots, Y_n\text{ are marginally exchangeable}.$$   Can we go the other direction (i.e., $\text{marginal exchangeability} \Longrightarrow \text{conditional iid}.$)?
\esh

\bsh \textbf{Theorem: de Finetti's}

Let $Y_i\in \mathcal{Y}$ for all $i \in \lrb{1,2,\ldots}$.  Assuming that, for any $n$, our (marginal) belief model for $Y_1,Y_2, \ldots, Y_n$ is exchangeable: $$p(y_1,y_2,\ldots,y_n)=p(y_{\pi_1},y_{\pi_2},\ldots,y_{\pi_n})$$
for all permutations $\bpi$ of $\lrb{1,2,\ldots,n}$.  

Then, there exist some parameter $\btheta$, some prior distribution on $\btheta$, and some generating distribution (sampling model) $p(y\given\theta)$, such that marginal model can be written as
$$p(y_1,y_2,\ldots,y_n)=\int_{\theta\in\Theta} \lrp{\prod_{i=1}^n p(y_i \given \theta)} p(\theta) d\theta.$$ 
\esh

The theorem above together with the result before it, imply that (note the arrow ingoing in both directions)
$$\left. \bmat Y_1, Y_2,\ldots, Y_n\given \theta \text{ iid}\\ \btheta\sim p(\theta) \emat\right\} \Longleftrightarrow Y_1, Y_2,\ldots, Y_n\text{ are marginally exchangeable}.$$

\pagebreak
\section{Some motivation for Bayesian statistics}

\bsh
Again, the essence of the Bayesian approach to statistical inference can be distilled into the three following steps:
\benum
\item assume a probability distribution (the prior) on any unknowns, 
\item assume the distribution of the knowns given the unknowns (this is the data-generating distribution or likelihood), 
\item follow the rules of probability to answer any questions of interest. 
\eenum
\esh
This provides a coherent framework for making inferences about unknown parameters $\theta$, as well as of any future or missing data, and for making rational decisions based on such inferences, where uncertainty is quantified by means of probability distributions.  Since essentially all statistical methods involve assuming the form of the generating distribution (likelihood), it is the prior that distinguishes the Bayesian approach, and makes it possible to just follow the rules of probability.

\subsection{Types of problems}

While this is only a subset of problems that can be attacked using the Bayesian machinery, these come up quite often:
\bit
\item estimate unknown parameters
\item infer hidden variables or missing data
\item test a hypothesis
\item conduct inference on sequential experiments
\eit
Solutions to all of them are consistent with decision theoretic optimality, which in the Bayesian context amounts to minimizing the posterior expected loss, as discussed in our \emph{Foundations of Bayesian Statistics}  notes.

\subsection{Methods to derive solutions to Bayesian problems}

Depending on the problem we are attempting to solve, there are different strategies available.  In the examples covered in the first couple of lectures (i.e., Foundations), we were able to determine the posterior distribution (along with the posterior expectation, and other derived distributions) in closed form, this is known as the {\bemph{exact solution}}, since the distribution is available in a closed and known form. As we will see in Chapter 3 of the Hoff book, this is often the case when we have exponential family likelihoods together with conjugate priors.  Among exponential family distributions, the Gaussian (a.k., normal) provides an important and highly flexible class that is amenable to exact solutions.

While deriving an exact solution is the ideal, these are only available when the analytical form of the posterior is known, which is the exception rather than the norm in more advanced modeling problems.  Whenever the exact form is not available, we have two types of alternatives to derive approximations to the solutions: {\bemph{deterministic}} and {\bemph{stochastic approximations}}.

Among the {\bemph{deterministic approximation}} methods are: numerical integration (quadrature, cubature), quasi-Monte Carlo, Laplace approximations, Variational Bayes. These strategies are viable and accurate whenever the corresponding integrals are low-dimensional.

On the other hand {\bemph{stochastic approximations}} are often the only option for high-dimensional integrals.  The idea behind these methods is that by drawing sufficient samples from the posterior (or from a distribution that convereges to the posterior) we can approximate posterior expectations. Some approaches corresponding to this class of strategies are Monte Carlo approximation, importance sampling, Markov Chain Monte Carlo (MCMC) (among these -- Gibbs Sampling, Metropolis and Metropolis-Hastings algorithms, slice sampling, Hamiltonian MCMC), sequential sampling methods, approximate Bayesian computation, etc.

We will delve into the details about several of these approaches as we make our way through the course's content.

\section{Comparison of Frequentist and Bayesian Approaches}

The quintessential difference between Bayes and frequentist perspectives stems from what is considered random and what is considered given/fixed. Following the notation we have worked with thus far ($\theta$ for the unknowns and $y$ for the data):
\bdes
\item[Bayesian inference] assumes that a particular $y$ is observed (given), ignoring all other possible datasets that could have been.  Conversely, $\theta$ is taken to be random. Here, we make the best decision possible \emph{given that} we have observed data $y$, allowing to build-in our beliefs (or lack thereof) about $\theta$.
\bit
\item We can only intepret probability from the Bayesian perspective as a subjective representation of the strength of our belief in the truth attached to a proposition. 

\item A considerable advantage of the Bayesian method is {\bemph{coherence}}; that is, there will be no inconsistencies under the assumed prior and likelihood when doing inference. This is great if the prior and likelihood are well specified; however, if the prior, the likelihood (or both) have issues, the resulting solution can also be consistently bad.

\eit

\item[Frequentist inference] assumes that $y$ is only one of many possible datasets arising from a \emph{generating distribution}, whereas $\theta$ is fixed but unknown. Frequentist decision procedures have guaranteed performance when used repeatedly. Given that $\theta$ is assumed fixed there is no probability distribution attached to it. 
\bit
\item Probability from the frequentist standpoint is associated with the \emph{frequency} (understood as the fraction of times) of an outcome in infinitely many trials, thus the name \emph{frequentist}. 
\item Frequentist tools provide {\bemph{calibration guarantees}}, in the sense that we can start by establishing performance features that our methods will possess; these features will be ensured if the procedure is applied repeatedly (e.g., this is the case when we specify the significance level $\alpha$ for a hypothesis test). 
\eit
\edes

In spite of the obvious differences between the two approaches, there is no need to be dogmatic about one or the other.  We know that if we find appropriate choices for the likelihood and prior, the Bayes procedure is guaranteed to be optimal; however, in many situations it is not clear if the prior and likelihoods are adequate, if the approximations used are good enough, or under which conditions the method will perform well.  In these cases, one may use frequentist tools, such as cross-validation, goodness-of-fit tests, and posterior predictive validation; or determine frequentist theoretical properties of the Bayesian approach, such as consistency (convergence to the true value of $\theta$), coverage and rates of covergence.




\end{document}