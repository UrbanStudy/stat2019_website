\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espaÃ±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{textgreek}


\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbs}[1]{\boldsymbol{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\N}{\mbb{N}_0}
\renewcommand{\d}{\text{d}}
\newcommand{\by}{\mbf{y}}
\newcommand{\mts}{\tilde{Y}}
\newcommand{\mtsv}{\tilde{\bv}}
\newcommand{\btw}{\tilde{\bw}}
\newcommand{\bhw}{\hat{\bw}}
\newcommand{\btx}{\tilde{\bx}}
\newcommand{\pt}{\tilde{p}}
\newcommand{\ba}{\mbf{a}}
\newcommand{\bb}{\mbf{b}}
\newcommand{\bc}{\mbf{c}}
\newcommand{\bd}{\mbf{d}}
\newcommand{\boe}{\mbf{e}}
\newcommand{\bk}{\mbf{k}}
\newcommand{\bq}{\mbf{q}}
\newcommand{\br}{\mbf{r}}
\newcommand{\bs}{\mbf{s}}
\newcommand{\bh}{\mbf{h}}
\newcommand{\bff}{\mbf{f}}
\newcommand{\bt}{\mbf{t}}
\newcommand{\bu}{\mbf{u}}
\newcommand{\bm}{\mbf{m}}
\newcommand{\bv}{\mbf{v}}
\newcommand{\bx}{\mbf{x}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\bz}{\mbf{z}}
\newcommand{\tby}{\tilde{\mbf{y}}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\bp}{\mbs{p}}
\newcommand{\bA}{\mbf{A}}
\newcommand{\tbA}{\tilde{\bA}}
\newcommand{\bB}{\mbf{B}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\tc}{\tilde{c}}
\newcommand{\tC}{\tilde{C}}
\newcommand{\tbC}{\tilde{\bC}}
\newcommand{\bF}{\mbf{F}}
%\newcommand{\bBs}{\mbf{B}^\star}
\newcommand{\bD}{\mbf{D}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bK}{\mbf{K}}
\newcommand{\bQ}{\mbf{Q}}
\newcommand{\bV}{\mbf{V}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\hN}{\hat{N}}
\newcommand{\tbc}{\tilde{\mbf{c}}}
\newcommand{\tba}{\tilde{\mbf{a}}}
\newcommand{\tbX}{\tilde{\mbf{X}}}
\newcommand{\tbW}{\tilde{\mbf{W}}}
\newcommand{\bSigma}{\mbs{\Sigma}}
\newcommand{\bGamma}{\mbs{\Gamma}}
\newcommand{\bUps}{\mbs{\Upsilon}}
\newcommand{\bPsi}{\mbs{\Psi}}
\newcommand{\vs}{v^\star}
\newcommand{\Vs}{V^\star}
\newcommand{\bvs}{\bv_i^\star}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bP}{\mbf{P}}
\newcommand{\bBs}{\bB^\star}
\newcommand{\bXs}{\bX^\star}
\newcommand{\bxs}{\bx^\star}
\newcommand{\bWs}{\bW^\star}
\newcommand{\bws}{\bw_i^\star}
\newcommand{\bwsp}{\left.\bw_i^\star\right.^\prime}
\newcommand{\bBsp}{\left.\bB^\star\right.^\prime}
\newcommand{\bVs}{\bV^\star}
\newcommand{\bpsi}{\mbs{\psi}}
\newcommand{\bphi}{\mbs{\phi}}
\newcommand{\bmu}{\mbs{\mu}}
\newcommand{\bbeta}{\mbs{\beta}}
\newcommand{\bxi}{\mbs{\xi}}
\newcommand{\bchi}{\mbs{\chi}}
\newcommand{\blambda}{\mbs{\lambda}}
\newcommand{\bLambda}{\mbs{\Lambda}}
\newcommand{\LamT}{\tilde{\Lambda}}
\newcommand{\GamT}{\tilde{\Gamma}}
\newcommand{\WT}{\tilde{W}}
\newcommand{\balpha}{{\mbs{\alpha}}}
\newcommand{\bepsilon}{{\mbs{\varepsilon}}}
\newcommand{\bgamma}{{\mbs{\gamma}}}
\newcommand{\btheta}{{\mbs{\theta}}}
\newcommand{\bseta}{{\mbs{\eta}}}
\newcommand{\bpi}{{\mbs{\pi}}}
\newcommand{\bI}{\mbf{I}}
\newcommand{\bH}{\mbf{H}}
\newcommand{\tbH}{\tilde{\mbf{H}}}
\newcommand{\1}{\mbs{1}}
\newcommand{\0}{\mbs{0}}
\newcommand{\detstar}[1]{\text{det}^+\left(#1\right)}
\renewcommand{\det}[1]{\text{det}\left(#1\right)}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\renewcommand{\exp}[1]{\text{exp}\left[#1\right]}
\newcommand{\M}{{M}}
\newcommand{\K}{{K}}
\newcommand{\peq}{{p}}
\newcommand{\MB}{{M_B}}
\newcommand{\MF}{{M_F}}
\newcommand{\MT}{{M_T}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\graph}{\Gamma}
\newcommand{\kset}{\Upsilon}
\newcommand{\order}{order}
\newcommand{\parents}{\mcal{P}}
\newcommand{\children}{\mcal{C}}
\newcommand{\extreme}{\mcal{E}}
\newcommand{\combined}{\mcal{A}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left\{#1\right\}}
\newcommand{\lrno}[1]{\left.#1\right.}
\newcommand{\lrsqb}[1]{\left[#1\right]}
\newcommand{\G}[1]{\Gamma_{#1}}
\renewcommand{\d}{\text{d}}
\newcommand{\Ps}[1]{\Pr{\left(#1\right)}}
\newcommand{\BF}[2]{{BF}_{#1,#2}(Y)}
\newcommand{\BFd}[3]{{BF}_{#1,#2}(Y,#3)}
\newcommand{\xmark}{\ding{55}}
\newcommand{\ben}{\begin{equation*}}
\newcommand{\een}{\end{equation*}}
\newcommand{\bean}{\begin{eqnarray*}}
\newcommand{\eean}{\end{eqnarray*}}
\newcommand{\bsm}{\begin{smallmatrix}}
\newcommand{\esm}{\end{smallmatrix}}
\newcommand{\bmat}{\begin{matrix}}
\newcommand{\emat}{\end{matrix}}
\newcommand{\tI}{\text{I}}
\newcommand{\tN}{\text{N}}
\newcommand{\trN}{\text{trunc.N}}
\newcommand{\nl}[1]{\text{log}{\lrp{#1}}}
\newcommand{\e}[1]{\text{exp}{\lrb{#1}}}
\newcommand{\indf}[1]{\tI_{\left\{#1\right\}}}
\newcommand{\parent}{\mcal{P}}
\newcommand{\gp}{\text{GP}{\lrp{\0,\mcal{C}(\cdot\given\bphi)}}}
\newcommand{\gpd}[3]{\text{GP}_{#1}{\lrp{\0,\mcal{C}_{#2}(\cdot\given\phi_{#3})}}}
\newcommand{\mnngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mbs{\mcal{C}}}_{#2}(\cdot,\cdot;\bphi_{#3})}}}
\newcommand{\nngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mcal{C}}_{#2}(\cdot,\cdot;\phi_{#3})}}}
\newcommand{\nngpw}[4]{\text{NNGP}_{#1}^{#2}{\lrp{0,\tilde{\mcal{C}}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpw}[4]{\text{GP}_{#1}^{#2}{\lrp{0,\mcal{C}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpq}{\text{GP}_q{\lrp{\0,\mbs{\mcal{C}}(\cdot\given\bphi)}}}
\newcommand{\gpk}{\text{GP}{\lrp{0,\mcal{C}(\cdot\given\tilde{\phi}_k)}}}
\newcommand{\nngp}{\text{NNGP}{\lrp{\0,\tilde{\mcal{C}}(\cdot\given\phi)}}}
\newcommand{\refset}{\mcal{T}}
\newcommand{\uset}{\mcal{U}}
\newcommand{\oset}{\mcal{T}}
\newcommand{\Xall}{\mbb{X}}
\newcommand{\given}{\,|\,}
\newcommand{\dtr}[1]{\textcolor{blue}{(#1)}}
\newcommand{\bemph}[1]{\bf \emph{#1}}
\newcommand{\var}[1]{\text{var}{(#1)}}

%-------------------------------
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{exa}{Example}
\newtheorem{note}{Note}
\newcommand{\bex}{\begin{exer}}
\newcommand{\eex}{\end{exer}}
\newcommand{\bexa}{\begin{exa}}
\newcommand{\eexa}{\end{exa}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\bdes}{\begin{description}}
\newcommand{\edes}{\end{description}}

\newcommand{\bsh}{\begin{shaded}}
\newcommand{\esh}{\end{shaded}}
%-------------------------------

%-------------------------------
%hiding proof solutions
%-------------------------------


\begin{document}


\setcounter{section}{0}
\title{Exponential Families and Conjugacy}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf Exponential Families and Conjugacy}\\
{\large STAT 572: Bayesian Statistics}\\
Fall 2019
\end{center}
\section{Introduction}

Several probabilistic models that we often work with belong to a class of distributions that share a particular formulaic representation, which has many special properties. These distributions are known as \textsf{Exponential Family} distributions. Some examples of exponential families are \textsf{Poisson}, \textsf{Beta}, \textsf{Bernoulli}, \textsf{Normal}, \textsf{log-Normal}, \textsf{Exponential}, and \textsf{Gamma}, among others.

So why are these distributions relevant to this course?  Well, it turns out that the posterior distribution obtained from an exponential family likelihood often has a form that is easy to work with, and can often be used with success to tackle Bayesian problems.  

Below we will make explicit the concepts of {\bemph{exponential families of distributions}} (for one and multiple parameters), {\bemph{sufficiency}}, {\bemph{natural/canonical forms}}, and {\bemph{conjugate families of priors}}.


\section{One Parameter Exponential Families}

\subsection{Sufficiency}

As you might recall from your mathematical statistics courses, a {\bemph{Statistic}} corresponds to a function of a random sample (i.e., a set of iid random variables $Y_1,Y_2,\ldots, Y_n$) that is often used to represent/estimate a characteristic of the population.   Sufficiency is a desirable property for a statistic to have, one which we will often make use of, and that in some instances may help us reduce the dimensionality of a problem.

\bsh
\textbf{Definition (Sufficient Statistic):}

\emph{Let $Y=(Y_1,Y_2,\ldots, Y_n)$ denote a random sample whose distribution is indexed by the parameter $\theta$ (while in one-parameter families $\theta$ is univariate, there are distrbutions in which it is a vector).} 

\emph{We say that a statistic $t(y)=t(y_{1:n})=t(y_1,y_2,\ldots,y_n)$ is {\bemph{parametric sufficient}} (or simply sufficient) for $\theta$ if it captures all the useful information in the data concerning $\theta$.  This implies that $t$ is sufficient for $\theta$ if $$p(\theta \given y)=p(\theta \given t(y) ).$$ In other words, this means that $t(y)$ is sufficient for $\theta$ if the posterior distribution for $\theta$ only depends on the data through the statistic $t(y)$.}
\esh


\bsh
\textbf{Theorem (Factorization Theorem):}
 
The statistic $t(y)$ is parametric sufficient if and only if
$$p(\theta \given y)\;=\;\frac{g\lrp{t(y),\theta}p(\theta)}{\int_{\Theta} g\lrp{t(y),\theta}p(\theta) d\theta},$$
for some function $g(\cdot)$.

The {\bemph{Factorization Theorem}} provides necessary and sufficient conditions for a statistic to be sufficient.
\esh

\subsection{The exponential family form}

\bsh
A \emph{one-parameter exponential family} is the collection of probability distributions indexed by a parameter $\theta\in\Theta$, having pdf/pmf of the form
\bea
p(y\given \theta) &=& \exp{\varphi(\theta) t(y)-n \kappa(\theta)} h(y),
\label{eq:exptfam}
\eea
for some functions $\varphi(\theta)$, $\kappa(\theta)$, and $h(y)$, and with $t(y)$ denoting a sufficient statistic for $\theta$. 

(The form we use in the class for exponential family distributions is slightly different to the one provided in the book, but they are equivalent nonetheless.  Convince yourself!)
\esh

Note that for $n>1$ with iid observations, we have $t(y)=t(y_{1:n}) = \sum_i \tilde{t}(y_i)$, and $h(y)=\prod_i \tilde{h}(y_i)$.

Interestingly, this form of the likelihood revals that $\kappa(\theta)$ acts as a log-normalizing constant, since
\bean
1\quad=\quad \int p(y \given \theta) dy &=& \int \exp{\varphi(\theta) t(y)- n \kappa(\theta)} h(y) dy\\
&=& \lrp{\int \exp{\varphi(\theta) t(y)} h(y) dy} \;e^{-n \kappa(\theta)}
\eean
$$\Longrightarrow 1 / e^{- n \kappa(\theta)} \quad=\quad  e^{n \kappa(\theta)}\quad = \quad \int \exp{\varphi(\theta) t(y)} h(y) dy,$$
equivalently $\kappa(\theta)=\frac{1}{n}\ln{\lrp{\int \exp{\varphi(\theta) t(y)} h(y) dy}}$. 

\bsh
{\note
Importantly, notice that for any exponential family of distributions, the function $g(t(y),\theta)$ used in the Factorization Theorem corresponds to $$g(t(y),\theta)=\exp{\varphi(\theta) t(y)- n \kappa(\theta)}.$$
}
\esh

\bsh
\bexa {\bemph{Bernoulli Happyness}}

Consider the example from the General Social Survey, where $n=129$ women were asked the question: \emph{Are you generally happy?} For this problem, we denote by $Y_i\given \theta\stackrel{iid}{\sim}\text{Bernoulli}(\theta)$ the response of each woman, with $Y_i=1$ if the $i$th woman reports being \emph{generally happy} and $Y_i=0$ otherwise. Represent in \emph{exponential form} the likelihood for $Y=(Y_1, \ldots,Y_{129})$. Express the likelihood for this problem in exponential family form.

%\vspace{7cm}

First, recall that the likelihood from a dataset consisting of $n$ Bernoulli$(\theta)$ iid variables is $$p(y \given \theta)=\theta^{\sum_i y_i} (1-\theta)^{n-\sum_i y_i} \lrp{\prod_{i=1}^{n} \1_{\lrsqb{y_i\in\lrb{0,1}}}}.$$

Given that the  likelihood factors into a function of both $\theta$ and the data, exclusively through  $\sum_i y_i$, then by the Factorization Theorem $\sum_i y_i$ is sufficient for $\theta$. Matching the remaining terms in the likelihood to those in Equation \eqref{eq:exptfam}, we have that written in exponential family form the generating model for $Y=Y_1,\ldots,Y_{n}$ iid Bernoulli$(\theta)$ is

\bean
p(y_{1:n}\given \theta) &=& \text{exp}\Bigg\{\underbrace{\ln{\lrp{\frac{\theta}{1-\theta}}}}_{=\varphi(\theta)} \underbrace{\lrp{\sum_{i=1}^n y_i}}_{t(y)} - n \underbrace{\lrp{- \ln{(1-\theta)}}}_{=\kappa(\theta)} \Bigg\} \underbrace{\lrp{\prod_{i=1}^{n} \1_{\lrsqb{y_i\in\lrb{0,1}}} }}_{=h(y)}
\eean

It is interesting to note that whenever $n=1$, a sufficient statistic for $\theta$ is $y_1$.
  
\eexa
\esh

\bsh
\bexa {\bemph{Poisson exponential family form}}

Assume that you now have a Poisson$(\theta)$ random sample $X_1,X_2,\ldots,X_n$.  Show that the Poisson distribution belongs to the exponential family class, identifying $\varphi(\theta)$, $\kappa(\theta)$, $h(y)$, and $t(y)$.

% \vspace{7cm}
The likelihood for this random sample is
$$p(x \given \theta)=\theta^{\sum_i x_i} e^{-n\theta} \lrp{\prod_{i=1}^{n} \frac{1}{x_i!}\1_{\lrsqb{x_i\in\lrb{0,1,\ldots}}}}.$$

Note that we can decompose the likelihood into the following elements
\bean
t(y) &=& \sum_{i=1}^{n}x_i \\
\varphi(\theta) &=& \ln{\theta}\\
\kappa(\theta) &=& \theta\\
h(y) &=& \prod_{i=1}^{n} \frac{1}{x_i!}\1_{\lrsqb{x_i\in\lrb{0,1,\ldots}}}.
\eean
Hence, the Poisson likelihood admits an exponential form representation.

\eexa
\esh

\subsection{Natural Form}

An exponential family distribution is said to be in {\bemph{natural form}} (a.k.a. {\bemph{canonical form}}) if $\varphi(\theta)=\theta$; in which case $\theta$ is called the {\bemph{natural or canonical parameter}}.

\bsh
\bexa (Exponential). Consider the random variable $X\sim \text{Exp}(\theta)$ (so $n=1$). 

% \vspace{7cm}
Its density is given by
\bean
p(x\given \theta)= \theta \; e^{-\theta x} \1_{\lrb{x>0}} &=&\exp{\theta (-x) - (-\ln{\theta})} \1_{\lrb{x>0}}
\eean
Hence, we have that
\bean
t(x) &=& -x\\
\varphi(\theta) &=& \theta\\
\kappa(\theta) &=& -\ln{\theta}\\
h(y) &=& \1_{\lrb{x>0}}.
\eean
Because $\varphi(\theta)=\theta$, this is the natural or canonical form for the family of Exponential distributions, where  $\theta$ corresponds to the natural/canonical parameter.
\eexa
\esh

\bsh
\bexa (Poisson). Find the natural form for the $\text{Poisson}(\theta)$ family of distributions (assume a single observation $X$).

% \vspace{7cm}
\bean
p(x \given \theta)&=&\theta^{x} e^{-\theta} \lrp{\frac{1}{x!}\1_{\lrsqb{x\in\lrb{0,1,\ldots}}}}\\
&=&\exp{\ln{\theta}\; x - \theta} \lrp{\frac{1}{x!}\1_{\lrsqb{x\in\lrb{0,1,\ldots}}}}
\eean

\vspace{3cm}
Hence, letting $\eta = \ln(\theta)$, we can rewrite the expression above as in natural form as
\bean
p(x \given \theta)&=&\exp{\eta\; x - e^\eta} \lrp{\frac{1}{x!}\1_{\lrsqb{x\in\lrb{0,1,\ldots}}}}.
\eean
with natural/canonical parameter $\eta=\ln(\theta)$.
\eexa
\esh


\section{Conjugacy}

\paragraph{Definition (Conjugate prior family):}
For a particular family of \emph{generating distributions} $\lrb{p(y\given \theta): \theta \in \Theta}$, a collection of priors $p_\alpha(\theta)$ indexed by $\alpha\in H$ is said to be a {\bemph{conjugate prior family}} if for any $\alpha$ and any data $y$, the resulting posterior is equal to $p_{\alpha'}(\theta)$ for some $\alpha'\in H$.


An alternative way to define a conjugate prior is to say that a prior $p(\theta) \in \parents$ is conjugate for a generating (sampling) model if $$p(\theta) \in \parents \Rightarrow p(\theta \given y) \in \parents.$$

\textbf{IMPORTANT!!!} \emph{The fact that a family of prior distributions is conjugate for a particular family of generating models does not make it automatically a good prior choice. The prior being conjugate simply facilitates posterior calculations, sometimes with the added benefit that the conjugate family is flexible enough to capture the prior information. There are instances when no single prior distribution within the family of conjugate priors able to fully represent our prior beliefs.   }

\emph{However, as we will see later in the course, in these situations a successful strategy is to use mixtures of prior distributions from the conjugate family.}



\bsh
\bexa (Beta-Bernoulli). 

The collection of Beta$(\theta \given a, b)$ prior distributions, with $a, b > 0$, is conjugate to the family of Bernoulli$(\theta)$ generating distributions, since the posterior is $$p(\theta \given y) = \text{Beta}\lrp{\theta \given a + \sum y_i, b + n - \sum y_i}.$$

\eexa
\esh

\bsh
\bexa (Gamma-Exponential). The collection of Gamma$(\theta \given a , b)$ priors, with $a, b > 0$, is conjugate to the Exp$(\theta)$ generating distribution, since the posterior is $p(\theta \given y) = \text{Gamma}\lrp{\theta \given a +n , b + \sum y_i}$.
\eexa
\esh

Although we have thus far talked about {\bemph{THE conjugate prior family}}, we should rather have said {\bemph{A conjugate prior family}}.  It turns out that the conjugate prior family of distributions IS NOT UNIQUE.

To see this is, consider the nonnegative function $q(\theta)$ and assume that $\lrb{p_\alpha(\theta): \alpha\in H}$ is a conjugate family. Letting $$z(\alpha)=\int p_\alpha(\theta) q(\theta) d\theta,$$ and if $0<z(\alpha)<\infty$ for all $\alpha\in H$, then the family $$\lrb{p_\alpha(\theta) q(\theta)/z(\alpha): \alpha\in H}$$ is a conjugate family.  A very useful case is when we set $q(\theta)=\1_{\theta\in A}$, where $A\subset \Theta$.

\subsection{Conjugate priors for exponential families}

Assuming very general conditions, conjugate priors for ANY exponential family distribution has pdf/pmf of the form 
\bea
p_{n_o,t_0}(\theta)&\propto& \exp{n_0 t_0 \varphi(\theta)-n_0 \kappa(\theta)} \1_{\lrb{\theta\in \Theta}},
\label{eq:conjexptpr}
\eea
where $n_0>0$ and $t_0\in\mathbb{R}$ are values for which $p_{n_o,t_0}(\theta)$ can be normalized.

Neveretheless, under certain families of priors it is not always possible to calculate the normalization constant in closed form.  In practice, when making reference to conjugate priors, it is often implied that the prior family has a computationally tractable normalization constant.

Notice that when combining a prior of the form \eqref{eq:conjexptpr} with an exponential family generating model represented by \eqref{eq:exptfam}, we obtain posteriors of the form

\bea
p(\theta\given y)&\propto& \lrp{e^{\lrb{t(y)\varphi(\theta) - n \kappa(\theta)}} h(y)}\lrp{e^{\lrb{n_0 t_0 \varphi(\theta)-n_0 \kappa(\theta)}} \1_{\lrb{\theta\in \Theta}}}\nonumber\\
&\propto& \exp{(t(y) + n_0 t_0 )\varphi(\theta) - (n+n_0)\kappa(\theta)} \1_{\lrb{\theta\in \Theta}}.
\label{eq:conjexptpost}
\eea

Recalling that if we have more than one observation (i.e., $n>1$) then $t(y)=\sum \tilde{t}(y_i)$, convince yourself that letting 

$$n^\star = n_0+n\quad\text{ and }\quad t^\star = \frac{n_0}{n_0 + n} t_0  + \frac{n}{n_0 + n}\frac{1}{n}\sum \tilde{t}(y_i),$$ 

implies that the posterior is given by $p_{n^\star,t^\star}(\theta)$; or in other words, that the posterior is also a member of the same family of distributions as the prior $p_{n_o,t_0}(\theta)$. 

Note that $t^\star$ is a convex combination of $t_0$ and $t(y)$ with weigths $n_0$ and $n$, respectively. Expressing in this form the prior and posterior provides some idea about how to interpret $t_0$ and $n_0$: $t_0$ is a prior \emph{best guess} and $n_0$ is the \emph{strength} we attach to this best guess.

\subsection{Summary one-parameter exponential families of distributions}

Assuming $n=1$, some examples of one-parameter exponential families, their conjugate families, and the corresponding families of posterior distributions are:
\begin{center}
\begin{tabular}{ c | c | c}
Generating Family & Conjugate Family & Posterior Family\\
\hline
$Y\sim \text{Bernoulli}(\theta)$ & $\theta\sim \text{Beta}(a,b)$ & $\theta\given Y=y\sim \text{Beta}(a+y,b+1-y)$\\
$Y\sim \text{Poisson}(\theta)$ & $\theta\sim \text{Gamma}(\alpha,\beta)$ & $\theta\given Y=y \sim \text{Gamma}(\alpha+y,\beta+1)$\\
$Y\sim \text{Exp}(\theta)$ & $\theta\sim \text{Gamma}(\alpha,\beta)$ & $\theta\given Y=y \sim \text{Gamma}(\alpha+1,\beta+y)$\\

\end{tabular}
\end{center}


\section{Multiparameter Exponential Families}

The extension to families of distributions that are indexed by more than one parameter is straightforward.  A $k$-parameter {\bemph{exponential family}} is a collection of distributions indexed by the $k$-dimensional parameter vector $\theta\in\Theta$, with pdf's/pmf's of the form
\bea
p(y\given\theta)&=&\exp{\varphi(\theta)^{T} t(y)- n \kappa(\theta)},
\label{eq:multiexpfam}
\eea
for some vector valued functions 
$$\varphi(\theta)=\lrp{\bmat \varphi_1(\theta) \\ \vdots \\ \varphi_k(\theta) \emat}\quad\text{ and }\quad  t(y) = \lrp{\bmat t_1(y) \\ \vdots \\ t_k(y) \emat},$$
and real valued functions $\kappa(\theta)$ and $h(y)$. Again, we have $\kappa(\theta)$ representing a log-normalizing constant. Above we refer to $\nu^T$ to denote the transpose of vector $\nu$.  The definitions  provided in the revious section regarding natural (canonical) forms and conjugacy hold for the multiparameter case.

\bsh
\bexa (Gamma) 

The family of Gamma$(\alpha, \beta)$ generating distributions, with $\alpha, \beta>0$, are an exponential family since:
\bean
\text{Gamma}(y\given \alpha, \beta) &=& \frac{\beta^\alpha}{\Gamma(\alpha)} y^{\alpha - 1} e^{ - y\; \beta}\1_{\lrb{y>0}}\\
&=& %e^{-\beta y +(\alpha-1) \ln{y}+\ln{\lrp{\frac{\beta^\alpha}{\Gamma(\alpha)}}}}\1_{\lrb{y>0}},
\eean
%where we can set $\theta = (\alpha, \beta)^T$, $\varphi(\theta) = (-\beta, \alpha - 1)^T$, $t(y) = (y, \ln{y})^T$, and $h(y) = \1_{\lrb{y > 0}}$ to match the form of Equation \eqref{eq:multiexpfam}.
\eexa
\esh

\bsh
\bexa (Beta) 

The family of Beta$(\alpha, \beta)$ generating distributions, with $\alpha, \beta>0$, are an exponential family since:


\bean
\text{Beta}(y\given \alpha, \beta) &=& \frac{1}{B(\alpha+\beta)} y^{\alpha - 1} y^{\beta-1}\1_{\lrb{y\in(0,1)}}\\
&=& %e^{(\alpha-1) \ln{y}+(\beta-1) \ln{(1-y)} -\ln{B(\alpha+\beta)}}\1_{\lrb{y\in(0,1)}},
\eean

\vspace{3cm}
%which can be expressed in the form of \eqref{eq:multiexpfam} with $\theta = (\alpha, \beta)^T$, $\varphi(\theta) = (\beta - 1, \alpha - 1)^T$, $t(y) = (\ln{y}, \ln{(1-y)})^T$, and $h(y) = \1_{\lrb{y\in(0,1)}}$.
\eexa
\esh

% 
% 
% \section{Posterior Inference}
% 
% \subsection{Confidence Regions}


\end{document}