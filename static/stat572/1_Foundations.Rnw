\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espaÃ±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbs}[1]{\boldsymbol{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\N}{\mbb{N}_0}
\renewcommand{\d}{\text{d}}
\newcommand{\by}{\mbf{y}}
\newcommand{\mts}{\tilde{Y}}
\newcommand{\mtsv}{\tilde{\bv}}
\newcommand{\btw}{\tilde{\bw}}
\newcommand{\bhw}{\hat{\bw}}
\newcommand{\btx}{\tilde{\bx}}
\newcommand{\pt}{\tilde{p}}
\newcommand{\ba}{\mbf{a}}
\newcommand{\bb}{\mbf{b}}
\newcommand{\bc}{\mbf{c}}
\newcommand{\bd}{\mbf{d}}
\newcommand{\boe}{\mbf{e}}
\newcommand{\bk}{\mbf{k}}
\newcommand{\bq}{\mbf{q}}
\newcommand{\br}{\mbf{r}}
\newcommand{\bs}{\mbf{s}}
\newcommand{\bh}{\mbf{h}}
\newcommand{\bff}{\mbf{f}}
\newcommand{\bt}{\mbf{t}}
\newcommand{\bu}{\mbf{u}}
\newcommand{\bm}{\mbf{m}}
\newcommand{\bv}{\mbf{v}}
\newcommand{\bx}{\mbf{x}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\bz}{\mbf{z}}
\newcommand{\tby}{\tilde{\mbf{y}}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\bp}{\mbs{p}}
\newcommand{\bA}{\mbf{A}}
\newcommand{\tbA}{\tilde{\bA}}
\newcommand{\bB}{\mbf{B}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\tc}{\tilde{c}}
\newcommand{\tC}{\tilde{C}}
\newcommand{\tbC}{\tilde{\bC}}
\newcommand{\bF}{\mbf{F}}
%\newcommand{\bBs}{\mbf{B}^\star}
\newcommand{\bD}{\mbf{D}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bK}{\mbf{K}}
\newcommand{\bQ}{\mbf{Q}}
\newcommand{\bV}{\mbf{V}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\hN}{\hat{N}}
\newcommand{\tbc}{\tilde{\mbf{c}}}
\newcommand{\tba}{\tilde{\mbf{a}}}
\newcommand{\tbX}{\tilde{\mbf{X}}}
\newcommand{\tbW}{\tilde{\mbf{W}}}
\newcommand{\bSigma}{\mbs{\Sigma}}
\newcommand{\bGamma}{\mbs{\Gamma}}
\newcommand{\bUps}{\mbs{\Upsilon}}
\newcommand{\bPsi}{\mbs{\Psi}}
\newcommand{\vs}{v^\star}
\newcommand{\Vs}{V^\star}
\newcommand{\bvs}{\bv_i^\star}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bP}{\mbf{P}}
\newcommand{\bBs}{\bB^\star}
\newcommand{\bXs}{\bX^\star}
\newcommand{\bxs}{\bx^\star}
\newcommand{\bWs}{\bW^\star}
\newcommand{\bws}{\bw_i^\star}
\newcommand{\bwsp}{\left.\bw_i^\star\right.^\prime}
\newcommand{\bBsp}{\left.\bB^\star\right.^\prime}
\newcommand{\bVs}{\bV^\star}
\newcommand{\bpsi}{\mbs{\psi}}
\newcommand{\bphi}{\mbs{\phi}}
\newcommand{\bmu}{\mbs{\mu}}
\newcommand{\bbeta}{\mbs{\beta}}
\newcommand{\bxi}{\mbs{\xi}}
\newcommand{\bchi}{\mbs{\chi}}
\newcommand{\blambda}{\mbs{\lambda}}
\newcommand{\bLambda}{\mbs{\Lambda}}
\newcommand{\LamT}{\tilde{\Lambda}}
\newcommand{\GamT}{\tilde{\Gamma}}
\newcommand{\WT}{\tilde{W}}
\newcommand{\balpha}{{\mbs{\alpha}}}
\newcommand{\bepsilon}{{\mbs{\varepsilon}}}
\newcommand{\bgamma}{{\mbs{\gamma}}}
\newcommand{\btheta}{{\mbs{\theta}}}
\newcommand{\bseta}{{\mbs{\eta}}}
\newcommand{\bpi}{{\mbs{\pi}}}
\newcommand{\bI}{\mbf{I}}
\newcommand{\bH}{\mbf{H}}
\newcommand{\tbH}{\tilde{\mbf{H}}}
\newcommand{\1}{\mbs{1}}
\newcommand{\0}{\mbs{0}}
\newcommand{\detstar}[1]{\text{det}^+\left(#1\right)}
\renewcommand{\det}[1]{\text{det}\left(#1\right)}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\renewcommand{\exp}[1]{\text{exp}\left[#1\right]}
\newcommand{\M}{{M}}
\newcommand{\K}{{K}}
\newcommand{\peq}{{p}}
\newcommand{\MB}{{M_B}}
\newcommand{\MF}{{M_F}}
\newcommand{\MT}{{M_T}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\graph}{\Gamma}
\newcommand{\kset}{\Upsilon}
\newcommand{\order}{order}
\newcommand{\parents}{\mcal{P}}
\newcommand{\children}{\mcal{C}}
\newcommand{\extreme}{\mcal{E}}
\newcommand{\combined}{\mcal{A}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left\{#1\right\}}
\newcommand{\lrno}[1]{\left.#1\right.}
\newcommand{\lrsqb}[1]{\left[#1\right]}
\newcommand{\G}[1]{\Gamma_{#1}}
\renewcommand{\d}{\text{d}}
\newcommand{\Ps}[1]{\Pr{\left(#1\right)}}
\newcommand{\BF}[2]{{BF}_{#1,#2}(Y)}
\newcommand{\BFd}[3]{{BF}_{#1,#2}(Y,#3)}
\newcommand{\xmark}{\ding{55}}
\newcommand{\ben}{\begin{equation*}}
\newcommand{\een}{\end{equation*}}
\newcommand{\bean}{\begin{eqnarray*}}
\newcommand{\eean}{\end{eqnarray*}}
\newcommand{\bsm}{\begin{smallmatrix}}
\newcommand{\esm}{\end{smallmatrix}}
\newcommand{\bmat}{\begin{matrix}}
\newcommand{\emat}{\end{matrix}}
\newcommand{\tI}{\text{I}}
\newcommand{\tN}{\text{N}}
\newcommand{\trN}{\text{trunc.N}}
\newcommand{\nl}[1]{\text{log}{\lrp{#1}}}
\newcommand{\e}[1]{\text{exp}{\lrb{#1}}}
\newcommand{\indf}[1]{\tI_{\left\{#1\right\}}}
\newcommand{\parent}{\mcal{P}}
\newcommand{\gp}{\text{GP}{\lrp{\0,\mcal{C}(\cdot\given\bphi)}}}
\newcommand{\gpd}[3]{\text{GP}_{#1}{\lrp{\0,\mcal{C}_{#2}(\cdot\given\phi_{#3})}}}
\newcommand{\mnngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mbs{\mcal{C}}}_{#2}(\cdot,\cdot;\bphi_{#3})}}}
\newcommand{\nngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mcal{C}}_{#2}(\cdot,\cdot;\phi_{#3})}}}
\newcommand{\nngpw}[4]{\text{NNGP}_{#1}^{#2}{\lrp{0,\tilde{\mcal{C}}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpw}[4]{\text{GP}_{#1}^{#2}{\lrp{0,\mcal{C}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpq}{\text{GP}_q{\lrp{\0,\mbs{\mcal{C}}(\cdot\given\bphi)}}}
\newcommand{\gpk}{\text{GP}{\lrp{0,\mcal{C}(\cdot\given\tilde{\phi}_k)}}}
\newcommand{\nngp}{\text{NNGP}{\lrp{\0,\tilde{\mcal{C}}(\cdot\given\phi)}}}
\newcommand{\refset}{\mcal{T}}
\newcommand{\uset}{\mcal{U}}
\newcommand{\oset}{\mcal{T}}
\newcommand{\Xall}{\mbb{X}}
\newcommand{\given}{\,|\,}
\newcommand{\dtr}[1]{\textcolor{blue}{(#1)}}
\newcommand{\bemph}[1]{\bf \emph{#1}}
\newcommand{\var}[1]{\text{var}{(#1)}}

%-------------------------------
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{exa}{Example}
\newtheorem{note}{Note}
\newcommand{\bex}{\begin{exer}}
\newcommand{\eex}{\end{exer}}
\newcommand{\bexa}{\begin{exa}}
\newcommand{\eexa}{\end{exa}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\bdes}{\begin{description}}
\newcommand{\edes}{\end{description}}

\newcommand{\bsh}{\begin{shaded}}
\newcommand{\esh}{\end{shaded}}
%-------------------------------

%-------------------------------
%hiding proof solutions
%-------------------------------
\newif\ifhideproofs
\hideproofstrue %uncomment to hide proofs

\ifhideproofs
\usepackage{environ}
\NewEnviron{hide}{}
\let\proof\hide
\let\endproof\endhide
\fi
%-------------------------------


\begin{document}


\setcounter{section}{0}
\title{Foundations of Bayesian Statistics}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf Foundations of Bayesian Statistics}\\
{\large STAT 572: Bayesian Statistics}\\
Fall 2019
\end{center}
\section{Introduction}
\subsection{The big picture}
The underlying mechanisms driving every phenomenon around us may be deterministic or random. However, even in cases where these are deterministic, there may be aspects of them that are unknown or are hard to measure. As such, real world phenomena are often ridden with \emph{uncertainty}. For example, think about how a disease spreads through a population, how the yield over a period with a particular stock portfolio is determined, the population dynamics of a particular species in a given region, etc. This uncertainty has to be accounted for when we model the word around us.

In our day to day, we use probabilities loosely to express our beliefs, or the information we have at hand. Probabilities provide a framework to represent a set of rational beliefs, there is a relationship between probability and information, and Bayes' Rule provides an optimal method for updating beliefs when new information is obtained. The material we will study throughout the course will provide tools to relate observations to data generating mechanisms where uncertainty is present, allowing this uncertainty propagate through to the inferences we make.

\subsubsection{Why Bayes?}

The idea of using probability to represent our state of information can be formalized using Bayesian theory, through Bayes' rule one can update our knowledge. The Bayesian approach yields:
\benum
\item parameter estimates with good statistical properties
\item parsimonious descriptions of observed data
\item predictions for missing data and forecasts of future data
\item a computational framework for model estimation, selection and validation
\eenum

\bit
\item \emph{Induction} is the process of reasoning from specific cases to a general principle.
\item The statistical endeavor relies induction, where \emph{the data} (the specific case) is used to identify a plausible \emph{data-generating mechanism} (the general principle).
\item Data is unknown and uncertain.  
\item In statistics, we assume that data is produced by a specific data-generating mechanism that depends on some unknown parameters (say $\btheta$).
% \item A probabilistic model is assumed  to describe pre-experimental uncertainty in the data, where the only unknown are the model parameters
\eit

\bsh
\emph{The goal of statistical induction is to use a data sample to infer population characteristics (i.e., parameters) given the data}
\esh

\bsh
$$\bmat
\text{Probability:}\hfill &\hfill \underbrace{\text{parameters}}_{\text{known}} \longrightarrow \underbrace{\text{data}}_{\text{unknown}}\\
\text{Statistics:}\hfill &\hfill \underbrace{\text{parameters}}_{\text{unknown}} \longleftarrow  \underbrace{\text{data}}_{\text{known}}
\emat$$
\esh


\paragraph{Pre-experimentally}
\bdes
\item[] Data and parameters are both unknown and uncertain
\item[Freq./Bayes] A probabilistic model that connects data and parameters is assumed (describes pre-exp't uncertainty in data)
\item[Bayes] Probability is used to describe pre-experimental uncertainty in parameters
\edes

\paragraph{Post-experimentally}
\bdes
\item[] Data is now known, but parameters are unknown and uncertain
\item[Freq.] Assumed probability distribution of data is used to infer plausible parameter values (unknown but assumed fixed)
\item[Bayes] Bayes rule is used to update uncertainty about the parameters with the observed data, represented through in an updated probabilistic statement
\edes


\subsubsection{Coin Flipping}

Flipping a coin is the most common example used to characterize a random event with two possible outcomes.
\bit
\item How random is this event? If the coin is fair, is it always the case that $\Pr(\text{heads})=0.5$?
\item This very simple physical system is fully determined by the initial conditions observed when the coin is being tossed.
\item For example, does it matter if the toss is always made starting with heads up?
\item What are the sources of randomness?
\eit

\paragraph{Experiment:}
\bit
\item Flip the coin 10 times, starting with heads each time.  Count and record the number of heads. 

\item Can you determine if the coin is fair? How would you go about it?

\item What can you say if all your tosses landed on tails?

\item If I told you that half of my coins (assume that I have an extremely large number of them) are biased towards heads, how can you incorporate this information into your inferential procedure?
\eit

Let's formalize the general coin-flipping  problem:
\bit
\item Let $Y_1,Y_2,\ldots,Y_n$ denote the random variables for the possible outcomes for each of the $n$ tosses.

\item Suppose the $Y_i$'s are i.i.d. (independent and identically distributed), with probability of heads equal to $\theta$. 

\item If $\theta$ is known, this becomes a probabilistic model characterizing the distribution of $Y_{1:n}=(Y_1,\ldots,Y_n)$, which allows us to study properties of $Y_{1:n}$ under this distribution.  This is a probabilistic problem.
\item Conversely, statistics serves an \emph{inversion purpose}, looking to extract information about $\theta$ (the cause / generating mechanism) from $Y_{1:n}$ (the consequence).
\eit


\bsh
{\bf Fun finding:}
Diaconis et al. (2007) argue that the coin flip is slightly more likely to land the same way it started,  they found that, if the coin starts on heads, then $P(heads)=0.51$. 
\esh

Ok, so if we observe data {\bf \emph{how can we verify the findings from Diaconis et al. (i.e., if the coin starts in heads then tosses are biased towards heads)?}}
\bit
\item We could estimate $\theta$ as the proportion of heads based on realizations of $Y_{1:n}$. 

\item Is $\theta$ close to 0.5? But... how close is \emph{close}?

\item How do we quantify our uncertainty about being correct?

\item From experience we know that $\theta$ is far from 0 and 1; however, when the number of tosses $n$ is small there is a non-trivial probability that tosses land either all on heads or  all on tails.  

\item How can we incorporate this prior knowledge about $\theta$ into the modeling problem?
\eit

\pagebreak

\subsection{The Bayesian take on the problem}

\bit
\item Central to Bayesian inference is the introduction of a {\bf  \emph{prior probability distribution}} for $\theta$, which characterizes how plausible each value of $\theta\in \Theta$ is, where $\Theta$ is the parameter space.

\item Inferences about $\theta$ are then made by obtaining the conditional distribution of $\theta$ given the data. This conditional distribution is known as the {\bf \emph{posterior probability distribution}}, alluding to the fact that it represents our uncertainty about $\theta$ after having observed the data.

\item This is attained via Bayes' theorem as
\bsh
\bea
p(\theta\given y_{1:n})&=&\frac{p(y_{1:n}\given \theta)p(\theta)}{p(y_{1:n})}\;\propto\; p(y_{1:n}\given \theta)p(\theta).
\label{eq:Bayes}
\eea
\esh
Expressing the posterior as ``proportional to the likelihood times the prior'' is possible since the denominator solely depends on the data $y_{1:n}$ but not on $\theta$. Bayes' theorem is essentially the definition of conditional probability $$\Pr(B\given A)\;=\; \frac{\Pr(A\cap B)}{\Pr(A)}\;=\; \frac{\Pr(A\given B)\Pr(B)}{\Pr(A)}$$ extended to conditional densities (which in Bayes' time didn't exist!).

\item In a nutshell, the Bayesian approach is to:
\bsh
\benum
\item assume a probability distribution (the prior) on any unknowns, 
\item assume the distribution of the knowns given the unknowns (this is the data-generating distribution or likelihood), 
\item then just follow the rules of probability to answer any questions of interest. 
\eenum
\esh
This provides a coherent framework for making inferences about unknown parameters $\theta$ as well as any future data or missing data, and for making rational decisions based on such inferences.  Since essentially all statistical methods involve assuming the form of the generating distribution, it is the prior that distinguishes the Bayesian approach, and makes it possible to just follow the rules of probability.
\eit

Back to the coin-flip problem... now, from the Bayesian perspective

\section{The Beta-Bernoulli Model}

\subsection{The likelihood function}

%Let's continue developing a satisfying solution to the coin-flipping problem.  
\bit
\item As you might already suspect, we'll model the binary outcomes from each coin-flip using the Bernoulli distribution.

\item As a convention, we assume that $Y_i=1$ ($1\leq i \leq n$) if the $i$th toss is heads and 0 otherwise. 

\item Binary outcomes are everywhere (neuroscience, ecology, computer vision, health, finance, epidemiology, etc.), which is why this distribution comes up so often.

\item When we say $Y\sim \text{Bernoulli}(\theta)$, then $$\Pr(Y=y\given \theta)=\left\{\bmat \theta&\text{if }y=1\\ 1-\theta&\text{if } y=0 \emat\right.,$$ with this probability being 0 for any other value of $y$.  Alternatively, the probability mass function (pmf) of $Y$ is $$p(y\given \theta)=\Pr(Y=y\given \theta) = \theta^y(1-\theta)^{1-y} \1_{\lrb{y\in\lrb{0,1}}}.$$

\item The mean or \emph{expected value} for a Bernoulli random variable $Y$ is $$E(Y)=\sum_{y\in\lrb{0,1}} y p(y\given \theta)=\theta$$

\item For the coin problem, if $Y_1,Y_2,\ldots,Y_n \stackrel{iid}{\sim}\text{Bernoulli}(\theta)$, we have that given $\theta$ the probabilistic model that describes the data is 
\bsh
\bea
\Pr(Y_{1:n}=y_{1:n}\given\theta) &=& p(y_{1:n}\given\theta)\;=\;\prod_{i=1}^n p(y_i\given \theta)\nonumber\\
&=& \prod_{i=1}^n \theta^{y_i}(1-\theta)^{1-y_i} \1_{\lrb{y_i\in\lrb{0,1}}}\nonumber\\
&=&  \theta^{\sum y_i}(1-\theta)^{n-\sum y_i} \lrp{\prod_{i=1}^n \1_{\lrb{y_i\in\lrb{0,1}}}}%\\
\label{eq:likelihoodBB}
\eea
where $y_{1:n}=(y_1,y_2,\ldots,y_n)$ correspond to a particular realization of $Y_{1:n}$. 
\esh

\item When viewed as a function of $\theta$, $p(y_{1:n}\given\theta)$ is referred to as the {\bf \emph{likelihood function}}, and is commonly denoted by $L(\theta;y_{1:n})$.  
\item Viewed as a distribution, this is what we referred to above as {\bf \emph{the data-generating mechanism}} (a.k.a. the sampling or generating distribution).
\eit

\subsection{The prior}

\bit
\item When Bayes originally studied the problem, he assumed a uniform prior for $\theta$, which is a particular case of the Beta distribution ($\text{Beta}(1,1)=\text{Uniform}(0,1)$).

\item For given $a,b >0$, we say $\btheta\sim\text{Beta}(a,b)$ to indicate that $\btheta$ has a probability density function (pdf) given by 
\bsh
\bea
p(\theta)&=&\text{Beta}(\theta\given a,b)\;=\;\frac{1}{B(a,b)}\theta^{a-1}(1-\theta)^{b-1} \1_{\lrb{\theta\in(0,1)}},
\label{eq:priorBB}
\eea
where $B(a,b)$ is Euler's Beta function.
\esh

\item Given that $B(a,b)$ is constant, we also say that $p(\theta)\propto \theta^{a-1}(1-\theta)^{b-1}$ on $(0,1)$.

\item Under this prior assumption, the (prior) mean for $\theta$ is $E(\btheta)=\int_0^1 \theta p(\theta) d\theta = a / (a+b)$. 

\eit

\begin{figure}[h]
<<echo=F, message=F, warning=F, fig.width=4, fig.height=4, fig.align='center'>>=
genprBetas <- function(a,b,colset="blue",addF=F){
  curve(dbeta(x,shape1=a,shape2=b),from=0.0,to=1,
        ylab=expression(p(theta)),xlab=expression(theta),
        n=500,col=colset,add=addF,lwd=2,ylim=c(0,5))
  abline(v=0.51,lty=3,lwd=1)
}


genprBetas(a=1,b=1,colset="grey",addF=F)
genprBetas(a=2,b=2,colset="blue",addF=T)
genprBetas(a=10,b=10,colset="red",addF=T)
genprBetas(a=0.5,b=0.5,colset="darkgreen",addF=T)
genprBetas(a=0.5,b=2,colset="magenta",addF=T)
genprBetas(a=2,b=0.5,colset="cornflowerblue",addF=T)
# abline(v=0.51,lwd=1,lty=3)
legend("top",lty=rep(1,4),lwd=2,
       col=c("grey","blue","red","darkgreen","magenta","cornflowerblue"),
       legend=c("a=1,b=1","a=2,b=2","a=10,b=10","a=0.5,b=0.5","a=0.5,b=2","a=2,b=0.5"),
       ncol=2,cex=0.7)
@
\caption{Prior density for $\btheta$ with different parameters $a,b>0$}
\label{fig:priortheta}
\end{figure}


{\bf Notation}
\bit 
\item $f(y) \propto g(y)$ (``$f$ is proportional to $g$'') means there is a constant $c$ such that $f(y) = cg(y)$ for all $y$. For functions of multiple variables, say $x$ and $y$, we write $\propto$ to indicate proportionality with respect to the variable of interest only, $y$ in our case. This simple device is surprisingly useful for deriving posterior distributions.
\item Commonly, we use capital letters to denote random variables (e.g., $Y$) and lowercase for particular values (e.g., $y$). However, in the case of \emph{theta}, we will use bold font to denote the random variable $\btheta$, and regular font for particular values $\theta$.
\item We will usually use $p$ for all pdf's and pmf's, following the usual convention that the symbol used (e.g., the $\theta$ in the expression $p(\theta)$) indicates which random variable we are talking about.
\eit


\subsection{The posterior}

\bit
\item Using Bayes' theorem \eqref{eq:Bayes} with the \emph{likelihood} \eqref{eq:likelihoodBB} and the \emph{prior} \eqref{eq:priorBB}, we can now specify the posterior for the coin-flipping problem.
\eit
\bsh
\bea
p(\theta\given y_{1:n})&\propto& p(y_{1:n} \given \theta) p(\theta) \nonumber\\
&=&    \lrp{\theta^{\sum y_i}(1-\theta)^{n-\sum y_i}}  \lrp{p^{a-1}(1-p)^{b-1} \1_{\lrb{\theta\in(0,1)}}} \nonumber \\
&=& \theta^{a + \sum y_i - 1 }(1-\theta)^{b + n-\sum y_i - 1}  \1_{\lrb{\theta\in(0,1)}} \nonumber \\
&\propto& \text{Beta}(a + \sum y_i, b + n-\sum y_i )
\eea
\esh

\bit
\item Therefore, $\btheta\given y_{1:n}\sim \text{Beta}(a + \sum y_i, b + n-\sum y_i )$, which is very convenient. 

\item The posterior has the same form (a Beta distribution) as the prior.  Priors that yield posteriors of the same form are special cases, which we refer to as {\bf \emph{conjugate priors}} (more on this later).

\item Because the posterior has this nice closed form, it makes it considerably easier to work with (e.g., compute integrals with respect to the posterior, sampling from it, etc.).
\eit

\begin{figure}[h]
<<echo=F, message=F, warning=F, fig.width=4, fig.height=4, fig.align='center'>>=
genBetas <- function(n,colset="blue",addF=F){
  y<-rbinom(n,1,0.51)
  curve(dbeta(x,shape1=(1+sum(y)),shape2=(1+n-sum(y))),from=0.0,to=1,
        ylab=expression(p(paste(theta,"|",y[1:n]))),xlab=expression(theta),
        n=500,col=colset,add=addF,lwd=2)
  abline(v=0.51,lty=3,lwd=1)
}


genBetas(1000,colset="cornflowerblue",addF=F)
genBetas(250,colset="blue",addF=T)
genBetas(100,colset="red",addF=T)
genBetas(10,colset="darkgreen",addF=T)
genBetas(0,colset="black",addF=T)
abline(v=0.51,lwd=1,lty=3)
legend("topright",lty=rep(1,4),lwd=2,
       col=c("cornflowerblue","blue","red","darkgreen","black"),
       legend=c("n=1000","n=250","n=100","n=10","n=0"))
@
\caption{Posterior density for $\btheta$ with different number of coin tosses}
\label{fig:posttheta}
\end{figure}

\pagebreak

\bexa Suppose we choose $a = 1$ and $b = 1$ (so the prior for $\btheta$ is uniform).

\bit
\item Let $n=9$ and generate iid samples $Y_1,\ldots,Y_n\sim \text{Bernoulli}(\theta)$ to obtain realizations $y_{1:n}$ of $Y_{1:n}$.  Assume a uniform prior for $\theta$.


\item Bayesian updating (one toss at a time): note what happens as information becomes available gradually.  As each toss is made, more information becomes available, and so the posterior from this draw can serve as the prior for the next one.


\begin{figure}[h]
<<echo=F, message=F, warning=F, fig.width=5.8, fig.height=5.8, fig.align='center'>>=
y <- cumsum(c(1,0,1,1,1,0,1,0,1))
W <- c("T","H")[c(1,0,1,1,1,0,1,0,1)+1]
colvec <- c("orange","blue")[c(1,0,1,1,1,0,1,0,1)+1] 
N <- 1:9
ps <- seq(0,1,length.out = 50)
prior <-  rep(1,50)
post <- dbeta(ps,shape1=(y[1]+1),shape2=(N[1]-y[1]+1))

par(mfrow=c(3,3))
for(ii in 1:9){
  post <- dbeta(ps,shape1=(y[ii]+1),shape2=(N[ii]-y[ii]+1))
  
  plot(x=ps,post,type="l",
     main="",xlab=expression(theta),ylab="",xaxt="n",yaxt="n")
  axis(side = 1, at = c(0,0.5,1),labels = c(0,0.5,1), cex=0.7)
  mtext(expression(paste("p(",theta," | ",y,")")),side=2,line=0)
  lines(x=ps,y=prior,lty=3,lwd=2)
  mtext(W,side=3,col=c(colvec[1:ii],rep(grey(0.9),(9-ii))),
        at=seq(0.05,0.95,length.out = 9),cex=0.9)
  prior <-  post
}

@
\caption{Posterior update for $\btheta$ one coin toss at a time}
\label{fig:onebyone}
\end{figure}


\item Now, all data at once

\begin{figure}[h]
<<echo=F, message=F, warning=F, fig.width=4, fig.height=4, fig.align='center'>>=
y <- cumsum(c(1,0,1,1,1,0,1,0,1))
W <- c("T","H")[c(1,0,1,1,1,0,1,0,1)+1]
colvec <- c("orange","blue")[c(1,0,1,1,1,0,1,0,1)+1] 
N <- 1:9
k <- 9
ps <- seq(0,1,length.out = 50)
prior <-  rep(1,50)
post <- dbeta(ps,shape1=(y[k]+1),shape2=(N[k]-y[k]+1))

plot(x=ps,post,type="n",
     main="",xlab=expression(theta),ylab="",xaxt="n",yaxt="n")
axis(side = 1, at = c(0,0.5,1),labels = c(0,0.5,1), cex=0.7)
abline(a=1,b=0,lty=3,lwd=2)
text(x=0.1,1.1,col="cornflowerblue",labels="prior",cex=1)

lines(x=ps,post,type="l",ylim=c(0,3),#range(c(prior,post)),
     ylab="",xaxt="n",yaxt="n")
axis(side = 1, at = c(0,0.5,1),labels = c(0,0.5,1), cex=0.7)
text(x=0.87,1.7,col="cornflowerblue",labels="posterior",cex=1)
mtext(expression(paste("p(",theta," | ",y,")")),side=2,line=0)
mtext(W,side=3,col=c(colvec[1:(k-1)],rep(grey(0.9),(9-(k-1)))),
        at=seq(0.05,0.95,length.out = 9),cex=1)

@
\caption{Uniform prior and posterior for $\btheta$ using $n=9$}
\label{fig:prpostalltheta}
\end{figure}


\eit
\eexa



\pagebreak

\section{Ingredients and Recipes for Bayesian Inference}


\subsection{Some Notation}

\bdes 
\item[Parameter:] $\theta$ quantifies unknown population characteristics (could be multivariate)

\item[Data:] $y$ represents the outcomes (these are values) of a survey or experiment

\item[Parameter Space:] $\Theta$ is the set of all possible values of $\theta$

\item[Sample space:] $\mathcal{Y}$ is the set of all possible datasets.
\edes 

\subsection{The Ingredients (a summary)}

\bdes 
\item[Prior:] $p(\theta)$, for all $\theta\in\Theta$.

\item[Sampling/generating model:] $p(y \given \theta)$ defined for $\theta\in\Theta$ and $y\in \mathcal{Y}$ (recall, if viewed as a function of $\theta$ this is called the likelihood function)

\item[Bayes' Rule and Posterior:] After obtaining data $y$, Bayes' rule tells us how to update our prior beliefs with the observed data through the posterior $$p(\theta \given y)\propto p(y \given \theta)p(\theta)$$

\edes


\subsection{Some Recipes}


While the goal is often to use Bayes' Theorem to derive the posterior distribution $p(\theta\given y)$, there are other important mathematical objects that can be obtained to draw inferences and make decisions using the Bayesian machinery. Below we refer to the data as $y$ to ease the notation, but this data may consist of many points (i.e., $y=y_{1:n}=(y_1,\ldots,y_n)$). Some of these objects are 

$$\bmat\text{marginal likelihood} & p(y)\\
\text{posterior predictive} & p(y^\star\given y)\\
\text{loss function} & \ell(s,a)\\
\text{posterior expected loss} & \rho(a,x)\\
\text{risk / frequentist risk} & R(\theta,\delta)\\
\text{integrated risk} & r(\delta)
\emat$$

\subsubsection{Marginal Likelihood}

The {\bemph {marginal likelihood}} is the resulting pmf/pdf after integrating out (marginalzing) the parameters $\theta$.  It can understood as the \emph{expected} likelihood with respect to the prior $p(\theta)$.  It is given by 
\bean
p(y)&=&\int_{\theta\in\Theta}p(y \given \theta)p(\theta)d\theta \;=\;E_{p(\theta)}\lrb{p(y \given \theta)}.
\eean
When $\theta$ is a vector this will be a multidimensional integral, and in some cases it can be REALLY hard to obtain.  Notice that this is the denominator of the posterior density, which is why it so convenient being able to work with the posterior up to proportionality.

\bexa {\bemph{Marginal obtained from the Beta-Bernoulli model}}

As in the coin-flipping problem, if $\btheta\sim \text{Beta}(a,b)$ and $Y_1,\ldots,Y_n$ are iid $\text{Bernoulli}(\theta)$, then the marginal likelihood is given by
\bean
p(y)\;=\;p(y_{1:n})&=&\int_{\theta\in\Theta}p(y_{1:n} \given \theta)p(\theta)d\theta\\
&=&\int_{\theta\in\Theta}\theta^{\sum y_i}(1-\theta)^{n-\sum y_i}  \;\frac{1}{B(a,b)} p^{a-1}(1-p)^{b-1} d\theta\\
&=& \frac{B(a+\sum y_i, b + n - \sum y_i)}{B(a,b)}
\eean


\eexa

\subsubsection{Posterior Predictive Distribution}

To make inference regarding a potential new observation $Y^\star$ given the observed data $Y=Y_{1:n}=y_{1:n}$, we can derive the {\bemph {posterior predictive distribution}}.  In particular if $Y_{1:n}$ and $Y^\star$ are independent given $\theta$, the posterior predictive pdf/pmf is 
\bean
p(y^\star\given y)\;=\; p(y^\star\given y_{1:n})&=&\int_{\theta\in\Theta} p(y^\star,\theta \given y_{1:n}) d\theta\\
&=& \int_{\theta\in\Theta} p(y^\star \given \theta, y_{1:n}) p(\theta \given y_{1:n}) d\theta\\
&=& \int_{\theta\in\Theta} p(y^\star \given \theta) p(\theta \given y_{1:n}) d\theta
\eean

\bexa {\bemph{Posterior predictive obtained from the Beta-Bernoulli model}}

Recall that the posterior resulting for the coin-tossing example with $\btheta\sim\text{Beta}(a,b)$ \emph{a priori} is $\text{Beta}(\theta\given a_n,b_n)$, where $a_n=a+\sum y_i$ and $b_n=b+n-\sum y_i$.  Hence, the posterior predictive density for a new observation $Y^\star$ is
\bean
p(y^\star\given y_{1:n})&=& \int_{\theta\in\Theta} p(y^\star \given \theta) p(\theta \given y_{1:n}) d\theta\\
&=& \int_{\theta\in\Theta} \theta^{y^\star}(1-\theta)^{1-y^\star} \frac{1}{B(a_n,b_n)}\theta^{a_n-1}(1-\theta)^{b_n-1} d\theta\\
&=&  \frac{1}{B(a_n,b_n)} \int_{\theta\in\Theta} \theta^{a_n+y^\star - 1}(1-\theta)^{b_n + 1-y^\star - 1}d\theta\\
&=&  \frac{1}{B(a_n,b_n)}\times B(a_n+y^\star,b_n+1-y^\star)\\
&=&  \frac{\Gamma(a_n+b_n)}{\Gamma(a_n)\Gamma(b_n)}\times \frac{\Gamma(a_n+y^\star)\Gamma(b_n+1-y^\star)}{\Gamma(a_n+y^\star + b_n+1-y^\star)}\\
&&\\
&&\text{then, since $\Gamma(x+1)=x\Gamma(x)$}\\
&&\\
&=&  \frac{a_n^{y^\star}\; b_n^{1-y^\star}}{a_n + b_n}\;\1_{\lrb{y^\star\in\lrb{0,1}}}.
\eean
Note that the above expression implies that $Y^\star\given Y_{1:n}=y_{1:n}\sim \text{Bernoulli}(\theta^\star)$, where $$\theta^\star =\frac{a_n}{a_n+b_n}$$
\eexa

\subsubsection{Decision Theoretic Objects}

In a few pages we go into some detail regarding the Bayesian approach to decision theory, there we will introduce the remaining recipes (i.e., loss ($\ell(s,a)$ and $\rho(a,x)$) and risk ($R(\theta,\delta)$ and $r(\delta)$) functions), so we skip those for now.


\section{Example: Estimating the Probability of a Rare Event}

This example comes straight from Hoff's book, but let's work through the details.  We are interested in estimating the prevalence of an infectious disease (i.e., the proportion of individuals infected) in a small city.  If the prevalence is high, more precautions would have to be set in place.  If a random sample of 20 individuals is collected, the problem can be set up as follows:

\begin{center}
\begin{tabular}{r | c}
Parameter Space & $\Theta = [0,1]$\\
Sample Space & $\mathcal{Y} = \lrb{0,1,2,\ldots,20}$\\
Generating Distribution & $Y\sim\text{Binomial}(20,\theta)$
\end{tabular}
\end{center}

All left to do is specify a sensible prior for the problem at hand. We are told that in comparable cities this disease's prevalence ranges between 0.05 and 0.2, with a mean of approximately 0.1.  We can certainly use this information to tune the prior.  As you may now imagine, a sensible choice is to go with a Beta$(a,b)$ distribution.  We need to choose suitable parameters $a$ and $b$ such that the distribution reflects the prior information available.

Let's suppose that we want the prior distribution to:
\bit
\item have about 70\% of its mass between 0.05 and 0.2, and
\item have a mean of 0.1. 
\eit

We can achieve this by finding the values of $a$ and $b$ that make the beta distribution as close to meeting these requirements as possible.  To do so, it is useful to recognize that we can parameterize the beta distribution in terms of its mean and a scaling factor (we will interpret this factor later in the analysis).  In particular, if $\btheta\sim\text{Beta}(a,b)$, then $E(\btheta)=\theta_0=a/(a+b)$.  Thus, denoting $\omega=(a+b)$ we have that: $$a = \omega\,\theta_0\text{ and }b = \omega\,(1-\theta_0),$$
where $\omega$ is the scaling factor mentioned above.  Letting $\omega=(a+b)$, for $\theta_0=0.1$ (as required) all we need to do is find the value of $\omega$ that yields the beta distribution that places 70\% of its mass between 0.05 and 0.2.  We may use the following code to obtain an approximate value for $\omega$:

<<echo=T>>=
#define function to minimize
get.ab <- function(w){
    #w is the strength of the prior in terms of the information it contains
    a <- w*0.1
    b <- w*(1-0.1)

    #mass between 0.05 and 0.2
    themass <- (pbeta(0.2,a,b)-pbeta(0.05,a,b))
    #difference between 0.7 and the observed mass of the distribution
    abs(themass - 0.7)
}


#minimize the function get.ab numerically to obtain a value for omega
res.optim <- optim(par=1,fn=get.ab,lower=0,upper=100,method= "L-BFGS-B")

#just checkin'
omega <- res.optim$par
a <- omega*0.1;  b <- omega*(1-0.1)
pbeta(0.2,a,b)-pbeta(0.05,a,b)
@

In this case $\omega =\Sexpr{round(omega,2)}$, which implies that a prior that closely represents our prior beliefs for $\btheta$ would be a $\text{Beta}(a=\Sexpr{round(a,0)},b=\Sexpr{round(b,0)})$.  Now, we can turn the crank on the Bayesian machinery and get the posterior.
\bsh
\bean
p(\theta\given y) &\propto& p(y\given \theta) p(\theta)\\
&=&  {20 \choose y}\theta^y (1-\theta)^{20-y}\frac{1}{B(2,20)} \theta^{2-1} (1-\theta)^{20-1}\\
&\propto& \theta^{2+y-1} (1-\theta)^{20+(20-y)-1}\\
&\propto& \text{Beta}\lrp{2+y,20+(20-y)}
\eean
%\vspace{3cm}
\esh

Before, when discussing the coin-tossing problem we mentioned that when $n$ is small it is not unlikely for all observations to be equal to 0, and this situation exacerbates when $\theta$ is actually close to 0.  Just considering the likelihood, note that
\bean
\Pr(Y=0\given\theta=0.05)&\approx&0.36\\
\Pr(Y=0\given\theta=0.10)&\approx&0.12\\
\Pr(Y=0\given\theta=0.20)&\approx&0.01,
\eean
so in the first two cases, having 0 infection cases is a likely event.

The likelihood for different values of $\theta$ along with the prior and posterior (for the case when $Y=0$) are shown in Figure \ref{fig:prevdists}.

\begin{figure}[h]
<<echo=F, message=F, warning=F, fig.width=9, fig.height=4, fig.align='center'>>=
#define the Beta(2,20) prior
a<-2 ; b<-20

par(mar=c(3,3,2,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))

n<-20
x<-0:n
del<-.25

#plot empty graph device (using the option type="n")
plot( x=range(x-del), y=c(0,.4),xlab="number infected in the sample",
      ylab="probability",type="n",main=expression(p(paste(y,"|",theta))))

#add barplots for theta=0.05, 0.1 and 0.2
points( x-del,dbinom(x,n,.05),type="h",col=gray(.75),lwd=3)
points( x,dbinom(x,n,.10),type="h",col=gray(.5),lwd=3)
points( x+del,dbinom(x,n,.20),type="h",col=gray(0),lwd=3)
legend(10,.35,legend=c(
  expression(paste(theta,"=0.05",sep="")),
  expression(paste(theta,"=0.10",sep="")),
  expression(paste(theta,"=0.20",sep="")) ),
  lwd=c(3,3,3),
  col=gray(c(.75,.5,0)) ,bty="n")


#plot prior and posterior densities
a<-2 ; b<-20
y<-0 ; n<-20


theta<-seq(0,1,length=500)
plot(theta, dbeta(theta,a+y,b+n-y),
     type="l",
     xlab="percentage infected in the population",
     ylab="", lwd=2, ylim=c(0,16),
     main=expression(paste(p(theta)," and ",p,"(",theta,"|",y,")"))
)
lines(theta, dbeta(theta,a,b),col="gray",lwd=2)
legend(.5,14,legend=c( expression(paste(italic("p"),"(",theta,")",sep="")),
                       expression(paste(italic("p"),"(",theta,"|",italic("Y")==0,")",sep=""))  ),
       bty="n", lwd=c(2,2),col=c("gray","black"))
@
\caption{Generating distribution Binom$(20,\theta)$ for $\theta=0.05,0.1,0.2$ (left). Prior and posterior pdf's for $\theta$ (right)}
\label{fig:prevdists}
\end{figure}

\subsection*{Sensitivity Analysis}

Since (for general $n$ with a Beta(a,b) prior) $\btheta\sim\text{Beta}(a+y,b+(n-y))$, its posterior mean is
\bsh
\bean
E(\btheta\given Y=y)&=&\frac{a+y}{(a+y)+(b+n-y)}\\
&=&\frac{a+y}{(a+y)+(b+n-y)}\\
&=&\frac{1}{(a+b+n)}a+\frac{1}{(a+b+n)}y\\
&=&\frac{a+b}{(a+b+n)}\lrp{\frac{a}{a+b}}+\frac{n}{(a+b+n)}\lrp{\frac{y}{n}}\\
&=&\textcolor{blue}{\frac{\omega}{(\omega+n)}\theta_0+\frac{n}{(\omega+n)}\bar{y}},
\eean
where $\omega=(a+b)$, $\theta_0=E(\btheta)$ and $\bar{y}=y/n$.
%\vspace{3cm}
\esh

Consider the form of the final expression above: {\bemph{it's a weighted average of the prior mean $\theta_0$ and the sample mean $\bar{y}$}}, where the sample mean's weight is the number of observations $n$ and the weight for the prior mean is $\omega$.  Hence,  $\omega$ determines the relative strength of the prior information.

For the assumed prior Beta$(2,20)$, we have that $\omega=(2+20)=22$, indicating that the prior we used is in fact highly informative, since we are giving the prior slightly more importance than the data when $n=20$. 

\begin{figure}[h]
<<echo=F, message=F, warning=F, fig.width=9, fig.height=4, fig.align='center'>>=
par(mar=c(3,3,2,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))
g<-50
th0<-seq(0.01,.5,length=g)
nu0<-seq(1,25,length=g)

PP10<-PM<-PLQ<-PUQ<-matrix(0,g,g)
for(i in 1:g) {for(j in 1:g) {
  a<-nu0[i]*th0[j]
  b<-nu0[i]*(1-th0[j])

  PM[i,j]<- (a+y)/(a+y+b+n-y) #posterior mean
  PP10[i,j]<- pbeta(.10,a+y,b+n-y)  #P(theta<0.1 | y = 0)
  PLQ[i,j]<- qbeta(.05,a+y,b+n-y)   #P(theta<0.05 | y = 0)
  PUQ[i,j]<- qbeta(.95,a+y,b+n-y)   #P(theta<0.95 | y = 0)
}}

contour(nu0,th0,PM,xlab=expression(italic(w)), ylab=expression(theta[0]),
        main=expression(E(paste(theta,"|",Y==0))) )
contour(nu0,th0,PP10,xlab=expression(italic(w)),
        levels=c(0.1,0.3,.5,.70,.90,.975),
        main=expression(P(paste(theta<0.1,"|",Y==0))))
@
\caption{Sensitivity to the prior (in terms of $\omega$ and $\theta_0$) when $Y=0$: $E(\btheta\given Y=0)$ (right) and $\Pr(\btheta<0.10\given Y=0)$ (left).}
\label{fig:sensprior}
\end{figure}

Given that we can parameterize the prior in terms of the prior mean $\theta_0$ and $\omega$, such that the prior
$$\text{Beta}(a,b)=\text{Beta}(\omega\theta_0,\omega(1-\theta_0),$$
we can study how the posterior information changes as we vary the prior (changing the strength of our beliefs $\omega$ and the prior mean $\theta_0$).

In particular, let's consider the situation when there are no infections (i.e., $Y=0$), depicted in Figure \ref{fig:sensprior}.  After observing $Y=0$ infections in our sample of $n=20$, a decision maker can use the plot on the right to assess under different prior alternatives if it might be necessary to recommend vaccination.  This could be the case, if for instance there was reasonable certainty (a high posterior probability) that the infection rate was more than 10\%. 

\subsection*{Bayes and Non-Bayes}

The frequentist approach to provide a parameter estimate with a measure of uncertainty are confidence intervals.  For the problem at hand the typical estimate for $\theta$ is usually $\bar{y}$ (i.e., the sample proportion), and recalling that if $Y\sim\text{Binom}(n,\theta)$ we have that
$$\var{\bar{Y}}=\var{Y/n}=n\theta(1-\theta)/n^2=\theta(1-\theta)/n,$$
which is estimated with $\bar{y}(1-\bar{y})/n$. Thus the Wald 95\% confidence interval for $\theta$ is
$$\bar{y}\pm 1.96 \sqrt{\bar{y}(1-\bar{y})/n}.$$

The problem with this confidence interval is that it is not very reliable if $n$ is small. Furthermore, whenever $\bar{y}=0$, then both limits of the confidence interval are 0, i.e., it's actually a point.  A fix to this was proposed by Agresti and Coull (1998), where $\bar{y}$ is replaced by $\hat{\theta}=\frac{4}{n+4}\frac{1}{2}+\frac{n}{n+4}\bar{y},$ and the adjusted confidence interval is
$$\hat{\theta}\pm 1.96 \sqrt{\hat{\theta}(1-\hat{\theta})/n}$$

Does $\hat{\theta}$ look familiar? It should, this is the posterior expectation when $\theta$ has a $\text{Beta}(2,2)$ prior, which is weakly informative (in this case with $\omega=4$) and centered about $\theta_0=0.5$. Interestingly, Agresti and Coull derived this interval from a slightly different point of view (following the work of Wilson (1927)).
% 
% %\section{Why Bayes}
% 


\section{Decisions, decisions, decisions}

The goal of decision theory is to use the information available to determine what the costs or utilities associated with different decisions are; this provides guidance regarding what the ``optimal'' course of action could be. From the Bayesian standpoint, the optimal decision is the one that minimizes the {\bemph{posterior expected loss function}}.

A few pages back we introduced without further explanation some loss and risk functions.  In this section we make these functions precise.  But first let's start with some notation:
\begin{center}
\begin{tabular}{c l}
$S$ & state (unknown, usually what we do inference on)\\
$y$ & observation (known)\\
$a$ & action (usually what we estimate it with)\\
$\ell(s,a)$ & loss function
\end{tabular}
\end{center}

\subsection{Bayesian Decision Theory: a primer}

\bit 
\item The \emph{state} $S$ from the Bayesian standpoint is a random variable, and the distribution that gives rise (the generator distribution) to our data $y$ depends on $S$ (to give some context, in the problems above $S=\btheta$).   
\bsh
Here, the optimal decision is to choose an action $a$ that minimizes the {\bemph{posterior expected loss}}, which is given by
$$\rho(a,y)\;=\;E(\ell(S,a)\given y)\;=\;\left\{\bmat \sum_s \ell(s,a) p(s\given y)&\text{if $S$ discrete}\\
&&\\
\int_{s\in\mathcal{S}} \ell(s,a) p(s\given y) ds &\text{if $S$ continuous}\emat\right.$$
\esh

\item {\bemph{A decision procedure}} $\delta$ is a mechanism to determine actions ``$a$'' based on data ``$y$''. Often, this is a deterministic function $a = \delta(y)$ (but sometimes introducing some randomness into a can be useful).

\item A {\bemph{Bayes decision procedure}} is one that finds the action ``$a$'' that minimizes the posterior expected loss $\rho(a, y)$, for each $y$.

\eit

\bexa {\bf An estimate for $\theta$ under quadratic loss}

\bit
\item Consider the situation where we generate data $y$ from $p(y\given \theta)$, with $\theta\sim p(\theta)$ a priori. Here
\begin{center}
\begin{tabular}{l l}
State: & $S=\btheta$\\
Observation & $y=y_{1:n}$\\
Action: & $a=\hat{\theta}$\\
Loss function: & $\ell(\theta,\hat{\theta})=(\theta-\hat{\theta})^2$ (quadratic loss)
\end{tabular}
\end{center}

\item Notice that since $(\theta-\hat{\theta})^2=\theta^2-2\theta\, \hat{\theta} + \hat{\theta}^2$, then
\bean
\rho(\hat{\theta},y_{1:n}) &=& E(\ell(\btheta,\hat{\theta})\given y_{1:n})\\
&=& E(\btheta^2\given y_{1:n})-2E(\btheta\given y_{1:n}) \hat{\theta} + \hat{\theta}^2
\eean
Because this a quadratic function in $\hat{\theta}$, it is convex and therefore minimized at the value where it's first derivative equals 0:
\bean
\frac{d}{d\hat{\theta}}\rho(\hat{\theta},y_{1:n}) &=& -2E(\btheta\given y_{1:n})  + 2\hat{\theta} \;=\;0\\
\Longrightarrow \hat{\theta} &=& \delta(y_{1:n}) \; = \;  E(\btheta\given y_{1:n})
\eean

\item So the posterior mean for $\btheta$ is an optimal Bayes decision procedure under quadratic loss, very nice!!!

\eit

\eexa

\bexa {\bf Predicting a new observation using 0--1 loss}

\bit
\item Say $Y^\star$ denotes the discrete random variable that represents possible oucomes for new observation from $p(y\given \theta)$, with $\theta\sim p(\theta)$ a priori. Here
\begin{center}
\begin{tabular}{l l}
State: & $S=Y^{\star}$\\
Observation & $y=y_{1:n}$\\
Action: & $a=\hat{y}^\star$\\
Loss function: & $\ell(\theta,\hat{\theta})=\1_{\lrb{s\not=a}}$ (0--1 loss)
\end{tabular}
\end{center}

\item Here the appropriate distribution is the posterior predictive distribution, such that
\bean
\rho(\hat{y}^\star,y_{1:n}) &=& E(\ell(Y^\star,\hat{y}^\star)\given y_{1:n})\\
&=& E(\1_{\lrb{Y^\star\not=\hat{y}^\star}} \given y_{1:n})\\
&=& \Pr(Y^\star\not=\hat{y}^\star \given y_{1:n})\\
&=& 1-\Pr(Y^\star=\hat{y}^\star \given y_{1:n})
\eean
which is minimized at
\bean
\hat{y}^\star&=&\delta(y_{1:n})\;=\;\underset{y^\star}{\text{arg max}}\quad p(y^\star\given y_{1:n})
\eean

\item So $\hat{y}^\star$ is the \emph{mode} of the posterior predictive distribution, which again, makes perfect sense.

\eit
\eexa


\bexa \bsh {\bf Disease prevalence revisited}

Let's suppose that in our small imaginary city, public health officials need to decide the amount of resources to allocate towards prevention and treatment of the disease we are concerned with, with the fraction of infected individuals $\theta$ still unknown.  They will decide on the resources needed based on a fraction $c$ of the population. If $c$ is chosen too large, there will be wasted resources, while if it is too small, preventable cases may occur and some individuals may go untreated. After some deliberation, they tentatively adopt the following loss function:
$$\ell(\theta,c)=\left\{\bmat |\theta-c|&\text{if }c\geq \theta \\ 10|\theta-c|&\text{if }c< \theta \emat \right.$$

Recall that, based on the available information, we assumed that $\btheta\sim \text{Beta}(2,20)$ a priori. Using this prior, we had that $\btheta\given y \sim \text{Beta}(a_n=2+y,b_n=20+n-y)$, such that the \emph{posterior expected loss} for this problem is 

\bean \rho(c,y)\quad = \quad E(\ell(\btheta,c)\given y)&=&\int_0^1 \ell(\theta,c)\frac{1}{B(a_n,b_n)}\theta^{a_n-1} (1-\theta)^{b_n-1}d\theta\\
&=&\int_0^c |\theta-c|\frac{1}{B(a_n,b_n)}\theta^{a_n-1} (1-\theta)^{b_n-1}d\theta \\
&&{}\qquad\qquad + \int_c^1 10|\theta-c|\frac{1}{B(a_n,b_n)}\theta^{a_n-1} (1-\theta)^{b_n-1}d\theta
\eean
which can be integrated numerically as follows
\esh
<<echo=T,fig.height=5,fig.width=5,fig.align='center'>>=
n <- 20
a <- 2
b <- 20
y <- 0 #assuming y = 0 (play around with y to see how rho changes)

rhofn <- function(c){
  
  ff <- function(theta){
      db <- dbeta(theta,shape1=(a+y),shape2=(b+n-y))
      abs(c-theta) * db * ( (c>=theta) + 10*(c<theta) )
    }
    
    res <- integrate(ff, lower=0, upper=1)
    res$value
}

vrhofn <- Vectorize(rhofn, vectorize.args = "c")
c.values <- seq(0,1,by=0.005)
rho.values <- vrhofn(c.values)

plot(x = c.values,
     y = rho.values,
     xlab = "c",
     ylab = expression(rho(theta,c)),
     type = "l",
     lwd = 2,
     col="orange")

#value of c that minimizes the posterior expected loss
(c.argmin <- c.values[which(rho.values==min(rho.values))])
@
\bsh
As such, we have that $c\approx\Sexpr{round(c.argmin,3)}$ minimizes the posterior expected loss, which is the optimal fraction of the population to be considered for resource allocation accounting for uncertainty in $\theta$, under the prior and loss functions assumed. Note that if we were to let $c$ be the sample average $\bar{y}=\Sexpr{y/n}$ this fraction would be grossly underestimated, thus possibly leading to large losses.


Now, let's compare the outcome from the Bayesian decision procedure to that of choosing $c=\bar{y}$ and $c=0.1$ constant. This comparison can be done by observing the optimal quantity $c$ derived from each of the decision procedures cosnidered while varying the value of $y$ (the number of infected cases).
\esh
<<echo=T,fig.height=5,fig.width=5,fig.align='center'>>=
n <- 20
a <- 2
b <- 20
yvec <- 0:20 
c.fixed <- 0.1
c.argminvec <- NA


for(y in yvec){
  rhofn <- function(c){
    
    ff <- function(theta){
      db <- dbeta(theta,shape1=(a+y),shape2=(b+n-y))
      abs(c-theta) * db * ( (c>=theta) + 10*(c<theta) )
    }
    
    res <- integrate(ff, lower=0, upper=1)
    res$value
  }
  
  Rfn <- function(theta)
  vrhofn <- Vectorize(rhofn, vectorize.args = "c")
  c.values <- seq(0,1,by=0.005)
  rho.values <- vrhofn(c.values)
  #value of c that minimizes the posterior expected loss
  (c.argminvec[y+1] <- c.values[which(rho.values==min(rho.values))])
}

plot(x = yvec,
     y = c.argminvec,
     ylim=c(0,1),
     xlab = "observed number of cases",
     ylab = "optimal c",
     type = "o",
     lwd = 2,
     pch=20,
     col="orange")
lines(x = yvec, y = yvec/n,col="blue",lwd=2)
points(x = yvec, y = yvec/n,col="blue",pch=20)
lines(x = yvec, y = rep(c.fixed,(n+1)),col="darkgreen",lwd=2)
points(x = yvec, y = rep(c.fixed,(n+1)),col="darkgreen",pch=20)
#add legend to plot
legend("topleft",legend=c("Bayes",expression(bar(y)),"c=0.1"),
       col=c("orange","blue","darkgreen"),
       lty=rep(1,3),pch=rep(20,3),bty="n",ncol=1)
@

\eexa

\subsection{Risks}

Now let's talk about the remaining two recipes  Let's assume a decision problem where $S=\btheta$.

\bit 
\item The {\bemph{risk}} (aka {\bemph{frequentist risk}}) associated with a decision procedure $\delta$ is
\bean
R(\theta,\delta)&=& E(\ell(\btheta,\delta(Y))\given \btheta=\theta)\\
&=& \int\ell(\theta,\delta(y))p(y\given \theta) dy
\eean
where $Y\sim p(y\given \theta)$ is continuous (the integral is replaced with a sum if $Y$ is discrete).  Notice that here the loss function is integrated with respect to the distribution of the data (the generating distribution / likelihood).


\item The frequentist risk gives a useful approach to compare decision procedures without having to choose a prior.  It averages the loss function over all possible datasets under the assumed likelihood.

\item Finally, the {\bemph{integrated risk}} associated with $\delta$ takes the frequentist risk $R(\theta,\delta)$ and integrates it over all possible values of $\theta\sim p(\theta)$ (i.e., with respect to the prior), namely
\bean
r(\delta)&=&E(R(\theta,\delta))\;=\; \int R(\theta,\delta) p(\theta) d\theta.
\eean

\eit

% \pagebreak
% 
% Recall that our goal is to make inference about $\theta$ given $y$. To do so, we use \emph{Statistical Induction}
% 
% \bdes
% \item[Induction:] Reasoning from specific cases to a general principle.
% \item[Statistical induction:]  Using a data sample to infer population characteristics.
% \item[Bayesian Inference:] Draw inferences about $\theta$ given data $Y_{1:n}$.
% \edes
% 
% 
% 
% \bdes
% \item[Pre-experimentally:] Recall that $Y_{1:n}$ are random variables, therefore before running the experiment the data $Y_{1:n}=(Y_1,\ldots,Y_n)$ (a realization of $Y_{1:n}$) are unknown. So is the parameter $\theta$
% 
% \item[Post-experimentally:] the values $Y_{1:n}$ are known, but $\theta$ remains unknown/uncertain.
% 
% \edes



\end{document}