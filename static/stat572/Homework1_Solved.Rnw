\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espaÃ±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{float}

\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbs}[1]{\boldsymbol{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\N}{\mbb{N}_0}
\renewcommand{\d}{\text{d}}
\newcommand{\by}{\mbf{y}}
\newcommand{\mts}{\tilde{Y}}
\newcommand{\mtsv}{\tilde{\bv}}
\newcommand{\btw}{\tilde{\bw}}
\newcommand{\bhw}{\hat{\bw}}
\newcommand{\btx}{\tilde{\bx}}
\newcommand{\pt}{\tilde{p}}
\newcommand{\ba}{\mbf{a}}
\newcommand{\bb}{\mbf{b}}
\newcommand{\bc}{\mbf{c}}
\newcommand{\bd}{\mbf{d}}
\newcommand{\boe}{\mbf{e}}
\newcommand{\bk}{\mbf{k}}
\newcommand{\bq}{\mbf{q}}
\newcommand{\br}{\mbf{r}}
\newcommand{\bs}{\mbf{s}}
\newcommand{\bh}{\mbf{h}}
\newcommand{\bff}{\mbf{f}}
\newcommand{\bt}{\mbf{t}}
\newcommand{\bu}{\mbf{u}}
\newcommand{\bm}{\mbf{m}}
\newcommand{\bv}{\mbf{v}}
\newcommand{\bx}{\mbf{x}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\bz}{\mbf{z}}
\newcommand{\tby}{\tilde{\mbf{y}}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\bp}{\mbs{p}}
\newcommand{\bA}{\mbf{A}}
\newcommand{\tbA}{\tilde{\bA}}
\newcommand{\bB}{\mbf{B}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\tc}{\tilde{c}}
\newcommand{\tC}{\tilde{C}}
\newcommand{\tbC}{\tilde{\bC}}
\newcommand{\bF}{\mbf{F}}
%\newcommand{\bBs}{\mbf{B}^\star}
\newcommand{\bD}{\mbf{D}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bK}{\mbf{K}}
\newcommand{\bQ}{\mbf{Q}}
\newcommand{\bV}{\mbf{V}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\hN}{\hat{N}}
\newcommand{\tbc}{\tilde{\mbf{c}}}
\newcommand{\tba}{\tilde{\mbf{a}}}
\newcommand{\tbX}{\tilde{\mbf{X}}}
\newcommand{\tbW}{\tilde{\mbf{W}}}
\newcommand{\bSigma}{\mbs{\Sigma}}
\newcommand{\bGamma}{\mbs{\Gamma}}
\newcommand{\bUps}{\mbs{\Upsilon}}
\newcommand{\bPsi}{\mbs{\Psi}}
\newcommand{\vs}{v^\star}
\newcommand{\Vs}{V^\star}
\newcommand{\bvs}{\bv_i^\star}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bP}{\mbf{P}}
\newcommand{\bBs}{\bB^\star}
\newcommand{\bXs}{\bX^\star}
\newcommand{\bxs}{\bx^\star}
\newcommand{\bWs}{\bW^\star}
\newcommand{\bws}{\bw_i^\star}
\newcommand{\bwsp}{\left.\bw_i^\star\right.^\prime}
\newcommand{\bBsp}{\left.\bB^\star\right.^\prime}
\newcommand{\bVs}{\bV^\star}
\newcommand{\bpsi}{\mbs{\psi}}
\newcommand{\bphi}{\mbs{\phi}}
\newcommand{\bmu}{\mbs{\mu}}
\newcommand{\bbeta}{\mbs{\beta}}
\newcommand{\bxi}{\mbs{\xi}}
\newcommand{\bchi}{\mbs{\chi}}
\newcommand{\blambda}{\mbs{\lambda}}
\newcommand{\bLambda}{\mbs{\Lambda}}
\newcommand{\LamT}{\tilde{\Lambda}}
\newcommand{\GamT}{\tilde{\Gamma}}
\newcommand{\WT}{\tilde{W}}
\newcommand{\balpha}{{\mbs{\alpha}}}
\newcommand{\bepsilon}{{\mbs{\varepsilon}}}
\newcommand{\bgamma}{{\mbs{\gamma}}}
\newcommand{\btheta}{{\mbs{\theta}}}
\newcommand{\bseta}{{\mbs{\eta}}}
\newcommand{\bpi}{{\mbs{\pi}}}
\newcommand{\bI}{\mbf{I}}
\newcommand{\bH}{\mbf{H}}
\newcommand{\tbH}{\tilde{\mbf{H}}}
\newcommand{\1}{\mbs{1}}
\newcommand{\0}{\mbs{0}}
\newcommand{\detstar}[1]{\text{det}^+\left(#1\right)}
\renewcommand{\det}[1]{\text{det}\left(#1\right)}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\renewcommand{\exp}[1]{\text{exp}\left[#1\right]}
\newcommand{\M}{{M}}
\newcommand{\K}{{K}}
\newcommand{\peq}{{p}}
\newcommand{\MB}{{M_B}}
\newcommand{\MF}{{M_F}}
\newcommand{\MT}{{M_T}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\graph}{\Gamma}
\newcommand{\kset}{\Upsilon}
\newcommand{\order}{order}
\newcommand{\parents}{\mcal{P}}
\newcommand{\children}{\mcal{C}}
\newcommand{\extreme}{\mcal{E}}
\newcommand{\combined}{\mcal{A}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left\{#1\right\}}
\newcommand{\lrno}[1]{\left.#1\right.}
\newcommand{\lrsqb}[1]{\left[#1\right]}
\newcommand{\G}[1]{\Gamma_{#1}}
\renewcommand{\d}{\text{d}}
\newcommand{\Ps}[1]{\Pr{\left(#1\right)}}
\newcommand{\BF}[2]{{BF}_{#1,#2}(Y)}
\newcommand{\BFd}[3]{{BF}_{#1,#2}(Y,#3)}
\newcommand{\xmark}{\ding{55}}
\newcommand{\ben}{\begin{equation*}}
\newcommand{\een}{\end{equation*}}
\newcommand{\bean}{\begin{eqnarray*}}
\newcommand{\eean}{\end{eqnarray*}}
\newcommand{\bsm}{\begin{smallmatrix}}
\newcommand{\esm}{\end{smallmatrix}}
\newcommand{\bmat}{\begin{matrix}}
\newcommand{\emat}{\end{matrix}}
\newcommand{\tI}{\text{I}}
\newcommand{\tN}{\text{N}}
\newcommand{\trN}{\text{trunc.N}}
\newcommand{\nl}[1]{\text{log}{\lrp{#1}}}
\newcommand{\e}[1]{\text{exp}{\lrb{#1}}}
\newcommand{\indf}[1]{\tI_{\left\{#1\right\}}}
\newcommand{\parent}{\mcal{P}}
\newcommand{\gp}{\text{GP}{\lrp{\0,\mcal{C}(\cdot\given\bphi)}}}
\newcommand{\gpd}[3]{\text{GP}_{#1}{\lrp{\0,\mcal{C}_{#2}(\cdot\given\phi_{#3})}}}
\newcommand{\mnngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mbs{\mcal{C}}}_{#2}(\cdot,\cdot;\bphi_{#3})}}}
\newcommand{\nngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mcal{C}}_{#2}(\cdot,\cdot;\phi_{#3})}}}
\newcommand{\nngpw}[4]{\text{NNGP}_{#1}^{#2}{\lrp{0,\tilde{\mcal{C}}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpw}[4]{\text{GP}_{#1}^{#2}{\lrp{0,\mcal{C}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpq}{\text{GP}_q{\lrp{\0,\mbs{\mcal{C}}(\cdot\given\bphi)}}}
\newcommand{\gpk}{\text{GP}{\lrp{0,\mcal{C}(\cdot\given\tilde{\phi}_k)}}}
\newcommand{\nngp}{\text{NNGP}{\lrp{\0,\tilde{\mcal{C}}(\cdot\given\phi)}}}
\newcommand{\refset}{\mcal{T}}
\newcommand{\uset}{\mcal{U}}
\newcommand{\oset}{\mcal{T}}
\newcommand{\Xall}{\mbb{X}}
\newcommand{\given}{\,|\,}
\newcommand{\dtr}[1]{\textcolor{blue}{(#1)}}
\newcommand{\bemph}[1]{\bf \emph{#1}}
\newcommand{\var}[1]{\text{var}{(#1)}}

%-------------------------------
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{exa}{Example}
\newtheorem{note}{Note}
\newcommand{\bex}{\begin{exer}}
\newcommand{\eex}{\end{exer}}
\newcommand{\bexa}{\begin{exa}}
\newcommand{\eexa}{\end{exa}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\bdes}{\begin{description}}
\newcommand{\edes}{\end{description}}

\newcommand{\bsh}{\begin{shaded}}
\newcommand{\esh}{\end{shaded}}
%-------------------------------

%-------------------------------
%hiding proof solutions
%-------------------------------
\newif\ifhideproofs
\hideproofstrue %uncomment to hide proofs

\ifhideproofs
\usepackage{environ}
\NewEnviron{hide}{}
\let\proof\hide
\let\endproof\endhide
\fi
%-------------------------------


\begin{document}
% \SweaveOpts{concordance=TRUE}


\setcounter{section}{0}
\title{%
STAT 572: Bayesian Statistics\\
Homework 1 \\ {\small (submit online in D2L on Monday October 14 before 5:00 pm)}}

\author{}

\date{}							% Activate to display a given date or no date

\maketitle

\thispagestyle{empty}


\benum
% \item We write $X \sim \text{Poisson}(\theta)$ if $X$ has a Poisson distribution with rate $\theta > 0$, that is, its pmf is
% $$p(x|\theta) = \text{Poisson}(x\given \theta) = e^{-\theta}\theta^x/x!$$
% for $x \in \lrb{0,1,2,\ldots}$ (and is 0 otherwise). Suppose $X_1,\ldots,X_n \sim \text{Poisson}(\theta)$ iid given $\theta$, and
% your prior is 
% $$p(\theta) = \text{Gamma}(\theta|a, b) = \frac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b \theta}\1_{\lrb{\theta > 0}}.$$
%  
% What is the posterior distribution for $\theta$?
% 
% \bsh
% Denote by $x_{1:n}=(x_1,x_2,\ldots,x_n)$, then
% \bean
% p(\theta\given x_{1:n}) &\propto& p(x_{1:n}\given \theta)p(\theta)\\
% &=& \lrp{ \prod_{i=1}^n e^{-\theta}\theta^{x_i}/x_i!} \lrp{\frac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b \theta}\1_{\lrb{\theta > 0}}}\\
% &\propto&  e^{- n \theta}\theta^{\sum x_i}\theta^{a-1}e^{-b \theta}\1_{\lrb{\theta > 0}}\\
% &\propto&  \theta^{\sum x_i + a-1} e^{- (n+b) \theta} \1_{\lrb{\theta > 0}}\\
% &\Longrightarrow&  \theta\given x_{1:n}\sim \text{Gamma}\lrp{\sum x_i + a, n+b}
% \eean
% \esh
% \item Let $\theta$ denote the probability of a given coin landing on heads, and say we want to test the hypotheses $$H_0: \theta=1/2\; \text{vs}\; H_1:\theta>1/2.$$ After tossing the coin $n=6$ times, suppose that we observe the sequence $H, H, H, H, H, T.$.
% 
% \benum
% \item  Let the random variable $Y$ count the number of heads after $n=6$ tosses, specify its distribution.  From the particular realization of the experiment shown above, we observe $y=5$. Now, use a frequentist approach to test the hypothesis at a significance level of $\alpha=0.05$ (use an exact test, not the normal approximation). Hint: find the p-value
% 
% \item  Now, define the random variable $X$ (identify it's generating distribution) that counts the number of tosses needed until the first ``T'' comes up.  Use a frequentist approach with this random variable to test the hypothesis at a significance level of $\alpha=0.05$ (use an exact test).
% 
% \item  Comment on the results obtained in parts (a) and (b).  Can you reconcile them?
% 
% \item Use the posterior distribution derived in the class (Foundations classnotes, page in the Beta-Bernoulli section
% \eenum

\item We write $Y \sim \text{Exp}(\theta)$ to indicate that $Y$ has the Exponential distribution, that is, its p.d.f. is $$p(y\given \theta) = \text{Exp}(y\given \theta) = \theta e^{-\theta y}\1_{\lrb{y > 0}}.$$

The Exponential distribution has some special properties that make it a good model for
certain applications. It has been used to model the time between events (such as neuron spikes, website hits, neutrinos captured in a detector), extreme values such as maximum daily rainfall over a period of one year, or the amount of time until a product fails (lightbulbs are a standard example).

Suppose you have data $y_1,\ldots ,y_n$ which you are modeling as iid observations from an Exponential distribution, and suppose that your prior is $\theta \sim \text{Gamma}(a, b)$, defined as in the previous question.

\benum
\item Derive the formula for the posterior density, $p(\theta \given y_{1:n})$. Give the form of the posterior in terms of one of the following distributions: Bernoulli, Beta, Exponential, and Gamma.

\bsh
Denote by $y_{1:n}=(y_1,y_2,\ldots,y_n)$, then
\bean
p(\theta\given y_{1:n}) &\propto& p(y_{1:n}\given \theta)p(\theta)\\
&=& \lrp{ \prod_{i=1}^n \theta e^{-\theta y_i}\1_{\lrb{y_i > 0}}} \lrp{\frac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b \theta}\1_{\lrb{\theta > 0}}}\\
&\propto&  \theta^n e^{- \theta \sum y_i} \theta^{a-1}e^{-b \theta}\1_{\lrb{\theta > 0}}\\
&\propto&  \theta^{n+a-1} e^{- \theta (\sum y_i + b)}  \1_{\lrb{\theta > 0}}\\
&\Longrightarrow&  \theta\given y_{1:n}\sim \text{Gamma}\lrp{n + a, \sum y_i+b }
\eean
\esh

\item (Use R) Now, suppose you are measuring the number of seconds between lightning strikes during a storm, your prior is Gamma$(0.1, 1.0)$, and your data is
$$(y_1,\ldots , y_8) = (20.9, 69.7, 3.6, 21.8, 21.4, 0.4, 6.7, 10.0).$$

Plot the prior and posterior pdf's. (Be sure to make your plots on a scale that allows you to clearly see the important features.)

\begin{figure}[H]
{\scriptsize
<<fig.height=4, fig.width=4, fig.align='center'>>=
y <- c(20.9, 69.7, 3.6, 21.8, 21.4, 0.4, 6.7, 10.0)
n <- length(y)
#prior parameters
a <- 0.1; b <- 1
#posterior parameters
a.n <- n + a; b.n <- sum(y)+b

#plotting
theta.vec <- seq(0.001,0.5, length.out = 500)
plot(x = theta.vec,
     y = dgamma(theta.vec,shape=a.n, rate=b.n),
     xlab = expression(theta),
     ylab = expression(paste("density for ",theta)),
     type="l", lwd = 2, col="blue")
lines(x = theta.vec,
     y = dgamma(theta.vec,shape=a, rate=b),
     lwd = 2,col="black")
legend("topright",lty=rep(1,2),col=c("blue","black"),
       legend=c(expression(paste("p(",theta,"| y)")),
                expression(p(theta))))

@
}
\label{fig:prandpost}\caption{(Part 2.b) Prior and posterior densities for $\theta$ from an exponential likelihood and a Gamma prior.}
\end{figure}

\item Give a specific example of an application where an Exponential model would be reasonable. Give an example where an Exponential model would NOT be appropriate, and explain why.

\bsh
For the exponential distribution to be a reasonable model the memoryless property must make sense. This property is most easily understood with random variables representing ``time to event''.  In such a case, the memoryless property implies that, for a random variable $X\sim\text{Exp}(\theta)$ conditioning on the event $[X>t]$ (for $t>0$), the probability of $[X>t+s]$ is equal to the probability of $[X>s]$.  As such, random variables that represent the time until an event occurrs, where the event actually becomes more likely as more time goes by, do not follow the memoryless property.  One example of this is for instance, if you wear the same pair of shoes every day, the time until your shoe soles get a hole in them is more likely as more time passes by.
\esh

\eenum

\item An ecologist records the number of eggs laid in a sample of sparrow nests of size $n = 20$. Let $Y_i$ be the number of eggs laid in nest $i$ for $i = 1,\ldots,20$. Based on this sample, the ecologist is interested in estimating $\theta$, the mean number of eggs per nest in the general population of nests. Assume $Y_1,\ldots , Yn\given \theta\sim  \text{Poisson}(\theta)$ iid, and also for now that $\theta \in\Theta = \lrb{0.1, 0.2,\ldots , 4.9, 5.0}$ and that $p(\theta) = 1/50$ for each $\theta\in \Theta$.

\benum
\item Let $X=\sum_i Y_i$ denote the random variable that counts the total number of eggs laid in the 20 nests. Using the form of Bayes rule on page 15 of the Hoff book, write down a formula for $p(\theta|x)$ and simplify as much as possible.


\bsh
Recalling that if $Y_i\given \theta \sim \text{Poisson}(\theta)$ and $Y_1,\ldots,Y_n$ are iid, then $X=\sum_i = Y_i \sim \text{Poisson}(n\theta)$ (this can be shown easily using the MGF of Poisson random variables), then the posterior density for $\theta$, having a (discrete) uniform prior over the set $\Theta$, is obtained as shown below.
\bean
p(\theta\given x) &\propto& p(x \given \theta)p(\theta)\\
&=& \lrp{  e^{- n \theta}(n\theta)^{x}/x!} \lrp{\frac{1}{50}\1_{\lrb{\theta \in \Theta}}}\\
&\propto&  e^{- n \theta}\theta^{x} \frac{1}{50}\1_{\lrb{\theta \in \Theta}}\\
\Longrightarrow p(\theta\given x)&=&  \frac{\theta^{x + 1 -1} e^{- n \theta} \frac{1}{50} }{\sum_{{\theta^\star}\in\Theta}{\theta^\star}^{x + 1-1} e^{- n {\theta^\star}}\frac{1}{50}} \1_{\lrb{\theta \in \Theta}}\\
&=&\frac{\theta^{x + 1 -1} e^{- n \theta} }{\sum_{{\theta^\star}\in\Theta}{\theta^\star}^{x + 1-1} e^{- n {\theta^\star}}} \1_{\lrb{\theta \in \Theta}}
\eean
\esh


\item (Use R) The ecologist observes that $x = 36$. Make a plot of $p(\theta\given x)$ versus $\theta$ for $\theta\in \Theta$.

\bsh
Since the numerator is proportional to the \textsf{gamma(shape=1,rate=n)} kernel, then the values obtained using the R function \textsf{dgamma(theta,shape=1,rate=n)} are proportional
to those in the numerator of $p(\theta\given x)$. Once the values in the numerator are derived, the posterior values are obtained by rescaling each of them by the sum over all $\theta\in\Theta$ for the corresponding Gamma$(1,n)$ pdf values (the constants from the Gamma density cancel out in this process).
\esh
\begin{figure}[H]
{\scriptsize
<<fig.height=4, fig.width=4, fig.align='center'>>=
x <- 36
n <- 20
#posterior parameters
a.n <- x + 1; b.n <- n

#plotting
theta.vec <- seq(0.1,5,by = 0.1)

#proportional to posterior numerator
post.theta <- dgamma(theta.vec, shape = a.n, rate = b.n)
#renormalize
post.theta <- post.theta/sum(post.theta)
barplot(names.arg  = theta.vec,
        height = post.theta,
        xlab = expression(theta),
        ylab = expression(paste("p(",theta," | y)")),
        col="blue")
@
}
\label{fig:postpoisg}\caption{(Part 3.b) Probability mass function for $\theta\given X$ with $X\given \theta\sim\text{Poisson}(20\theta)$ and $p(\theta)=\frac{1}{50}\1_{\lrb{\theta\in\Theta}}$ }
\end{figure}

\item (Use R)  Find $E[\btheta\given x]$, the posterior mean of $\btheta$.

\bsh
Below we use the values obtained for the posterior probability of each $\theta\in\Theta$ using the definition of expectation for discrete random variables, that is $$E(\theta\given X=x)=\sum_{\theta\in\Theta} \theta p(\theta\given x).$$
\esh
<<>>=
#E[theta|x]
(E.theta <- sum(theta.vec*post.theta))
@


\item (Use R) Find two numbers, $\theta_l$ and $\theta_h$ such that $\Pr(\theta_l\leq \btheta \leq \theta_h \given x) \approx 0.95$. This is an approximate 95\% posterior confidence interval. Note that there is more than one way of doing this.

\bsh
We may obtain a 95\% Credible Interval (CI) using two different strategies.  The first of them consists of simply obtaining quantiles 2.5 and 97.5 of the posterior distribution. The second alternative is referred to in the literature as the 95\% Highest Posterior Density (HPD) region. To implement this last strategy we can slide a horizontal line from the top or bottom of the posterior pdf/pmf curve given $Y=y$, until we find the values $l(y)$ and $u(y)$ such that $p(l(x)\given X=x)\approx p(u(x)\given X=x)$ and $P(l(x)\leq \btheta \leq u(x) \given X=x)\approx 0.95$. Below we implement both of these strategies.
\esh
<<>>=
#---------------------------------------------------------
#First Strategy: Find quantiles 2.5 and 97.5
#---------------------------------------------------------
(CI95 <- c(q2.5=theta.vec[min(which(cumsum(post.theta)>=0.025))],
           q97.5=theta.vec[min(which(cumsum(post.theta)>=0.975))]))

#Sanity Check CI95, verify:
#P(theta<=q97.5))-P(theta< q2.5) \approx 0.95
#---------------------------------------------------------
#id positions of l(x) and u(x) in theta.vec
pos.lims.CI <- which(theta.vec%in%CI95)
#move position one to the left of lower limit since:
# P(theta \in [l(y),u(y)]) = P(theta <= u(y))- P(theta < l(y))
pos.lims.CI[1] <- pos.lims.CI[1]-1
#get cumulative probs for each limit
cumprobs.lims.CI <- cumsum(post.theta)[pos.lims.CI]
#get the difference
diff(cumprobs.lims.CI)
@

<<>>=
#---------------------------------------------------------
#Second Strategy: Find quantiles HPD95
#---------------------------------------------------------
cumprob <- 0
pmf <- sort(unique(post.theta),decreasing = T)
HPD95 <- range(theta.vec)
th.vals <- NULL
k <- 1
while(cumprob<=0.95){
  th.vals <- theta.vec[post.theta>=pmf[k]]
  HPD95 <- range(th.vals)
  cumprob <- sum(post.theta[post.theta>=pmf[k]])
  k <- k+1
}

#Highest Posterior Density 95% region
names(HPD95) <- c("l(x)","u(x)")
HPD95
@

<<>>=
#Sanity Check HPD, verify:
#P(theta<=u(x)))-P(theta<l(x)) \approx 0.95
#---------------------------------------------------------
#id positions of l(x) and u(x) in theta.vec
pos.lims.HPD <- which(theta.vec%in%HPD95)
#move position one to the left of lower limit since:
# P(theta \in [l(y),u(y)]) = P(theta <= u(y))- P(theta < l(y))
pos.lims.HPD[1] <- pos.lims.HPD[1]-1
#get cumulative probs for each limit
cumprobs.lims.HPD <- cumsum(post.theta)[pos.lims.HPD]
#get the difference
diff(cumprobs.lims.HPD)

@


\item (Use R) Remake the plot in part (b) but with $n = 40$ and $x = 72$. Describe and explain the differences you see between this plot and the one in (b).
\eenum

\begin{figure}[H]
{\scriptsize
<<fig.height=4, fig.width=8, fig.align='center'>>=
x <- 72
n <- 40
#posterior parameters
a.n2 <- x + 1; b.n2 <- n

#plotting

#get posterior numerator
post.theta2 <- dgamma(theta.vec, shape = a.n2, rate = b.n2)
#renormalize
post.theta2 <- post.theta2/sum(post.theta2)
par(mfrow=c(1,2))
barplot(names.arg  = theta.vec,
        height = post.theta,
        xlab = expression(theta),
        ylab = expression(paste("p(",theta," | y)")),
        col="blue",
        main=expression(paste("p(",theta,"|",X[20]==36,")")))
barplot(names.arg  = theta.vec,
        height = post.theta2,
        xlab = expression(theta),
        ylab = expression(paste("p(",theta," | y)")),
        col="blue",
        main=expression(paste("p(",theta,"|",X[40]==72,")")))
@
}
\label{fig:comppost}\caption{(Part 3.e) Comparison of the posteriors for $\theta\given X$ with $(X=36,n=20)$ (left) and  $(X=72,n=40)$ (right).}
\end{figure}

\bsh
Notice that the new $x$ and $n$ values are each exactly twice the old ones.  This implies that with twice the sample size we observed twice as many counts.  This translates into a posterior pmf that is more concentrated around the mean with the new values.  As we have more information, we are more certain about the information conveyed by the data about the parameter $\theta$.
\esh

\item Suppose the data $y_{1:n}\given \theta$ is modeled as iid Exp$(\theta)$, and the prior is
$$p(\theta) = \text{Gamma}(\theta \given a, b) = \frac{b^a}{\Gamma(a)}\theta^{a-1} e^{-b\theta}\1_{\lrb{\theta > 0}}.$$ From problem 1, we know that the posterior is $ p(\theta\given y_{1:n}) = \text{Gamma}(\theta \given a_n, b_n)$,
where $a_n = a + n$ and $b_n = b + \sum_{i=1}^n y_i$. What is the posterior predictive density $p(y_{n+1} \given y_{1:n})$? Give your answer as a closed-form expression (not an integral).

\bsh
Recalling that the definition of the posterior predictive density is:
$$p(y_{n+1}\given y_{1:n})=\int p(y_{n+1}\given \theta)p(\theta \given y_{1:n})d\theta,$$
we have
\bean
p(y_{n+1}\given y_{1:n})&=& \int \text{Exp}(y_{n+1}\given \theta)\text{Gamma}(\theta \given a_n,b_n) d\theta\\
&=& \int \lrp{\theta e^{-\theta y_{n+1}} \1_{\lrb{y_{n+1}>0} }} \lrp{\frac{b_n^{a_n}}{\Gamma(a_n)}\theta^{a_n-1} e^{-b_n\theta}\1_{\lrb{\theta > 0}}} d\theta\\
&=& \frac{b_n^{a_n}}{\Gamma(a_n)} \lrp{\int \theta^{1+a_n-1} e^{-(y_{n+1}+b_n)\theta }  d\theta}  \1_{\lrb{y_{n+1}>0}}\\
&=& \frac{b_n^{a_n}}{\Gamma(a_n)} \frac{\Gamma(1+a_n)}{(y_{n+1}+b_n)^{1+a_n}} \times {} \\
&&{}\qquad\qquad  \Bigg(\int \underbrace{\frac{(y_{n+1}+b_n)^{1+a_n}}{\Gamma(1+a_n)} \theta^{1+a_n-1} e^{-(y_{n+1}+b_n)\theta }}_{\text{Gamma}(\theta\given 1+a_n,y_{n+1}+b_n)}  d\theta\Bigg)  \1_{\lrb{y_{n+1}>0}}\\
&=& \frac{b_n^{a_n}}{(y_{n+1}+b_n)^{1+a_n} } \frac{a_n \Gamma(a_n)}{\Gamma(a_n)} \1_{\lrb{y_{n+1}>0}}\qquad\text{since $\Gamma(x+1)=x \Gamma(x)$}\\
&=& \frac{a_n}{(y_{n+1}+b_n)}\lrp{\frac{b_n}{y_{n+1}+b_n}}^{a_n}  \1_{\lrb{y_{n+1}>0}}
\eean
\esh


\item (decision theory) Suppose that in the small imaginary city of our class example, where the prevalence of a rare disease was studied (Foundations: Section 4, page 11), public health officials need to decide the amount of resources to allocate towards prevention and treatment of the disease we are concerned with, with the fraction of infected individuals $\theta$ still unknown. They will decide on the resources needed based on a fraction $c$ of the population. If $c$ is chosen too large, there will be wasted resources, while if it is too small, preventable cases may occur and some individuals may go untreated. After some deliberation, they tentatively adopt the following loss function: 

$$\ell(\theta,c) = (\theta-c+0.5)^2\quad\text{for }c\in\lrsqb{0,1}$$

\benum
\item Assume that the number of people sampled is again $n=20$, that $y=0$, and use $\btheta\sim \text{Beta}(2,20)$ as the prior again to derive the posterior expected loss.

\bsh
Recall that the posterior for $\btheta$ assuming a binomial likelihood and a Beta$(2,20)$ is $$\text{Beta}(a+y,b+n-y)\equiv \text{Beta}(2,40).$$ Letting $a_n=a+y$, and $b_n=b+n-y$, we have that the expected posterior loss function is given by 
\bean
\rho(c;y)&=&\int_{0}^1 (\theta-c+0.5)^2 \frac{1}{B(a_n,b_n)} \theta^{a_n-1}(1-\theta)^{b_n-1}d\theta\\
&=& \underbrace{\int_{0}^1 \theta^2\frac{1}{B(a_n,b_n)} \theta^{a_n-1}(1-\theta)^{b_n-1}d\theta}_{E[\btheta^2 | y]=\text{var}(\btheta | y)+E[\btheta | y]^2} \\
&&\quad -2(c-0.5) \underbrace{\int_{0}^1 \theta \frac{1}{B(a_n,b_n)} \theta^{a_n-1}(1-\theta)^{b_n-1}d\theta}_{E[\btheta | y]} \\
&&\quad + (c-0.5)^2 \underbrace{\int_{0}^1 \frac{1}{B(a_n,b_n)} \theta^{a_n-1}(1-\theta)^{b_n-1}d\theta}_{=1} \\
&=& \text{var}(\btheta | y)+E[\btheta | y]^2 + -2(c-0.5) E[\btheta | y] + (c-0.5)^2
\eean
\esh

\item Find an optimal value (call it $\hat{c}$) for $c$ following a Bayesian decision procedure using the set up from part (a).
\bsh
The posterior expected loss function above is a quadratic function in $c$. Given that it is a convex function it has a global minimum.  To find the value that minimizes $\rho(c;y)$ we simply differentiate with respect to $c$, set the derivative to $0$, and solve for $c$, as follows:
\bean
\frac{d\rho(c;y)}{dc}&=& \frac{d}{dc}\lrp{\text{var}(\btheta | y)+E[\btheta | y]^2 + -2(c-0.5) E[\btheta | y] + (c-0.5)^2}\\
&=& -2 E[\btheta | y] + 2 (c-0.5) \stackrel{set}{=} 0\\
&&\\
\Longrightarrow&& \hat{c}\;=\; E[\btheta | y] + 0.5 \;=\;\frac{a_n}{a_n+b_n} +0.5 \;\approx\; 0.55
\eean
\esh

\begin{figure}[H]
{\scriptsize
<<fig.height=4, fig.width=4, fig.align='center'>>=
n=20; y=0; a=2; b=20
rhofn <- function(c){
  ff <- function(theta){
    db <- dbeta(theta,shape1=(a+y),shape2=(b+n-y))
    ((theta-c+0.5)^2) * db
  }
  res <- integrate(ff, lower=0, upper=1)
  res$value
}
vrhofn <- Vectorize(rhofn, vectorize.args = "c")
c.values <- seq(0,1,by=0.005)
rho.values <- vrhofn(c.values)
plot(x = c.values,
     y = rho.values,
     xlab = "c",
     ylab = expression(rho(c,y)),
     type = "l",
     lwd=2,
     col="orange")
@
}
\label{fig:postexploss}\caption{(Part 4.b) Posterior expected loss function for the problem setup provided.}
\end{figure}



\item Graphically compare $\ell(\theta,\hat{c})$ and $\ell(\theta,\bar{y})$ as $\theta$ ranges from 0 to 1.

\begin{figure}[H]
{\scriptsize
<<fig.height=4, fig.width=4, fig.align='center'>>=
ybar <- y/n
c.hat <- ((2+y)/(2+y+40+n-y))+0.5
theta <- seq(0,1,by=0.005)

plot(x=theta,y=(theta-ybar+0.5)^2,
     ylim=c(0,2.5),
     type="l",
     col="cornflowerblue",
     xlab=expression(theta),
     ylab=expression(l(theta,c)))
lines(x=theta,y=(theta-c.hat+0.5)^2,col="orange")
legend("topleft",col=c("orange","cornflowerblue"),
       lty=rep(1,2),
       bty="n",
       legend=c(expression(c==hat(c)),
                expression(c==bar(y))))
@
}
\label{fig:losscomp}\caption{(Part 4.c) Loss function comparison for $c=\hat{c}$ and $c=\bar{y}$.}
\end{figure}

And so, the optimal Bayes solution $c=\hat{c}$ provides a smaller loss values than those obatiend for $c=\bar{y}$ for all $\theta\in\lrp{0,1}$.

\item Compare the outcome from the Bayesian decision procedure to the procedures that choose (i) $c=\bar{y}$ and (ii) $c = 0.1$ constant (i.e., setting it to the prior mean). This comparison can be done by observing the optimal quantity $c$ derived from each of the decision procedures considered while varying the value of $y$ (the number of infected cases).

\begin{figure}[H]
{\scriptsize
<<fig.height=4, fig.width=4, fig.align='center'>>=
yvec <- 0:20
ybar <- yvec/n
c.hat <- ((2+yvec)/(2+yvec+40+n-yvec))+0.5

plot(x=yvec,y=ybar,
     type="l",
     col="cornflowerblue",
     xlab="y",
     ylab=expression(c(y)))
lines(x=yvec,y=c.hat,col="orange")
lines(x=yvec,y=rep(0.1,21),col="forestgreen")
legend("topleft",
       col=c("orange","cornflowerblue","forestgreen"),
       lty=rep(1,3),
       bty="n",
       legend=c(expression(c==hat(c)),
                expression(c==bar(y)),
                expression(c==0.1)))
@
}
\label{fig:compvaryingy}\caption{(Part 4.d) Optimal $c$ as $y$ ranges from 0 to 20, letting  $c=\hat{c}$, $c=\bar{y}$, and $c=0.1$.}
\end{figure}


\eenum

\eenum
\end{document}