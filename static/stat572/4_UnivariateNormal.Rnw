\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espaÃ±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{float}

\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbs}[1]{\boldsymbol{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\renewcommand{\d}{\text{d}}
\newcommand{\by}{\mbf{y}}
\newcommand{\mts}{\tilde{Y}}
\newcommand{\mtsv}{\tilde{\bv}}
\newcommand{\btw}{\tilde{\bw}}
\newcommand{\bhw}{\hat{\bw}}
\newcommand{\btx}{\tilde{\bx}}
\newcommand{\pt}{\tilde{p}}
\newcommand{\ba}{\mbf{a}}
\newcommand{\bb}{\mbf{b}}
\newcommand{\bc}{\mbf{c}}
\newcommand{\bd}{\mbf{d}}
\newcommand{\boe}{\mbf{e}}
\newcommand{\bk}{\mbf{k}}
\newcommand{\bq}{\mbf{q}}
\newcommand{\br}{\mbf{r}}
\newcommand{\bs}{\mbf{s}}
\newcommand{\bh}{\mbf{h}}
\newcommand{\bff}{\mbf{f}}
\newcommand{\bt}{\mbf{t}}
\newcommand{\bu}{\mbf{u}}
\newcommand{\bm}{\mbf{m}}
\newcommand{\bv}{\mbf{v}}
\newcommand{\bx}{\mbf{x}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\tw}{\tilde{w}}
\newcommand{\bz}{\mbf{z}}
\newcommand{\tby}{\tilde{\mbf{y}}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\bp}{\mbs{p}}
\newcommand{\bA}{\mbf{A}}
\newcommand{\tbA}{\tilde{\bA}}
\newcommand{\bB}{\mbf{B}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\tc}{\tilde{c}}
\newcommand{\tC}{\tilde{C}}
\newcommand{\tbC}{\tilde{\bC}}
\newcommand{\bF}{\mbf{F}}
%\newcommand{\bBs}{\mbf{B}^\star}
\newcommand{\bD}{\mbf{D}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bK}{\mbf{K}}
\newcommand{\bQ}{\mbf{Q}}
\newcommand{\bV}{\mbf{V}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\hN}{\hat{N}}
\newcommand{\tbc}{\tilde{\mbf{c}}}
\newcommand{\tba}{\tilde{\mbf{a}}}
\newcommand{\tbX}{\tilde{\mbf{X}}}
\newcommand{\tbW}{\tilde{\mbf{W}}}
\newcommand{\bSigma}{\mbs{\Sigma}}
\newcommand{\bGamma}{\mbs{\Gamma}}
\newcommand{\bUps}{\mbs{\Upsilon}}
\newcommand{\bPsi}{\mbs{\Psi}}
\newcommand{\vs}{v^\star}
\newcommand{\Vs}{V^\star}
\newcommand{\bvs}{\bv_i^\star}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bP}{\mbf{P}}
\newcommand{\bBs}{\bB^\star}
\newcommand{\bXs}{\bX^\star}
\newcommand{\bxs}{\bx^\star}
\newcommand{\bWs}{\bW^\star}
\newcommand{\bws}{\bw_i^\star}
\newcommand{\bwsp}{\left.\bw_i^\star\right.^\prime}
\newcommand{\bBsp}{\left.\bB^\star\right.^\prime}
\newcommand{\bVs}{\bV^\star}
\newcommand{\bpsi}{\mbs{\psi}}
\newcommand{\bphi}{\mbs{\phi}}
\newcommand{\bmu}{\mbs{\mu}}
\newcommand{\bbeta}{\mbs{\beta}}
\newcommand{\bxi}{\mbs{\xi}}
\newcommand{\bchi}{\mbs{\chi}}
\newcommand{\blambda}{\mbs{\lambda}}
\newcommand{\bLambda}{\mbs{\Lambda}}
\newcommand{\LamT}{\tilde{\Lambda}}
\newcommand{\GamT}{\tilde{\Gamma}}
\newcommand{\WT}{\tilde{W}}
\newcommand{\balpha}{{\mbs{\alpha}}}
\newcommand{\bepsilon}{{\mbs{\varepsilon}}}
\newcommand{\bgamma}{{\mbs{\gamma}}}
\newcommand{\btheta}{{\mbs{\theta}}}
\newcommand{\bseta}{{\mbs{\eta}}}
\newcommand{\bpi}{{\mbs{\pi}}}
\newcommand{\bI}{\mbf{I}}
\newcommand{\bH}{\mbf{H}}
\newcommand{\tbH}{\tilde{\mbf{H}}}
\newcommand{\1}{\mbs{1}}
\newcommand{\0}{\mbs{0}}
\newcommand{\detstar}[1]{\text{det}^+\left(#1\right)}
\renewcommand{\det}[1]{\text{det}\left(#1\right)}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\renewcommand{\exp}[1]{\text{exp}\left[#1\right]}
\newcommand{\M}{{M}}
\newcommand{\K}{{K}}
\newcommand{\peq}{{p}}
\newcommand{\MB}{{M_B}}
\newcommand{\MF}{{M_F}}
\newcommand{\MT}{{M_T}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\graph}{\Gamma}
\newcommand{\kset}{\Upsilon}
\newcommand{\order}{order}
\newcommand{\parents}{\mcal{P}}
\newcommand{\children}{\mcal{C}}
\newcommand{\extreme}{\mcal{E}}
\newcommand{\combined}{\mcal{A}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\lrp}[1]{\left(#1\right)}
\newcommand{\lrb}[1]{\left\{#1\right\}}
\newcommand{\lrno}[1]{\left.#1\right.}
\newcommand{\lrsqb}[1]{\left[#1\right]}
\newcommand{\G}[1]{\Gamma_{#1}}
\newcommand{\N}{\mcal{N}}
\renewcommand{\d}{\text{d}}
\newcommand{\Ps}[1]{\Pr{\left(#1\right)}}
\newcommand{\BF}[2]{{BF}_{#1,#2}(Y)}
\newcommand{\BFd}[3]{{BF}_{#1,#2}(Y,#3)}
\newcommand{\xmark}{\ding{55}}
\newcommand{\ben}{\begin{equation*}}
\newcommand{\een}{\end{equation*}}
\newcommand{\bean}{\begin{eqnarray*}}
\newcommand{\eean}{\end{eqnarray*}}
\newcommand{\bsm}{\begin{smallmatrix}}
\newcommand{\esm}{\end{smallmatrix}}
\newcommand{\bmat}{\begin{matrix}}
\newcommand{\emat}{\end{matrix}}
\newcommand{\tI}{\text{I}}
\newcommand{\tN}{\text{N}}
\newcommand{\trN}{\text{trunc.N}}
\newcommand{\nl}[1]{\text{log}{\lrp{#1}}}
\newcommand{\e}[1]{\text{exp}{\lrb{#1}}}
\newcommand{\indf}[1]{\tI_{\left\{#1\right\}}}
\newcommand{\parent}{\mcal{P}}
\newcommand{\gp}{\text{GP}{\lrp{\0,\mcal{C}(\cdot\given\bphi)}}}
\newcommand{\gpd}[3]{\text{GP}_{#1}{\lrp{\0,\mcal{C}_{#2}(\cdot\given\phi_{#3})}}}
\newcommand{\mnngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mbs{\mcal{C}}}_{#2}(\cdot,\cdot;\bphi_{#3})}}}
\newcommand{\nngpd}[3]{\text{NNGP}_{#1}{\lrp{\0,\tilde{\mcal{C}}_{#2}(\cdot,\cdot;\phi_{#3})}}}
\newcommand{\nngpw}[4]{\text{NNGP}_{#1}^{#2}{\lrp{0,\tilde{\mcal{C}}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpw}[4]{\text{GP}_{#1}^{#2}{\lrp{0,\mcal{C}_{#2}(\cdot\given \phi_{#3}^{#4})}}}
\newcommand{\gpq}{\text{GP}_q{\lrp{\0,\mbs{\mcal{C}}(\cdot\given\bphi)}}}
\newcommand{\gpk}{\text{GP}{\lrp{0,\mcal{C}(\cdot\given\tilde{\phi}_k)}}}
\newcommand{\nngp}{\text{NNGP}{\lrp{\0,\tilde{\mcal{C}}(\cdot\given\phi)}}}
\newcommand{\refset}{\mcal{T}}
\newcommand{\uset}{\mcal{U}}
\newcommand{\oset}{\mcal{T}}
\newcommand{\Xall}{\mbb{X}}
\newcommand{\given}{\,|\,}
\newcommand{\dtr}[1]{\textcolor{blue}{(#1)}}
\newcommand{\bemph}[1]{\bf \emph{#1}}
\newcommand{\var}[1]{\text{var}{(#1)}}

%-------------------------------
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{exa}{Example}
\newtheorem{note}{Note}
\newcommand{\bex}{\begin{exer}}
\newcommand{\eex}{\end{exer}}
\newcommand{\bexa}{\begin{exa}}
\newcommand{\eexa}{\end{exa}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\bdes}{\begin{description}}
\newcommand{\edes}{\end{description}}

\newcommand{\bsh}{\begin{shaded}}
\newcommand{\esh}{\end{shaded}}
%-------------------------------

%-------------------------------
%hiding proof solutions
%-------------------------------


\begin{document}


\setcounter{section}{0}
\title{The Univariate Normal Model}

\thispagestyle{empty}

\begin{center}
{\LARGE \bf The Univariate Normal Model}\\
{\large STAT 572: Bayesian Statistics}\\
Fall 2018
\end{center}
\section{The Normal Distribution}

A random variable $Y$ having normal distribution, represented as $Y\sim\N(\mu,\sigma^2)$ (sometimes called the Gaussian distribution), with mean $\mu\in\mathbb{R}$ and variance $\sigma^2>0$ (and standard deviation $\sigma=\sqrt{\sigma^2}$), has pdf $$p(y\given \mu,\sigma^2)= \frac{1}{\sqrt{2\pi \sigma^2}}\e{-\frac{1}{2\sigma^2}(y-\mu)^2}$$ for $y\in\mathbb{R}$.

As we will discover later, when doing Bayesian inference it can be convenient to write the normal pdf in terms of the precision $\lambda=1/\sigma^2$ instead of using the variance. In this case, since $\sigma^2=1/\lambda=\lambda^{-1}$, replacing $\sigma^2$ by $\lambda^{-1}$ in the pdf above provides $$p(y\given \mu,\lambda)= \sqrt{\frac{\lambda}{2\pi}}\e{-\frac{\lambda}{2}(y-\mu)^2}.$$ 

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=5, fig.height=4, fig.align='center', eval=T>>=
pnf <- function(mu,sig){function(x)dnorm(x,mu,sig)}
pnf1 <- pnf(0,1)
pnf2 <- pnf(0,2)
pnf3 <- pnf(1,0.75)
pnf4 <- pnf(2,0.5)

plot(x=c(-6,6),y=c(0,1),type="n",
     ylab=expression(paste("N(",y,"|",mu,",",sigma^2,")") ),
     xlab="y")
curve(pnf1,from=-6,to=6,add=T,lwd=2,col=1)
curve(pnf2,from=-6,to=6,add=T,lwd=2,col=2)
curve(pnf3,from=-6,to=6,add=T,lwd=2,col=3)
curve(pnf4,from=-6,to=6,add=T,lwd=2,col=4)
legend("topleft",col=1:4,lty=rep(1,4),
       legend=c(expression(list(mu==0,sigma==1)),
                expression(list(mu==0,sigma==2)),
                expression(list(mu==1,sigma==0.75)),
                expression(list(mu==2,sigma==0.5))))
@
\caption{Normal probability density curves for different values of $\mu$ and $\sigma^2$}
\label{fig:normalpdf}
\end{figure}



As you may already know, the normal distribution is one of the most important distributions used in probability and statistics, and this position is justly deserved.  The normal distribution has many special properties, most importrant among these is the central limit theorem (CLT), which states that the sum of a large number of independent random variables tends to be approximately normally distributed. The CLT explains why real-world data so often appears approximately normal, and from a modeling perspective, it helps us to understand when a normal model would be appropriate.

Many problems that we encounter in the  real-world quantities tend to be normally distributed—for example, human heights, weights and other body measurements, cumulative hydrologic measures such as annual rainfall or monthly river discharge, errors in physical observations, etc. Other measurements are products of many independent variables (rather than sums); in these case the logarithm will be approximately normal since it is a sum of many independent variables. Because of the CLT, even if a variable is not directly normally distributed, the average of a large random sample (a set of iid random variables) will be approximately normally distributed. 

\subsection{Properties of the normal distribution}

\benum
\item Mean$=$Median$=$Mode$=\mu$ 
\item The distribution is symmetric about the mean
\item 95\% of the distribution is within $\pm 1.96$ of the mean
\item If $Y\sim \N(\mu,\sigma^2)$ and $a\in\mathbb{R}$, then
$$Y+a \sim \N(\mu+a,\sigma^2)\quad\text{and}\quad aY \sim \N(a\mu,a^2\sigma^2)$$
\item If $Y\sim \N(\mu,\sigma^2)$ and $X\sim \N(\nu,\tau^2)$ are independent, and if $a$ and $b$ are constant, then $$a Y+ b X \sim \N(a\mu+b\nu,a^2\sigma^2+b^2\tau^2).$$
\eenum

\subsection{R functions for normal distributions}
For $Y\sim \N(\mu,\sigma^2)$, we have the following R functions
\bdes
\item[\textsf{rnorm}:] random number generation
\item[\textsf{dnorm}:] $p(y\given \text{mean}=\mu,\text{sd}=\sigma)$ for a particular value $y\in\mcal{Y}$
\item[\textsf{pnorm}:] $\Pr(Y\leq y \given \text{mean}=\mu,\text{sd}=\sigma)$ for a particular value $y\in\mcal{Y}$
\item[\textsf{qnorm}:] returns the quantiles for particular probabilities
\edes

\bsh

\note \textsf{WARNING --} All of the R functions above take as inputs the \textsf{mean} and the \textsf{standard deviation} (instead of the variance).

\note The properties described above coupled with the analytic tractability of the distribution, make the normal model a convenient choice to develop complex models while keeping computational complexity to a minimum.

\esh

\section{Conjugate priors for the normal mean}

Let's develop the Bayesian model by steps.  To start simple, let's assume that we have a random sample (a set of iid random variables) $Y_1,Y_2,\ldots, Y_n$ such that $Y_i\sim \N(\theta,\lambda^{-1})$ (i.e., with mean $\theta$ and precision $\lambda$), where $\lambda=1/\sigma^2$ is fixed and known.  


\subsection{Posterior derivation}

\bsh

If we assume that a priori $$\btheta \sim \N(\theta_0,\lambda_0^{-1}),$$ then what is the posterior for $\btheta$?

Let's begin by obtaining the likelihood for $n$ observations drawn from a $\N(\theta,\lambda^{-1})$ model:

\vspace{7cm}
% \bea
% p(y_{1:n} \given \theta,\lambda) &=&\prod_{i=1}^n \lrp{\frac{\lambda}{2\pi}}^{1/2}\e{-\frac{\lambda}{2} (y_i-\theta)^2}\nonumber \\
% &\propto& \lambda^{n/2} \e{-\frac{\lambda}{2} \sum_{i=1}^n (y_i-\theta)^2}\nonumber\\
% &=& \lambda^{n/2} \e{-\frac{\lambda}{2} \lrp{\sum_{i=1}^n y_i^2 - 2 \theta \sum_i y_i+n \theta^2}}\nonumber\\
% &=& \lambda^{n/2} \e{-\frac{\lambda}{2} \lrp{\sum_{i=1}^n y_i^2 - 2 n \theta \bar{y} +n \theta^2}}\nonumber \\
% &\underset{\theta}{\propto}& \lambda^{n/2} \e{-\frac{n \lambda}{2} \lrp{\theta^2 - 2 \theta \bar{y}}}
% \eea

Replacing the last expression above into the posterior formula, the posterior is

\vspace{6.7cm}
% \bean
% p(\theta \given y_{1:n}) &\propto& p(y_{1:n} \given \theta,\lambda) p(\theta)\nonumber \\
% &\underset{\theta}{\propto}& \lrp{ \lambda^{n/2} \e{-\frac{n \lambda}{2} \lrp{\theta^2 - 2 \theta \bar{y}}} } \lrp{\lambda_0^{1/2}\e{-\frac{\lambda_0}{2} (\theta-\theta_0)^2}} \\
% &=& \lrp{ \lambda^{n/2} \e{-\frac{n \lambda}{2} \lrp{\theta^2 - 2 \theta \bar{y}}} } \lrp{\lambda_0^{1/2}\e{-\frac{\lambda_0}{2} (\theta^2-2\theta\; \theta_0+  \theta_0^2)}} \\
% &\underset{\theta}{\propto}& \lrp{ \lambda^{n/2} \e{-\frac{n \lambda}{2} \lrp{\theta^2 - 2 \theta \bar{y}}} } \lrp{\lambda_0^{1/2}\e{-\frac{\lambda_0}{2} (\theta^2-2\theta\; \theta_0)}} \\
% &\underset{\theta}{\propto}& \e{-\frac{1}{2} \lrp{\theta^2\lrp{n \lambda+\lambda_0} - 2 \theta \lrp{ n \lambda \bar{y} +\lambda_0\theta_0}} }.
% \eean

Completing the square for $\theta$, we have that

\vspace{7cm}
% \bea
% p(\theta \given y_{1:n})&\underset{\theta}{\propto}& \e{-\frac{(n \lambda+\lambda_0)}{2} \lrp{\theta^2 - 2 \theta \frac{1}{(n \lambda+\lambda_0)}(n \lambda \bar{y} + \lambda_0 \theta_0 ) + \lrp{\frac{1}{(n \lambda+\lambda_0)}(n \lambda \bar{y} +\lambda_0\theta_0 )}^2}}{}\nonumber\\ 
% &&{}\qquad \times \e{-\frac{(n \lambda+\lambda_0)}{2} \lrp{ - \lrp{\frac{1}{(n \lambda+ \lambda_0)}(n \lambda \bar{y} + \lambda_0 \theta_0 )}^2 } } \nonumber \\
% &\underset{\theta}{\propto}& \e{-\frac{(n \lambda+\lambda_0)}{2} \lrp{\theta - \lrsqb{\frac{n \lambda}{n \lambda+\lambda_0} \bar{y} +\frac{\lambda_0}{n \lambda+\lambda_0}\theta_0 }}^2}
% \eea
% which corresponds to the kernel of a $\N(\theta^\star,{\lambda^\star}^{-1})$, with precision 
% \bea
% \lambda^\star&=&(n\lambda +\lambda_0),\quad\text{and mean }\nonumber\\
% \theta^\star&=&\lrp{\frac{n \lambda}{n \lambda+\lambda_0} \bar{y} +\frac{\lambda_0}{n \lambda+\lambda_0}\theta_0}.\label{eq:parsfixmu}
% \eea

% And so $\btheta \given y_{1:n} \sim \N(\theta^\star,{\lambda^\star}^{-1})$, which implies that the normal family of priors for $\btheta$ is conjugate with the normal likelihood.

\esh

\subsection{Example: Is human height bimodal?}

The distribution of heights of adult humans—when separated according to sex (female or male) is a classic example of a normal distribution. It seems that the reason why height tends to be normally distributed is because there are many independent genetic and environmental factors which contribute additively to overall height, and this leads to a normal distribution due to the central limit theorem. However, the combined distribution of heights (pooling females and males together) is not normal, and is often said to be bimodal -- that is, having two modes (i.e., two maxima). But is this really the case?

\begin{figure}[h]
<<echo=F, message=F, warning=F, fig.width=6, fig.height=4, fig.align='center'>>=
X <- mice::selfreport
with(X[!is.na(X$hm),],
     {
       plot(density(hm,from=140,to=215),  panel.first = grid(col=grey(0.6)),lwd=2,
            ylim=c(0,0.07),xlim=c(140,215),main="",lab=c(15,7,5),cex.axis=0.7,
            xlab="height (cm)",ylab="estimated density")
       lines(density(hm[sex=="Male"],from=140,to=215),col="red",lwd=2)
       lines(density(hm[sex=="Female"],from=140,to=215),col="blue",lwd=2)
       legend("topright",col=c("blue","red","black"),lty=rep(1,3),
              lwd=rep(2,3),legend=c("female","male","both"),bty="n")
      })

@
\caption{Estimated densities for the measured heights (cm) of Dutch men and women combined and by gender.}
\label{fig:heights}
\end{figure}

Figure \ref{fig:heights} shows estimated densities of the heights of Dutch women, Dutch men and of the combined populations, based on a sample of 695 women and 562 men found in the \textsf{selfreport} dataset included in the R package \textsf{mice}. A first glance of the data appears to indicate that the combined distribution does not look bimodal. However, the heights of women and men separately appear to be roughly normally distributed, but quite different one from the other.  Can we test bimodality of the combined density in a more precise way?
For now, assume that female heights and male heights are each normally distributed, both having the same standard deviation. Also assume that there is an equal proportion of women and men in the population. Then, it is known that the combined distribution is bimodal if and only if the difference between the means is greater than twice the standard deviation.

\subsubsection{Building the model}

For the problem above, denote by $Y_1,\ldots,Y_n\overset{iid}{\sim}\N(\theta_f,\sigma^2)$ a random sample of $n=695$ female heights, and by $X_1,\ldots,X_m\overset{iid}{\sim}\N(\theta_m,\sigma^2)$ a random sample of $m=562$ male heights.  % From here we have that the distribution for the heights of females and males combined is given by 
% 
% $$\frac{1}{2}\N(\theta_f,\sigma^2)+\frac{1}{2}\N(\theta_m,\sigma^2),$$
% which is an example of a {\bemph{two-component mixture}}, meaning that the sample can be drawn from the first component with probability $1/2$ (i.e., a female), and likewise with probability $1/2$ from the second component (i.e., a male), which is where the assumption of both genders being in equal proportion in the population comes into play.

For now, in this example we assume that the variance $\sigma^2$ is known and set the standard deviation $\sigma= 8$ cm, and so we only have to specify a prior for $(\theta_f,\theta_m)$, which we assume to be 

$$p(\theta_f,\theta_m)=p(\theta_f)p(\theta_m)=\N(\theta_f\given \mu_f,\tau^2)\; \N(\theta_f\given \mu_m,\tau^2),$$
setting $\mu_f=165$ cm, $\mu_m=178$ cm, and $\tau=15$ cm (the std. deviation) based on common knowledge of human heights.  This prior form assumes independence between $\theta_f$ and $\theta_m$. Importantly, note that $\tau$ represents our uncertainty about the mean heights (not of the person--specific heights).

It is known that the combined distribution is bimodal if $|\theta_f-\theta_m|>2\sigma$; hence, to address the bimodality question, we can derive the posterior probability

$$\Pr(\text{bimodal}\given \text{data})=\Pr(|\btheta_f-\btheta_m|> 2\sigma \given y_{1:n}, x_{1:m}).$$

Given that the posterior of $\theta_f$ is only affected by the $Y_i$'s and that for $\theta_m$ by the $X_j$'s, we may use the formula for the posterior we obtained in the previous section, parameterized as in Equation \eqref{eq:parsfixmu}.  Thus, letting $\lambda=1/\sigma^2$ and $\lambda_0=1/\tau^2$ we have that
$$p(\theta_f\given y_{1:n})= \N\lrp{\mu_f^\star,\lambda_f^\star}\quad\text{and}\quad
p(\theta_m\given x_{1:m})= \N\lrp{\mu_m^\star,\lambda_m^\star}$$
where
\bean
\mu_f^\star &=&\lrp{\frac{n \lambda}{n \lambda+\lambda_0} \bar{y} +\frac{\lambda_0}{n \lambda+\lambda_0}\mu_f},\\
\mu_m^\star &=&\lrp{\frac{m \lambda}{m \lambda+\lambda_0} \bar{x} +\frac{\lambda_0}{m \lambda+\lambda_0}\mu_m},\\
\lambda_f^\star&=&(n\lambda+\lambda_0),\quad\text{and}\\
\lambda_m^\star&=&(m\lambda+\lambda_0).
\eean
Note that these posterior solely depend on the data through the sufficient statistics $\bar{Y}$ and $\bar{X}$. From the data we have that $\bar{y}=\Sexpr{with(X,round(mean(hm[sex=="Female"],na.rm=T),2))}$ and $\bar{x}=\Sexpr{with(X,round(mean(hm[sex=="Male"],na.rm=T),2))}$. These posterior densities as well as the corresponding priors are plotted in Figure \ref{fig:prpost.hts} below.

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=6, fig.height=4, fig.align='center'>>=
X <- X[!is.na(X$hm),]
n <- sum(X$sex=="Female")
m <- sum(X$sex=="Male")

#sample means (sufficient statistics)
ybar <- with(X,mean(hm[sex=="Female"]))
xbar <- with(X,mean(hm[sex=="Male"]))

#likelihood fixed parameters
sigma <- 8
lambda <- 1/(sigma^2)

#prior parameters
mu.f <- 165
mu.m <- 178
tau <- 15
lambda0 <- 1/(tau^2)

#weights for posterior info
w.f <- n*lambda / (n*lambda+lambda0)
w.m <- m*lambda / (m*lambda+lambda0)

#posterior parameters
mu.f.star <- w.f * ybar + (1-w.f) * mu.f 
mu.m.star <- w.m * xbar + (1-w.m) * mu.m
tau.f.star <- 1 / sqrt(n*lambda+lambda0)
tau.m.star <- 1 / sqrt(m*lambda+lambda0)

h.vec <- seq(140, 215, by = 0.5)

#prior density values
f.prior <- dnorm(h.vec,mu.f,tau)
m.prior <- dnorm(h.vec,mu.m,tau)

#posterior density values
f.post <- dnorm(h.vec,mu.f.star,tau.f.star)
m.post <- dnorm(h.vec,mu.m.star,tau.m.star)

ylims <- range(c(f.post,m.post))
plot(x=h.vec, y=f.prior, panel.first = grid(col=grey(0.6)), lwd=2, type = "l",
            ylim=ylims,xlim=c(140,215),main="",lab=c(15,7,5),cex.axis=0.7,
            xlab="height (cm)",ylab="estimated density",col="blue",lty=3)
       lines(x=h.vec, y=m.prior,col="red",lty=3)
       lines(x=h.vec, y=f.post,col="blue",lty=1)
       lines(x=h.vec, y=m.post,col="red",lty=1)
       legend("topright",col=rep(c("blue","red"),each=2),
              lty=rep(c(1,3),2),lwd=rep(2,4),
              legend=c(expression(paste("p(",theta[f],"|",y[1:n],")")),
                       expression(p(theta[f]) ),
                       expression(paste("p(",theta[m],"|",x[1:n],")")),
                       expression(p(theta[m]) ) ),bty="n")
       
@
\caption{Prior and posterior densities for Dutch men and women.}
\label{fig:prpost.hts}
\end{figure}

Given that we are interested in the posterior distribution of the random variable $\Delta_\theta=\btheta_f-\btheta_m$, which is a linear combination of two inependent random variables, from the properties of the normal distribution given in Section 1.1, we have that
$$ \Delta_\theta \sim \N\lrp{\mu_f^\star-\mu_m^\star,\frac{1}{\lambda_f^\star}+\frac{1}{\lambda_m^\star}}.$$

\begin{figure}[h]
<<echo=F, message=F, warning=F, fig.width=6, fig.height=4, fig.align='center'>>=

#parameters for the posterior distribution of theta.f-theta.m
mu.diff <- mu.f.star-mu.m.star
tau.diff <- sqrt(tau.f.star^2+tau.m.star^2)

#sample means (sufficient statistics)

h.vec <- mu.diff + seq(-3*tau.diff, 3*tau.diff, by = 0.05)

#posterior density values
diff.post <- dnorm(h.vec,mu.diff,tau.diff)
pdiff.g2sigma <- pnorm(-2*sigma,mu.diff,tau.diff,lower.tail = T)+
  pnorm(2*sigma,mu.diff,tau.diff,lower.tail = F)

ylims <- range(diff.post)
plot(x=h.vec, y=diff.post, panel.first = grid(col=grey(0.6)), 
     lwd=2, type = "l",ylim=ylims,xlim=range(h.vec),
     main="",lab=c(15,7,5),cex.axis=0.7,
     xlab="difference in height (cm)",ylab="estimated density",col="blue",lty=1)
x.ll <- seq(-3*tau.diff+mu.diff,-2*sigma,length.out = 100)
y.ll <- dnorm(x.ll,mu.diff,tau.diff)
polygon(x=c(x.ll,rev(x.ll)), c(rep(0,length(y.ll)),rev(y.ll)), 
        col = grey(0.7),border = grey(0.7),lty=1)
lines(x=h.vec, y=diff.post,col="blue")
abline(v=2*sigma)
legend("topright",col=rep(c("blue","red"),each=2),lty=1,lwd=2,
       legend=expression(paste("p(",theta[f]-theta[m],"|","data",")")),bty="n")
       
@
\caption{Posterior density for $\Delta_\theta=\btheta_f-\btheta_m$.}
\label{fig:postdiff}
\end{figure}

The parameter values for the posterior distribution of the difference $\Delta_\theta$ are thus: $$\mu_f^\star-\mu_m^\star=\Sexpr{round(mu.diff,2)},$$ and $$\frac{1}{\lambda_f^\star}+\frac{1}{\lambda_m^\star}=\Sexpr{round(tau.diff,2)}^2.$$

Using this posterior distribution we have that 
\bean
\Pr(|\Delta_\theta|>2\sigma) &=& \Pr(\Delta_\theta < - 2\sigma) + \Pr(\Delta_\theta > 2\sigma)\\
&=& \Sexpr{pdiff.g2sigma},
\eean
implying that the distribution of the differences $\Delta_\theta$, which is centered about 13 or 14 cm has a very small probability of taking values less than $-2 \sigma = -16$ or greater than $2 \sigma = 16$.  And so, there is very little evidence for the distribution for the heights combined for females and males to be multimodal.

\subsection{Posterior predictive distribution}

Recall that for a new observation $\tilde{Y}$, the posterior predictive density is defined as $p(\tilde{y}\given y_{1:n})$.  For the normal model, where $\sigma^2$ is assumed to be fixed and known, this density is given by

\pagebreak

\bsh
\vspace{10cm}
% \bean
% p(\tilde{y}\given y_{1:n})&=& \int_{-\infty}^\infty p(\tilde{y}\given \theta) p(\theta \given y_{1:n}) d\theta\\
% &=& \int_{-\infty}^\infty \N(\tilde{y}\given \theta,\lambda^{-1}) \N(\theta \given\theta^\star,{\lambda^\star}^{-1}) d\theta\\
% &=& (2\pi)^{-1/2-1/2}(\lambda\lambda^\star)^{1/2} \int_{-\infty}^\infty \e{-\frac{\lambda}{2}(\tilde{y}- \theta)^2 -\frac{\lambda^\star}{2}(\theta-\theta^\star)^2} d\theta\\
% &=& (2\pi)^{-1/2-1/2}(\lambda\lambda^\star)^{1/2} \int_{-\infty}^\infty \e{-\frac{\lambda}{2}(\theta^2-2 \theta \tilde{y}+\tilde{y}^2) -\frac{\lambda^\star}{2}(\theta^2-2\theta \theta^\star+{\theta^\star}^2)} d\theta\\
% &=& (2\pi)^{-1/2-1/2}(\lambda\lambda^\star)^{1/2} \int_{-\infty}^\infty \e{-\frac{1}{2}\left[\theta^2(\lambda+\lambda^\star)-2 \theta(\lambda \tilde{y}+\lambda^\star\theta^\star)\right]-\frac{\lambda}{2}\tilde{y}^2-\frac{\lambda^\star}{2}{\theta^\star}^2} d\theta\\
% &=& (2\pi)^{-1/2} \left(\frac{\lambda\lambda^\star}{\lambda+\lambda^\star}\right)^{1/2}\e{-\frac{\lambda}{2}\tilde{y}^2-\frac{\lambda^\star}{2}{\theta^\star}^2 + \frac{1}{2} \frac{1}{\lambda+\lambda^\star}(\lambda \tilde{y}+\lambda^\star\theta^\star)^2}\; \times\;\\
% && \int_{-\infty}^\infty (2\pi)^{-1/2}(\lambda+\lambda^\star)^{1/2}\e{-\frac{\lambda+\lambda^\star}{2}\left(\theta -\frac{1}{\lambda+\lambda^\star}(\lambda \tilde{y}+\lambda^\star\theta^\star)\right)^2}d\theta\\
% &=& (2\pi)^{-1/2} \left(\frac{\lambda\lambda^\star}{\lambda+\lambda^\star}\right)^{1/2}\e{-\frac{1}{2} \frac{\lambda\lambda^\star}{\lambda+\lambda^\star}(\tilde{y}-\theta^\star)^2 }
% \eean
% and so noting that $$\frac{\lambda+\lambda^\star}{\lambda\lambda^\star}={\lambda^\star}^{-1}+\lambda^{-1},$$
% 
% the posterior predictive density for $\tilde{Y}$ is $\N\lrp{\tilde{y}\given \theta^\star,{\lambda^\star}^{-1}+\lambda^{-1}}$.
\esh

Although I took the time to go over this painful derivation, we can avoid it by simply using the properties of the normal distribution. In particular, we know that
$$\tilde{Y}\given \theta, \sigma^2\sim \N(\theta,\lambda^{-1})\Longrightarrow \tilde{Y}=\theta + \tilde{\epsilon},$$
where $\tilde{\epsilon}\given \theta,\sigma^2\sim\N(0,\lambda^{-1})$, for a particular value of $\theta$.

However, recalling that $\btheta\given y_{1:n}\sim \N(\theta^\star,{\lambda^\star}^{-1})$, and denoting by $\bpsi=\btheta\given y_{1:n}$, we have that $$\tilde{Y}\given y_{1:n}=\bpsi+\tilde{\epsilon},$$ which by the properties of the normal (specifically the one regarding the distribution of the sum of two independent normal random variables), is distributed as follows 
\bean
\tilde{Y}\given y_{1:n}&\sim& \N\lrp{E(\bpsi)+E(\tilde{\epsilon}),var(\bpsi)+var(\tilde{\epsilon})}\\
&=&\N(\theta^\star,{\lambda}^{-1}+{\lambda^\star}^{-1}).
\eean

\pagebreak
\section{Conjugate priors for the normal mean and precision}

Up to now, we have assumed that the variance for the data is known, which is of course an unrealistic assumption in many, if not most, situations.  We now extend the Bayesian inference problem for the normal model where the variance is assumed unknown, and our uncertainty about it is built into the problem by specifying a prior distribution for it.

Making use of the axioms of probability we may represent the joint pdf for $(\btheta,\blambda)$ as $p(\btheta,\blambda)=p(\theta\given \lambda) p(\lambda)$.

It turns out that the \textsf{Normal-Gamma}$(\mu_0,\nu,\alpha,\beta)$ family of distributions is conjugate for $(\btheta,\blambda)$ when the data generating distribution is $\N(\theta,\lambda^{-1})$.  The
\textsf{Normal-Gamma}$(\mu_0,\nu,\alpha,\beta)$ can be equivalently represented as
\bea
\blambda &\sim& \text{Gamma}(\alpha,\beta)\nonumber\\
\btheta \given \lambda &\sim& \N(\mu_0,(\nu\lambda)^{-1}).
\eea

\subsection{Posterior derivation}

Because of conjugacy, the posterior for $\btheta,\blambda\given y_{1:n}$ must be \textsf{Normal-Gamma}$(\mu^\star,\nu^\star,\alpha^\star,\beta^\star)$ (a member of the same family as the prior), for some parameter values $\mu^\star,\nu^\star,\alpha^\star$, and $\beta^\star$.  We now derive the posterior distribution for this problem.

First, note that the \textsf{Normal-Gamma}$(\mu_0,\nu,\alpha,\beta)$ prior is
\bsh
\vspace{8cm}
% \bea
% p(\btheta,\blambda) &=& \N(\theta\given \mu_0,(\nu\lambda)^{-1})\text{Gamma}(\alpha,\beta)\nonumber\\
% &=& \lrp{\sqrt{\frac{\nu\lambda}{2\pi}} \e{-\frac{\nu\lambda}{2}(\theta-\mu_0)^2}} \lrp{\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1} \e{- \beta \lambda}}\nonumber\\
% &\propto& \lambda^{1/2+\alpha-1} \e{-\frac{\lambda}{2}(\nu\theta^2-2\nu\theta\mu_0+\nu\mu_0^2+2\beta},\label{eq:prnormg}
% \eea
% and the likelihood can be written as
% \bea
% p(y_{1:n}\given \theta,\lambda) &=& \lrp{\frac{\lambda}{2\pi}}^{n/2}\prod_i \e{-\frac{\lambda}{2}(y_i-\theta)^2}\nonumber\\
% &\propto& \lambda^{n/2} \e{-\frac{\lambda}{2}\lrp{n\theta^2 -2 n\theta \bar{y} + \sum_{i}y_i^2 }}.\label{eq:liknorm}
% \eea
\esh

Hence, using the resulting equations in \eqref{eq:prnormg} and \eqref{eq:liknorm}, the posterior density for $\btheta,\blambda$ is given by

\bsh
\vspace{12cm}
% \bea
% p(\theta,\lambda\given y_{1:n}) &\propto& p(y_{1:n}\given \theta,\lambda) p(\theta,\lambda)\nonumber\\
% &\propto& \lrp{\lambda^{n/2} \e{-\frac{\lambda}{2}(n\theta^2 -2 n\theta \bar{y} +  \sum_{i}y_i^2 }} \lrp{\lambda^{1/2+\alpha-1} \e{-\frac{\lambda}{2}(\nu\theta^2-2\nu\theta\mu_0+\nu\mu_0^2+2\beta}}\nonumber\\
% &\propto& \lambda^{1/2+\alpha+n/2 -1}\e{-\frac{\lambda}{2}\lrp{\theta^2(n+\nu)-2\theta(n\bar{y}+\nu\mu_0)+\lrp{\sum_i y_i^2 +\nu \mu_0^2+2\beta} }}\nonumber\\
% &\propto& \lambda^{1/2}\e{-\frac{\lambda (n+\nu)}{2}\lrp{\theta-\frac{1}{(n+\nu)}(n\bar{y}+\nu\mu_0)}^2}\times{}\nonumber\\
% &&\quad\quad \lambda^{\alpha+n/2 -1}\e{-\frac{\lambda}{2}\lrp{\lrp{\sum_i y_i^2 +\nu \mu_0^2+2\beta} -\frac{1}{(n+\nu)}\lrp{n\bar{y}+\nu\mu_0}^2} }\nonumber\\
% &\propto& \lrp{\lambda^{1/2}\e{-\frac{\lambda \nu^\star}{2}\lrp{\theta-\mu^\star}^2}} \lrp{\lambda^{\alpha^\star -1}\e{-\lambda \beta^\star }}\label{eq:postnormg}%\nonumber\\
% %&\propto& \N(\theta \given \mu^\star, \nu^\star \lambda)\text{Gamma}(\lambda\given \alpha^\star,\beta^\star),\label{eq:postnormg}
% \eea
\esh
where
\bit
\item $\mu^\star=\frac{n}{\nu+n}\bar{y}+\frac{\nu}{\nu+n}\mu_0$
\item $\nu^\star=\nu+n$
\item $\alpha^\star=\alpha+n/2$
\item $\beta^\star=\frac{1}{2} (\sum_i y_i^2+\nu \mu_0^2+2\beta-\nu^\star {\mu^\star}^2)$
\eit

\bsh
\note It is interesting to note that $\beta^\star$ can be alternatively expressed as
$$\beta^\star=\beta + \frac{1}{2}\sum_i (y_i-\bar{y})^2+\frac{1}{2}\frac{\nu n}{\nu+n}(\bar{y}-\mu_0)^2,$$
which enables viewing more clearly how $\beta^\star$ (the posterior rate parameter for $\blambda$) is constituted. It is a linear combination of the prior variation, the observed variation, and the variation between the observed mean and the mean prior beliefs.
\esh

So, in summary, we have that
\bea
\blambda \given y_{1:n} &\sim& \text{Gamma}(\alpha^\star,\beta^\star)\nonumber\\
\btheta\given \lambda, y_{1:n} &\sim& \N(\mu^\star,(\nu^\star\lambda)^{-1}),
\eea
or equivalently
\bean
p(\theta,\lambda\given y_{1:n})&=&p(\lambda\given y_{1:n})p(\theta\given \lambda, y_{1:n})\\
&=&\text{Gamma}(\lambda \given  \alpha^\star,\beta^\star) \;\N(\theta\given \mu^\star,(\nu^\star\lambda)^{-1})\\
&=&\text{Normal-Gamma}(\mu^\star,\nu^\star,\alpha^\star,\beta^\star).
\eean

\subsection{Example: The pigmaleon effect}

Can the expectations for how a student will perform influence her/his achievement? In a famous study, Rosenthal and Jacobson (1968) performed an experiment in a California elementary school to try to answer this question. At the beginning of the year, all students were given an IQ test. For each class, the researchers randomly selected around 20\% of the students, and told the teacher that these students were ``spurters'' that could be expected to perform particularly well that year. (This was not based on the test--the spurters were randomly chosen). At the end of the year, all students were given another IQ test. The change in IQ score for the first-grade students was recorded.

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=6, fig.height=4, fig.align='center'>>=
library(ggplot2)
pyg.data <- Sleuth3::ex1321
(summ.pyg <- with(pyg.data,by(data = Gain, INDICES = Treatment,
                 FUN = function(x){
                   data.frame(n=round(length(x),0),mean=mean(x),sd=sd(x))})))

ggplot(data=pyg.data, aes(Gain,fill=Treatment)) +
  geom_histogram(alpha=0.2) +
  xlab("IQ test score gain") +
  ylab("# of students")
@
\label{fig:histPyg}\caption{Histogram for the change in IQ score for students under the two treatment regimes.}
\end{figure}

The original data from the experiment is not available; however, the dataset \textsf{ex1321} in the R package \textsf{Sleuth3} was generated by simulating data that matched the summary statistics from the data of the original experiment. In this dataset, students referred to as ``spurters'' are labeled \emph{pygmalion} (after the Greek myth of Pygmalion, a sculptor who fell in love with a statue he had carved) and the rest are labeled \emph{control}.

From the histograms in Figure \ref{fig:histPyg} it seems that the mean for the \emph{pygmalion} group was higher.  We may test this formally. The distributions do not look perfectly normal, but for the sake of argument let's push through with the normality assumption. The problem setup under this assumption is
\bean
\text{Pygmalion: }X_1,\ldots,X_{n_p} &\overset{iid}{\sim}& \N(\mu_p,\lambda_p^{-1})\\
\text{Control: }Y_1,\ldots,Y_{n_c} &\overset{iid}{\sim}& \N(\mu_c,\lambda_c^{-1})
\eean
We are interested in the difference between the means—in particular, is $\mu_p > \mu_c$ ? We don’t know the standard deviations $\sigma_p = \lambda_p^{-1/2}$ and $\sigma_c = \lambda_c^{-1/2}$. The frequentist approach to this problem is rather complicated when $\sigma_p\not=\sigma_c$ (involving approximate t-distributions based on the Welch–Satterthwaite degrees of freedom).

Conversely, from the Bayesian perspective all we need to determine is $$\Pr(\bmu_p>\bmu_c\given \text{data}).$$
Let's consider using a conjugate prior for the means and variances then, and assume that, a priori, the parameters for the two groups have the same distribution and are independent across groups.  Their priors are given by
\bean
\text{Pygmalion: }\bmu_p,\blambda_p &\sim& \text{Normal-Gamma}(\mu_0,\nu,\alpha,\beta)\\
\text{Control: }\bmu_c,\blambda_c &\sim& \text{Normal-Gamma}(\mu_0,\nu,\alpha,\beta).
\eean
All left to do is to assign values for the parameters in the prior (a.k.a. hyperparameters). We use subjective prior knowledge to specify these, as
\bit
\item $\mu_0$: we don't know whether students will improve or not, on average.
\item $\nu=1$: we are uncertain about how big the mean change will be, setting it to 1 is equivalent to assigning  the strength of one observation to the information coming from the prior on $\mu_p$ and $\mu_c$.
\item $\alpha=1/2$: refelcts uncertainty about how much the standard deviation of the changes is
\item $\beta=100\alpha$ this value leads to a expected standard deviation of the changes in the scores of about 10 points, since $\sqrt{E(\lambda)}=\sqrt{\alpha/\beta}=1/10$, and so $\sigma\approx 10$ a priori.
\eit

\bsh
\note {\bemph{How to check whether a prior conforms to our beliefs?}}
\benum
\item Draw some samples from the prior and look at them—this is probably the best general strategy.
\item It's also a good idea to look at sample hypothetical datasets $x_{1:n_p}, y_{1:n_c}$ drawn using these sampled parameter values.
\item Plot the cdf and check various quantiles (first quartile, median, third quartile), if univariate.
\item Plot the pdf, but beware—it can be misleading.
\item Look at various moments (e.g., mean, standard deviation), but beware—they can be misleading.
\eenum
\esh


\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=6, fig.height=4, fig.align='center'>>=
#hyperparameters
mu0 <- 0
nu <- 1
alpha <- 1/2
beta <- 100*alpha

lambda.pr.sample <- rgamma(500,shape=alpha,rate=beta)
mu.pr.sample <- rnorm(500,mean=0,sd=1/sqrt(lambda.pr.sample))

plot(x=mu.pr.sample,y=sqrt(1/lambda.pr.sample),
     panel.first = grid(ny=8,nx=12,col=grey(0.6)),
     pch = 20, col = "forestgreen",cex=0.6,cex.lab=0.8,
     ylim=c(0,40), xlim=c(-50,50),
     xlab=expression(paste(mu," (mean change in IQ score)")),
     ylab=expression(paste(lambda^{-1/2}," (std. dev change)") ))
@
\label{fig:prPyg}\caption{Samples from the Normal-Gamma prior selected.}
\end{figure}

<<>>=
#data
x <- pyg.data$Gain[pyg.data$Treatment=="pygmalion"]
y <- pyg.data$Gain[pyg.data$Treatment=="control"]

#data summaries
np <- summ.pyg$pygmalion$n
x.bar <- summ.pyg$pygmalion$mean

nc <- summ.pyg$control$n
y.bar <- summ.pyg$control$mean

#posterior parameters
mu.p.s <- (np/(nu+np))*x.bar
mu.c.s <- (nc/(nu+nc))*y.bar

nu.p <- nu+np
nu.c <- nu+nc

alpha.p <- alpha+np/2
alpha.c <- alpha+nc/2

beta.p <- 0.5*(sum(x^2)+2*beta-nu.p*mu.p.s^2)
beta.c <- 0.5*(sum(y^2)+2*beta-nu.c*mu.c.s^2)
@

From the derivations in the previous section we have that the posterior distributions for $\bmu_p,\blambda_p$ and for $\bmu_c,\blambda_c$ are given by
\bean
\text{Pygmalion: }\bmu_p,\blambda_p &\sim& \text{Normal-Gamma}(\mu_p^\star,\nu_p^\star,\alpha_p^\star,\beta_p^\star)\\
\text{Control: }\bmu_c,\blambda_c &\sim& \text{Normal-Gamma}(\mu_c^\star,\nu_c^\star,\alpha_c^\star,\beta_c^\star),
\eean
where

\begin{center}
\begin{tabular}{r | r}
Pygmalion & Control\\
\hline
$\mu_p^\star=\frac{n_p}{\nu+n_p}\bar{x}+\frac{\nu}{\nu+n_p}\mu_0=\Sexpr{round(mu.p.s,2)}$ & $\mu_c^\star=\frac{n_c}{\nu+n_c}\bar{y}+\frac{\nu}{\nu+n_c}\mu_0=\Sexpr{round(mu.c.s,2)}$ \\
$\nu_p^\star=\nu+n_p=\Sexpr{nu.p}$ &   $\nu_c^\star=\nu+n_c=\Sexpr{nu.c}$\\
$\alpha_p^\star=\alpha+n_p/2=\Sexpr{alpha.p}$ &  $\alpha_c^\star=\alpha+n_c/2=\Sexpr{alpha.c}$  \\
$\beta_p^\star=\frac{1}{2} (\sum_i x_i^2+\nu \mu_0^2+2\beta-\nu_p^\star {\mu_p^\star}^2)=\Sexpr{round(beta.p,1)}$ & $\beta_c^\star=\frac{1}{2} (\sum_i y_i^2+\nu \mu_0^2+2\beta-\nu_c^\star {\mu_c^\star}^2)=\Sexpr{round(beta.c,1)}$
\end{tabular}
\end{center}

\begin{figure}[H]
<<echo=F, message=F, warning=F, fig.width=6, fig.height=4, fig.align='center'>>=
nsim <- 10^4
lambda.post.p <- rgamma(nsim,shape=alpha.p,rate=beta.p)
lambda.post.c <- rgamma(nsim,shape=alpha.c,rate=beta.c)
mu.post.p <- rnorm(nsim,mean=mu.p.s,sd=1/sqrt(nu.p*lambda.post.p))
mu.post.c <- rnorm(nsim,mean=mu.c.s,sd=1/sqrt(nu.c*lambda.post.c))

prob.pgreaterc <- mean(mu.post.p>mu.post.c)

plot(x=mu.post.c,y=sqrt(1/lambda.post.p),
     grid(ny=8,nx=12,col=grey(0.6)),
     pch = 1, col = "red",cex=0.6,cex.lab=0.8,
     ylim=c(10,20), xlim=c(0,20),
     xlab=expression(paste(mu," (mean change in IQ score)")),
     ylab=expression(paste(lambda^{-1/2}," (std. dev change)") ))
points(x=mu.post.p,y=sqrt(1/lambda.post.c),
       col="cornflowerblue",cex=0.6, pch=2)
legend("topleft",pch=c(1,2),col=c("red","cornflowerblue"),
       legend=c("control","pygmalion"),bty="n")
@
\label{fig:postPyg}\caption{Samples drawn from the Normal-Gamma posterior densities obtained.}
\end{figure}

Figure \ref{fig:postPyg} shows a scatterplot of samples from the posteriors. Now, we can answer our original question: ``What is the posterior probability that $\mu_p > \mu_c$?'' The easiest way to do this is to take a bunch of samples from each of the posteriors, and see what fraction of times we have $\mu_p > \mu_c$ . This is an example of a Monte Carlo approximation (much more to come on this in the future). To do this, we draw $\text{nsim} = 10^4$ samples from each posterior:
\bean
\text{Pygmalion: }\bmu_p,\blambda_p &\sim& \text{Normal-Gamma}(\Sexpr{round(mu.p.s,1)},\Sexpr{nu.p},\Sexpr{alpha.p},\Sexpr{round(beta.p,1)})\\
\text{Control: }\bmu_c,\blambda_c &\sim& \text{Normal-Gamma}(\Sexpr{round(mu.c.s,1)},\Sexpr{nu.c},\Sexpr{alpha.c},\Sexpr{round(beta.c,1)}),
\eean
and the approximate $\Pr(\bmu_p>\bmu_c \given x_{1:n_p},y_{1:n_c})$ as
$$\Pr(\bmu_p>\bmu_c \given x_{1:n_p},y_{1:n_c}) \approx \frac{1}{\text{nsim}}\sum_{k=1}^{\text{nsim}}\1_{\lrb{\mu_p^{(k)}>\mu_c^{(k)}}}=\Sexpr{round(prob.pgreaterc,3)}$$


This data strongly supports the hypothesis that teachers' expectations play a role in student performance. This is evidenced in both the figure and by the probability calculated, the posterior probability that the \textsf{pygmalion} group has a higher mean change in IQ score is about 0.97.

\subsection{Conjugate prior for the normal mean and variance}

We began our discussion by formulating the normal problem in terms of the mean and the precision, just for the mere convenience.  Nevertheless, we could as well have found a conjugate family of priors in terms of the mean and the variance.

We say that if a random variable $Y\sim \text{Gamma}(a,b)$ with pdf
$$p(y\given a,b)=\frac{b^a}{\Gamma(a)} y^{a-1} e^{-b y} \1_{\lrb{y>0}},$$
then $Z=1/Y$ is InvGamma$(a,b)$ distributed, with pdf given by
$$p(z\given a,b)=\frac{b^a}{\Gamma(a)} z^{-a-1} e^{-\frac{b}{z}} \1_{\lrb{z>0}},$$

Therefore, the good news is that we can use what we have developed so far for the precision, since we have that

$$\lambda\sim \text{Gamma}(a,b)\;\Longrightarrow\;\sigma^2\sim \text{InvGamma}(a,b).$$

So solving the problem for the precision or for the variance is equivalent, but now the joint prior and posterior for $\btheta$ and $\sigma^2$ belong to the InvGamma-Normal family, which is obtained making the obvious modifications.

\end{document}