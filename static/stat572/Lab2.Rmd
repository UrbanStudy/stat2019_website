---
title: 'Bayesian Statistics: Lab 2'
author: ''
date: "October 10, 2019"
output:
  html_document: default
  pdf_document: default
---


# Confidence Regions 

When doing Bayesian inference, it is often convenient to identify regions of the parameter space, which are likely to contain the true parameter value, that is, given $X=x$ finding values $l(x)$ and $u(x)$ in $\Theta$, such that the probability that $l(x)<\boldsymbol{\theta}<u(x)$ is large.  There are a few different of ways to go about this.  

## Background on Confidence Regions

From the frequentist perspective, we may define a 95% random interval $[l(X),u(X)]$ that has 95% **frequentist coverage** for $\theta$, but this interval is established before the data is collected, this interval is such that
$$\Pr(l(X)<\theta<u(X)\, |\, \theta)= 0.95.$$

Remembering that from Bayesian standpoint we always make a decision after having observed the data, that is, having $X=x$, the interval $[l(x),u(x)]$ has 95% **Bayesian Coverage** for $\theta$ if
$$\Pr(l(x)<\boldsymbol{\theta}<u(x)\,|\, X=x)=0.95$$

Because the frequentist coverage formulates the interval before observing the data, while the Bayesian coverage does the so after observing the data, these may be regarded as pre- and post-experimental 95% coverages, respectively.

So, how do we interpret the frequentist coverage after observing the data (i.e., post-experimentally)? By replacing $X=x$, note that 
$$\Pr(l(x)<\theta< u(x)\,|\,\theta)=\left\{\begin{matrix}0 &\text{if }\theta\not\in[l(x),u(x)]\\ 1 &\text{if }\theta\in[l(x),u(x)] \end{matrix}\right..$$
As such, the post-experimental interpretation of the frequentist interval for a single run of the experiment is not very useful; however, if: 

- the experiment is run independently several times, and
- post-experimental intervals $[l(x),u(x)]$ with 95% frequentist coverage are calculated for each of them, then
- the proportion of them that contain the true value of $\theta$ is close to 0.95.

*Interestingly, an interval $[l(x),u(x)]$ that has a 95% Bayesian coverage also has the property that* $$\Pr(l(X)<\theta<u(X) \,|\, \theta) = 0.95+\epsilon_n,$$ with $|\epsilon_n|<a/n$ for some constant $a$ (Hartigan, 1966). 

- So, intervals with 95% Bayesian coverage also have asymptotically 95% frequentist coverage (since $a/n\rightarrow 0$ as $n\rightarrow \infty$).

## Obtaining Bayesian confidence regions (a.k.a. Credible Intervals)

There are at least two alternatives to calculate $1-\alpha$ Bayesian Credible Intervals:

1. Quantile-based intervals: find quantiles $\alpha/2$ and $(1-\alpha/2)$ from the posterior of $\theta \,|\, X=x$ (for a 95% interval, set $\alpha=0.05$). In other words, find $$l(x)=\theta_{\alpha/2}: \Pr(\boldsymbol{\theta}<\theta_{\alpha/2} \,|\, X=x)=\alpha/2\quad\text{and}\quad u(x)=\theta_{1-\alpha/2}: \Pr(\boldsymbol{\theta}>\theta_{1-\alpha/2} \,|\, x)=\alpha/2.$$ 
2. Highest Posterior Density (HPD) Region: find the set $s(x)=[l(x),u(x)] \in \Theta$, such that 
- $\Pr(\boldsymbol{\theta}\in s(x) \,|\, X = x)=\Pr(\boldsymbol{\theta}\in [l(x),u(x)] \,|\, X = x)=1-\alpha$
- All values of $\theta$ contained in the interval $s(x)$ have higher pdf/pmf values than any value outside of $s(x)$.  Another way to express this is by saying that if $\theta_1\in s(x)$, then
$$p(\theta_1\,|\, x)> p(\theta_2\,|\, x)\quad\text{for any }\theta_2\not\in s(x).$$


## Posterior Inference with the Poisson$(\theta)$ likelihood

Enough preambles, let's try this out. Consider one of the problems in homework 1; here is a summary:

- An ecologist records the number of eggs laid in a sample of sparrow nests of size $n = 30$. 
- $Y_1,\ldots , Yn\,|\, \theta\sim  \text{Poisson}(\theta)$ (iid), $Y_i$ is the number of eggs laid in nest $i$. 
- The ecologist is interested in estimating $\theta$, the mean number of eggs per nest in the general population of nests. 
- For now, assume $\theta \in\Theta = \left\{0.5, 1.0, \ldots, 4.5, 5.00 \right\}$.
- $p(\theta) = 1/25$ for each $\theta\in \Theta$.
- Recall that $X=\sum_i Y_i \sim \text{Poisson}(n\theta)$. 
- Then, the posterior for $\theta$ given $X=x$ is
$$p(\theta_j\,|\, x) =\frac{\theta_j^{x} e^{- n \theta_j} \boldsymbol{1}_{\left\{\theta_j \in \Theta\right\}}}{\sum_{k=1}^{5} \theta_k^{x} e^{- n {\theta_k}}\boldsymbol{1}_{\left\{\theta_k \in \Theta\right\}} } $$
Suppose that $x = 26$. Compute a 95% Confidence Interval.

### $\Theta$ is a finite parameter space, let's enumerate all of its values, calculate the posterior probability for each $\theta\in\Theta$ and plot $p(\theta | x)$.

```{r fig.height=5, fig.width=6, fig.align='center'}
x <- 26
n <- 30

#define parameter space Theta (all values that theta can take)
Theta <- seq(0.2,5,by = 0.2)

#calculate the numerator of p(theta | x)
post.theta <- exp(-n*Theta)*(Theta^x)
#renormalize (i.e., divide by the sum to get them to add up to 1)
post.theta <- post.theta/sum(post.theta)

plot(x  = Theta,
     y = post.theta,
     xlab = expression(theta),
     type = "h",
     ylab = expression(paste("p(",theta," | x)")),
     col="blue")
```


### Now calculate the quantile-based 95% credible interval (CI).

When you have a common distribution, such as normal, gamma, or possion, obtaining the quantile based CI is a simple as using the corresponding quantile function, such as 'qnorm', 'qgamma' or 'qpois'. 

Nevertheless, in our particular this is not the case, we have a closed form but it is not hard coded into R, so we will have to do it manually through the following steps:

1. Calculate the cumulative distribution function (cdf) $$F(\theta | X=x)=\Pr(\boldsymbol{\theta} \leq \theta | X=x),$$
2. Find the largest value of $\theta$ such that $F(\theta | X=x)\leq 0.025$, call it $\theta_{0.025}$
3. Find the smallest value of $\theta$ such that $1-F(\theta | X=x)\leq 0.025$, call it $\theta_{0.975}$

We can either do these steps one at a time, or we may do them all with a single line of code.  For completeness, let's first do each step at once, and then we can combine them into a single line of code.

```{r fig.height=5, fig.width=6, fig.align='center'}
#---------------------------------------------------------
#First Strategy: Quantile based 95% CI
#---------------------------------------------------------

##-----step-by-step approach
#1. Calculate cdf (use the function cumsum)
F.theta.x <- cumsum(post.theta)
#2. Get theta_0.025
small.thetas <- (F.theta.x >= 0.025) # find which theta's have F(theta)<=0.025
loc.theta.low <- min(which(small.thetas)) # find position of theta_0.025 in Theta
theta_0.025 <- Theta[loc.theta.low] #get lower limit of CI

#3. Get theta_0.975
big.thetas <- ((1-F.theta.x) <= 0.025) # find which theta's have F(theta)>=0.975
loc.theta.high <- min(which(big.thetas)) # find position of theta_0.975 in Theta
theta_0.975 <- Theta[loc.theta.high] #get upper limit of CI

(Q.CI1 <- c(q2.5=theta_0.025,q97.5=theta_0.975))

##-----one-step approach
(Q.CI2 <- c(q2.5=min(Theta[which(cumsum(post.theta)>=0.025)]),
           # q97.5=Theta[min(which((1-cumsum(post.theta))<=0.025))]))
           q97.5=min(Theta[which(cumsum(post.theta)>=0.975)])) )


##Sanity check
#id positions of l(x) and u(x) in theta.vec
pos.lims.Q <- which(Theta %in% Q.CI2)
#substract one position from lower limit since:
# P(theta \in [l(y),u(y)]) = P(theta <= u(y))- P(theta < l(y))
pos.lims.Q[1] <- pos.lims.Q[1]-1
#get cumulative probs for each limit
cumprobs.lims.Q <- cumsum(post.theta)[pos.lims.Q]
#the difference below gives the actual Bayesian coverage of the CI
diff(cumprobs.lims.Q)

##-----quantiles from simulated values
#generate, for example, 1 million samples from the posterior of theta
theta.samples <- sample(Theta,1000000,prob=post.theta,replace = T)

#use the quantile funtion in R to find the quantiles.
quantile(theta.samples,c(0.025,0.975))

#Let's plot the Quantile Based 95% CI
plot(x  = Theta,
     y = post.theta,
     xlab = expression(theta),
     type = "h",
     ylab = expression(paste("p(",theta," | x)")),
     col="blue")
#abline is used to plot straight lines
abline(v=Q.CI2,col="red",lwd=2,lty=2)
```

### Calculate the 95% CI based on the HPD region.

- To get the HPD region, the strategy consists in sliding a horizontal line (starting from 0) over the posterior pmf curve. 
- The value of this horizontal line, call it $h$, represents a cutoff value for $p(\theta\,|\, x)$.
- For each $h$, we calculate $\sum_{\theta:p(\theta|x)\geq h} p(\theta|x)$ (or integral in the continuous case) of of probability accumulated for all the values of 
- when the horizontal line is at 0, notice and looking at the values

until we find the first pair of $l(y)$ and $u(y)$ such that $P(l(x)\leq \boldsymbol{\theta} \leq u(x) | X=x)\approx 0.95$.  The code for this  one is a little more involved. I'll explain as we go.

```{r fig.height=5, fig.width=6, fig.align='center'}
#---------------------------------------------------------
#Second Strategy: 95% HPD region based CI
#---------------------------------------------------------
cumprob <- 0
pmf <- sort(unique(post.theta),decreasing = T) #order pmf
HPD95 <- range(Theta)
th.vals <- NULL
k <- 1
while(round(cumprob,2)<0.95){
  th.vals <- Theta[post.theta>=pmf[k]]
  HPD95 <- range(th.vals)
  cumprob <- sum(post.theta[post.theta>=pmf[k]])
  k <- k+1
}

#Highest Posterior Density 95% region
names(HPD95) <- c("l(x)","u(x)")
HPD95

##Sanity check
#id positions of l(x) and u(x) in theta.vec
pos.lims.HPD <- which(Theta%in%HPD95)
#move position one to the left of lower limit since:
# P(theta \in [l(y),u(y)]) = P(theta <= u(y))- P(theta < l(y))
pos.lims.HPD[1] <- pos.lims.HPD[1]-1
#get cumulative probs for each limit
cumprobs.lims.HPD <- cumsum(post.theta)[pos.lims.HPD]
#the difference below gives the actual Bayesian coverage of the HPD
diff(cumprobs.lims.HPD)


#Let's plot both the Quantile Based and HPD 95% CI
plot(x  = Theta,
     y = post.theta,
     xlab = expression(theta),
     type = "h",
     ylab = expression(paste("p(",theta," | x)")),
     col="blue")
#abline is used to plot straight lines
abline(v=Q.CI2,col="red",lwd=2,lty=2)
abline(v=HPD95,col="forestgreen",lwd=2,lty=3)
legend("topright",lty=c(2,3),col=c("red","forestgreen"),
       legend=c("Quantile Based","HPD Region"))
```

Notice that neither of the two intervals above actually give exactly 95% Bayesian coverage.  This has to do with the fact the the parameter space is discrete (and there are therefore jumps in the probabilities). Because of this, the 95% HPD region is contained in the quantile based 95% CI.

Nonetheless, obtaining the 95% HPD region is the preferred method to obtain Bayesian CI's because it characterizes the smallest set of values in $\Theta$ that accummulate 95% of the probability mass in the distribution.

## On your own

1. Use the same setup as before for the generating distribution of (i.e., $X | \theta \sim \text{Poisson}(n\theta)$ and $p(\theta)=1/50$ with $\Theta=\{0.1, 0.2, \ldots, 5\}$), but now assume that $x=9$ and $n=40$. Repeat the same analysis as before, obtaining the quantile based 95%CI and the 95% HPD region for $\theta | X=x$.
2. Again consider $X | \theta \sim \text{Poisson}(n\theta)$, with $x=9$ and $n=40$. However, instead of having $\Theta$ be a discrete parameter space with uniform prior, assume that $\Theta=(0,\infty)$ with $\theta\sim \text{Gamma}(a=1.25,b=0.5)$, using the same parametrization for the Gamma density as in the homework. Perform the same analysis as in part 1.
3. Compare the results from parts 1 and 2.
