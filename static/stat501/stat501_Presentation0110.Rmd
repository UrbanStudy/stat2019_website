---
title: 'A Brief Introduction to Data Augmentation'
author: "Shen Qu"
subtitle: 'STAT 501: Statistical Literature and Problems'
# runtime: shiny
output:
  ioslides_presentation: 
    fig_width: 8
    fig_height: 5
    incremental: no
  beamer_presentation: 
  slidy_presentation: 
    duration: 45
header-includes:
- \usepackage{amssymb}
- \usepackage{amsmath}
---




```{r setup, include=F}
knitr::opts_chunk$set(message=F, warning=F, echo=T,cache = F,collapse = F)
options(width = 2000)
options(scipen=6)
options(digits=4)
if (!require(pacman)) {install.packages("pacman"); library(pacman)}
p_load(mixtools,scales,TTR,BayesLogit,mvtnorm,pander) #,EnvStats
# mixtools, for ellipse # scales::alpha #shiny, shinythemes ,
# html_document: 
```

# Question

- Binary response regression models

Observed $Y_1,..,Y_n\sim Bern(p_i)$, i=1,..,n.

Covariates $X=X_1,...X_p$

Desired $\beta=\beta_0,\beta_1,...,\beta_p$



## General Framework

Generalized Linear Models

\[\begin{aligned}
Y=\{0,1\}\qquad\quad&\\
\downarrow\qquad\qquad\quad&\\
Pr(Y_i=1|\beta)=p_i=&H(\eta_i)&\\
&\downarrow\\
&H^{-1}(p_i)=\eta_i=\mathbf{x}_i^T\boldsymbol{\beta}\\
\end{aligned}\]

The link function $H$ is a CDF

---

- Logit models:

\(\begin{aligned}
p_i=Logistic(\eta_i)&=\frac{\exp(\eta_i)}{1+\exp(\eta_i)}\\
Logit(p_i)&=\ln\frac{p_i}{1-p_i}=\eta_i=\mathbf{x}_i^T\boldsymbol{\beta}\\
\end{aligned}\)

($\eta$ is log odds of success)

- Probit models:

\(\begin{aligned}
p_i=\Phi(\eta_i)&=\frac12+\frac12\rm erf(\frac{\eta_i}{\sqrt {2}})\\
\Phi^{-1}(p_i)&=\sqrt {2}\rm erf^{-1}(2p_i-1)=\eta_i=\mathbf{x}_i^T\boldsymbol{\beta}\\
\end{aligned}\)

 the standard normal cdf $\Phi$ is not an elementary function.

## IRLS Algorithm

Iteratively Reweighted Least Squares (MLE)

\[\log L(\boldsymbol{\beta}) = \sum[ y_i \log(\frac{p_i}{1-p_i}) + n_i\log(1-p_i)]\]

Using Fisher Scoring procedure

\[\begin{aligned}
\mathbf{x}_i^T\boldsymbol{\hat\beta}^{(0)}=\hat\eta_i &\longrightarrow \frac{\exp(\hat\eta_i)}{1+\exp(\hat\eta_i)}=\hat p_i\ \downarrow\\
 &\downarrow \ z_i=\hat{\eta_i} + \frac{y_i-n_i\hat{p}_i}{n_i\hat{p}_i(1-\hat{p}_i)}\\
\hat{\boldsymbol{\beta}}^{(1)}=\mathbf{(X'WX)^{-1}X'Wz}&\longleftarrow\mathbf{W}=diag[n_i\hat{p}_i(1-\hat{p}_i)]
\end{aligned}\]

$z$ the adjusted dependent variable.
$\hat{\boldsymbol{\beta}}^{(1)}$ the weighted least squares estimate

<!-- v.s. asymptotic expansions or numerical integration -->
<!-- v.s.??? Newton-Raphson algorithm "gradient descent level II"???   -->

## Bayesian statisitcs

The posterior density

\[\pi(\theta|y)=\frac{f(y|\theta)\cdot\pi(\theta)}{c(y)=\int_{\Theta}f(y|\theta)d\theta}\]

$\pi(\theta)$ is the prior density.

When we can get $c(y)$.

Bayesian estimate $\hat\theta=E[\theta|y]$

Draw many $\theta_i^*\overset{iid}{\sim}\pi(\theta|y)$

$\hat\theta_n=\frac1n\sum_{i=1}^n\theta_i^*\overset{a.s}{\to}\hat\theta$ by SLLN

---

But for our question, $c(y)$ is intractable.

\[\pi(\boldsymbol{\beta}|y) = \frac{\pi(\boldsymbol{\beta})\prod_{i=1}^{n}H (\mathbf{x}_i^T\boldsymbol{\beta})^{y_i}(1-H(\mathbf{x}_i^T\boldsymbol{\beta}))^{1-y_i}}{\int\pi(\boldsymbol{\beta})\prod_{i=1}^{n}H (\mathbf{x}_i^T\boldsymbol{\beta})^{y_i}(1-H(\mathbf{x}_i^T\boldsymbol{\beta}))^{1-y_i}d\boldsymbol{\beta}}\]

The solutions include:

Asymptotic expansions (require large sample size), 

Numerical integration (hard for high-dimension), 

and Monte Carlo integration


## EM algorithm

E-step

\[Q(\theta',\theta)=\int_Z\log[p(\theta'|z,y)]\cdotp(z|y,\theta)dz\]

where $\theta$ is the current guess to the mode of the observed posterior $p(\theta'|y)=\frac1{c(y)} p(\theta)L(y|\theta)$.

$p(\theta'|z,y)$ is the augmented posterior,

$p(z|y,\theta)$ is the conditional predictive distribution of the latent data $Z$, conditional on the current guess to the posterior mode.

---

M-step

\[\frac{\partial Q(\theta',\theta)}{\partial\theta'}\overset{set}{=}0\]

Continue E-step and M-step .... until

\[||\theta'-\theta|| \text{ or } |Q(\theta'',\theta')-Q(\theta',\theta)|<\delta\]


EM algorithm (Dempster et al., 1977). 


## DA Algorithm

the target \(p(\theta|y)=\int_Zp(\theta|z,y)\cdot p(z|y)dz\)

the predictive density $p(z|y)=\int_\Theta p(z|\phi,y)\cdot p(\phi|y)d\phi$

\(\begin{aligned}
p(\theta|y)&=\int_Zp(\theta|z,y)\cdot p(z|y)dz\\
&=\int_Zp(\theta|z,y)\cdot [\int_\Theta p(z|\phi,y)\cdot p(\phi|y)d\phi]\ dz\\
&=\int_\Theta p(\phi|y)\cdot [\int_Zp(\theta|z,y) \cdot p(z|\phi,y)dz]\ d\phi\\
&=\int_\Theta p(\phi|y)\cdot K(\theta,\phi)\ d\phi\\
\end{aligned}\)

where the kernel $K(\theta,\phi)$ is a nonnegative weight function used in density estimation.

# A Data-Augmentation schemes


A well-behaved Markov chain Monte Carlo (MCMC)

Simulating from the proper conditional predictive distribution

Generating the underlying data $Z$

## Motivation

Assume pdf $f_X(x):\mathbb{R}^p \to [0,\infty)$, function $g: \mathbb{R}^p \to \mathbb{R}$

want to estimate $E[g(x)]$.

\[E_{f_X}[g(x)]= \int_{\mathbb{R}^p} g(x) f_X(x) dx\]

When $E[g(x)]$ is hard to numerical integral or analytical approximate, 

we can use simulation based methods.

## Monte Carlo Sampling


Regardless of the distribution, if we have $g(X_1),g(X_2),\ldots,g(X_m) \overset{iid}{\sim} f_X(x)$ 

then $\frac{1}{m}\sum_{i=1}^m g(X_i)$ is a good estimator for $E(g)$.

\[E_{f_{X}}[g(x)]\approx \frac{1}{m}\sum_{i=1}^{m} g(X_{i})\]


- Consistency: 

If $E[|g|]<\infty$, then $\frac{1}{m}\sum_{i=1}^m g(X_i) \overset{a.s.}\longrightarrow E_{f_{X}}[g]$

- Unbiasedness: 

$E[\frac{1}{m}\sum g(X_i)]=E[g(X)]$


## Monte Carlo Markov chain (MCMC)

- 'Current' and 'next'

$\theta\in\Theta\subseteq\mathbb{R}^k$, where $k$ may be "high" dimension

\(\pi(\theta|y)=\frac{f(y|\theta)\pi(\theta)}{c(y)}\)
when $c(y)=\int f(y|\theta)\pi(\theta) d\theta$ is hard to compute, 

Define $\{X_n\}\in E$, $A\in \mathcal{J}$. $\mathcal{J}$ is a $\delta$-field on $E$, 

if \(\scriptsize{P(X_n=i_n|X_1=i_1,..,X_{n-1}=i_{n-1})=P(X_n=i_n|X_{n-1}=i_{n-1})}\)

or \(P(X_n\in A|X_1,...,X_{n-1})=P(X_n\in A|X_{n-1})\)

then $\{X_n\}$ is a M.C.



## DA algorithm

When it is impossible to just simulate from $f_X(x)$, require the conditions:

 1. the invariant x-marginal pdf/pmf. 
 \[f_X(x)=\int_{\mathbb{R}^q} f(x,z)dz\]
 
 2. the applicable conditional pdf/pmf.
 \[f_{X|Z}(x|z)=\frac{f_{X,Z}(x,z)}{f_Z(z)}\]

and $f_{Z|X}(z|x)$.

---

- Constructing a Markov chain, one iteration includes:

1. Draw $Z \sim f_{Z |X}(·|x)$.

2. Draw $X_{i+1} \sim f_{X|Z} (·|z)$.

Repeat to simulate $f_X(x)$

Tanner and Wong (1987), Swendsen and Wang (1987)

```{r,eval=T, echo=F}
# functions of cumulative standard diviation
# 1 runSD(t_EM[,1], n=1, cumulative=TRUE)
# 2
biasedSd<-function(data){sqrt(mean((data-mean(data))^2))}
cumSd<-function(data){sapply(Reduce(c,data,accumulate = T), biasedSd)}
# cumSd(t_EM[,1])
# sd(t_EM[,1])
cumean<- function(x){cumsum(x)/seq_along(x)} 
```

## Ex1: Bivariate Normal Density

```{r,echo=F,eval=F}
# method 1
rbvn<-function (n, m1, s1, m2, s2, rho){
     X1 <- rnorm(n, mu1, s1)
     X2 <- rnorm(n, mu2 + (s2/s1) * rho *
           (X1 - mu1), sqrt((1 - rho^2)*s2^2))
     cbind(X1, X2)
}
bvn <- rbvn(N,mu1,s1,mu2,s2,rho)
# method 2
bvn <- MASS::mvrnorm(N, mu = mu, Sigma = sigma )
# method 3
M <- t(chol(sigma)) # M %*% t(M)
Z <- matrix(rnorm(2*N),2,N) # 2 rows, N/2 columns
bvn <- t(M %*% Z) + matrix(rep(mu,N), byrow=TRUE,ncol=2)

colnames(bvn) <- c("bvn_X1","bvn_X2")
```


```{r,echo=F}
# Function to draw ellipse for bivariate normal data
ellipse_bvn <- function(bvn, alpha){
  Xbar <- apply(bvn,2,mean)
  S <- cov(bvn)
  ellipse(Xbar, S, alpha = alpha, col="red")
}
```

```{r echo = FALSE}
# Target parameters for univariate normal distributions
N <- 20000 ;mu1 <- 0; s1 <- 1; mu2 <- 0; s2 <- 1; rho <- 1/sqrt(2)
# Parameters for bivariate normal distribution
mu <- c(mu1,mu2); s <- c(s1,s2) # Mean
sigma <- matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),2) # Covariance matrix
```



The unknown true $X \sim N(0,1)$; $Z \sim N(0,1)$

Exist $(X,Z) \sim N_2(0,1,\frac{1}{\sqrt 2})$ and 

$f_X(x)=\int_{\mathbb{R}^q} f(x,z)dz$

We know $f(Z|X=x)$ and $f(X|Z=z)$

1. Draw $(Z|X=x) \sim N(\frac{x}{\sqrt 2},\frac12)$.

2. Draw $(X|Z=z) \sim N(\frac{z}{\sqrt 2},\frac12)$.


```{r,echo=F}
g.bvn<-function (n, mu1, s1, mu2, s2, rho,step=20){
  x <- 0; y <- 0; 
  mat      <- matrix(ncol=2,nrow = n)
  mat[1, ] <- c(x, y)
  for (i in 2:n) {
    x <- rnorm(1, mu1+(s1/s2)*rho*(y-mu2), sqrt((1-rho^2)*s1^2))
    y <- rnorm(1, mu2+(s2/s1)*rho*(x-mu1), sqrt((1-rho^2)*s2^2))
    mat[i, ] <- c(x, y)} 
  colnames(mat) <- c("X","Y"); 
  thinned=seq(round(n*0.2),n,step)  
  mat[thinned,]
  }
```



---

```{r,eval=T,echo=F}
bvn <- g.bvn(N, mu1, s1, mu2, s2, rho)
```


```{r,eval=T,echo=F}
par(mfrow=c(2,2))

plot(bvn,type = "n", xlab="X",ylab="Y")# 
points(bvn,col=alpha("steelblue", 0.4),pch=1,cex = 0.1)
ellipse_bvn(bvn,.05)
#ellipse_bvn(bvn,.5)
# plot(bvn,type="l",col=alpha("steelblue", 0.4),lwd=.1)
plot(cumean(bvn[,1]),main=bquote(mu[.(1)]),
     type="l",xlab="index",ylab="running average",col="cornflowerblue")
  abline(h=mu1,col="red")
hist(bvn[,1],40,freq = F, xlab="X",main= "") # xlab=bquote(X[.(1)])
curve(dnorm(x,mu1,s1),add = T,col=2)
# lines(density(bvn[,i]), col = "steelblue", lwd = 2)
hist(bvn[,2],40,freq = F, xlab="Y",main= "") # xlab=bquote(X[.(1)])
curve(dnorm(x,mu2,s2),add = T,col=2)
```


# Conditions and Properties

Harris ergodic, which satisfies three properties:
irreducible, aperiodic, and recurrent.

A sufficient condition for Harris ergodicity is

$$\mathcal{K}:k(x'|x)>0\quad\forall x', x\in\mathbf{X}$$

## Definition

A Markov chain, $X = \{X_i\}^\infty_{i=0}$,with state space X. 

If the current state of the chain is $X = x$, then the density of the next state, $X'$, is $k(x'|x)$.

The Markov transition density (Mtd) is

$$k(x'|x)=\int_Z f_{X|Z}(x'|z)f_{Z|X}(z|x) dz$$

---

Check $k(x'|x)$ is a pdf:

$$\begin{aligned}
\int_Xk(x'|x)dx'&=\int_X\left[\int_Z f_{X|Z}(x'|z)f_{Z|X}(z|x) dz\right] dx'\\
&=\int_Zf_{Z|X}(z|x) \left[\int_X f_{X|Z}(x'|z)dx'\right] dz\\
&=\int_Zf_{Z|X}(z|x)dz=1
\end{aligned}$$

## Invariant (stationarity)

$f_X$ is an invariant density for $K$ when

$$f_{X}(x') =\int_X k(x'|x)f_{X}(x)dx$$

The 1-step transition probability $k(x'|x)$ does not depend on $n$,
then the Markov chain is time/step homogeneous.

Aperiodic: Any state that can come back with positive probability.

$\pi$-irreducible: Any states $x$, $x'$, $\pi(x)>0$, $\pi(x')>0$

## Symmatric

\(\begin{aligned}
k(x'|x)f_{X}(x)&=\int_Z f_{X|Z}(x'|z)f_{Z|X}(z|x) dzf_{X}(x)\\
&=\int_{Z} \frac{f(x',z)}{f_Z(z)}\cdot\frac{f(z,x)}{f_X(x)}f_{X}(x) dz\\
&=\int_{Z} \frac{f(x,z)}{f_Z(z)}\cdot\frac{f(z,x')}{f_X(x')}f_{X}(x') dz\\
&=k(x|x')f_{X}(x')
\end{aligned}\)

Thus the MC $X$ is reversible with respect to $f_X$

## Detailed balance

For all $x, x'\in \mathbf{X}$, let

\[r=\min\left(1,\frac{f(x|x')f_X(x')}{f(x'|x)f_X(x)}\right)\]

The MCMC is called Gibb's sampler if $r\equiv1$.



## Convergence

For a well-behaved Markov chain X, the marginal density of $X_i$
will *converge* to the invariant density $f_X$ no matter how the chain is started.

$$\frac{1}{m}\sum_{i=1}^m g(X_i)\overset{a.s}{=}E_{f_X}[g(x)]$$
s.t. $P\left(\lim\limits_{n\to\infty}|\frac{1}{m}\sum_{i=1}^m g(X_i)-E_{f_X}[g(x)]|<\varepsilon\right)=1$


 Geometric ergodicity and CLT


## Ex4:  Location-scale student's t

The unknown true $Y_i\sim t_{\nu=4,\mu,\sigma^2}$,i=1,..,m,
\[p(y|\mu,\sigma^2)=\frac{\Gamma(\frac{\nu+1}2)}{\sqrt{\nu\sigma^2}\Gamma(\frac{\nu}2)}(1+\frac{(y-\mu)^2}{\nu\sigma^2})^{-\frac{\nu+1}2}\]

But we know 

\[(Y_{i}|Z_{i},\mu,\sigma^2)\sim N(\mu,\frac{\sigma^2}{z_i}), \]
\[(Z_{i}|\mu,\sigma^2) \sim Gamma(\frac\nu2,\frac\nu2)\]

---

$p(z,y|\mu,\sigma^2)=p(y|z,\mu,\sigma^2)p(z|\mu,\sigma^2)$
$=\prod_{i=1}^m\underbrace{\frac{\sqrt{z_i}}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{z_i}{2\sigma^2}(y_i-\mu)^2\right]}_{N(\mu,\sigma^2/z_i)}\underbrace{\frac{(\frac{\nu}2)^{(\frac{\nu}2)}}{\Gamma(\frac{\nu}2)}z_i^{\frac{\nu}2-1}\exp\left[-\frac{\nu}{2}z_i\right]}_{Gamma(\nu/2,\nu/2)}$


$=\prod_{i=1}^m\underbrace{\frac{\Gamma(\frac{\nu+1}2)}{\sqrt{\pi\nu\sigma^2}\Gamma(\frac{\nu}2)}(\frac{(y_i-\mu)^2}{\nu\sigma^2}+1)^{-\frac{\nu+1}2}}_{p(y_i|\mu,\sigma^2)\sim t_{\nu,\mu,\sigma^2}}$

$\times\underbrace{\frac{\left[\frac{(y_i-\mu)^2}{2\sigma^2}+\frac{\nu}{2}\right]^{(\frac{\nu+1}2)}}{\Gamma(\frac{\nu+1}2)}z_i^{\frac{\nu+1}2-1}\exp\left[-\left(\frac{(y_i-\mu)^2}{2\sigma^2}+\frac{\nu}{2}\right)z_i\right]}_{p(z_i|y_i,\mu,\sigma^2)\sim Gamma\left(\frac{\nu+1}2,\frac{(y_i-\mu)^2}{2\sigma^2}+\frac{\nu}2\right)}$

---

Check invariant property

$$\begin{aligned}\int_Z p(z,y|\mu,\sigma^2)dz_i=&\int_{\mathbb{R}_{+}} p(z_i|y_i,\mu,\sigma^2)p(y_i|\mu,\sigma^2)dz_i\\
=&p(y_i|\mu,\sigma^2)\int_{\mathbb{R}_{+}} p(z_i|y_i,\mu,\sigma^2)dz_i\\
=&p(y_i|\mu,\sigma^2)
\end{aligned}$$

---

$$\begin{aligned}
l(\mu,\sigma^2)=&\log[p(y|z,\mu,\sigma^2)]\\
=&\log\prod_{i=1}^m\frac{\sqrt{z_i}}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{z_i}{2\sigma^2}(y_i-\mu)^2\right]\\
=&\log(2\pi)^{-\frac{m}{2}}-\frac{1}{2}\sum\log(\frac{\sigma^2}{z_i})-\frac{\sum z_i(y_i-\mu)^2}{2\sigma^2}\\
\frac{\partial l(\mu,\sigma^2)}{\partial\mu}=&\frac{1}{\sigma^2}\sum_{i=1}^mz_i(y_i-\mu)\overset{set}{=}0\\
\frac{\partial l(\mu,\sigma^2)}{\partial\sigma^2}=&-\frac{1}{2}\sum_{i=1}^m\frac{z_i}{\sigma^2}+\frac{1}{2\sigma^4}\sum z_i(y_i-\mu)^2\overset{set}{=}0\\
\implies\hat\mu=&\frac{1}{z_{.}}\sum_{i=1}^mz_iy_i; \quad\hat\sigma^2=\frac{1}{z_{.}}\sum_{i=1}^mz_i(y_i-\hat\mu)^2\\
\end{aligned}$$


---

\(\begin{aligned}
&p(\mu,\sigma^2,z|y)\\
&\propto\pi(\mu,\sigma^2)p(z,y|\mu,\sigma^2)=\frac{1}{\sigma^2}p(z,y|\mu,\sigma^2)\\
&\propto\frac{1}{\sigma^2}\prod_{i=1}^m\frac{\sqrt{z_i}}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{z_i}{2\sigma^2}(y_i-\mu)^2\right]\\
&\propto(\frac{\sum z_i}{\sigma^2})^{\frac12}(\frac{1}{\sigma^2})^{\frac{m+1}2}\exp\left[-\frac{\sum z_iy_i^2-2\sum z_iy_i\mu+\sum z_i\mu^2}{2\sigma^2}\right]\\
&\propto\underbrace{(\frac{z_.}{\sigma^2})^{\frac12}\exp\left[-\frac{z_.}{2\sigma^2}\left(\mu^2-2\frac{\sum z_iy_i}{z_.}\mu+(\frac{\sum z_iy_i}{z_.})^2\right)\right]}_{N(\hat\mu,\sigma^2/z_.)}\\
&\times\underbrace{(\frac{1}{\sigma^2})^{\frac{m-1}2+1}\exp\left[-\frac{1}{2\sigma^2}(\sum z_iy_i^2-z_.\hat\mu^2)\right]}_{IG(\frac{m-1}2,z_{.}\hat\sigma^2/2)}\\
\end{aligned}\)

---

Check

\(\begin{aligned}
z_{.}\hat\sigma^2=&\sum z_i(y_i-\hat\mu)^2\\
=&\sum z_iy_i^2-2\sum z_iy_i\hat\mu+\sum z_{i}\hat\mu^2\\
=&\sum z_iy_i^2-2z_{.}\hat\mu\hat\mu+z_{.}\hat\mu^2\\
=&\sum z_iy_i^2-z_{.}\hat\mu^2
\end{aligned}\)

Then we get the augmented conditional posterior pdf.

\[\begin{aligned}
\pi(\mu|\sigma^2,y,z)&\sim N(\hat\mu,\frac{\sigma^2}{z_{.}})\quad \square\\
\pi(\sigma^2|y,z)&\sim IG(\frac{m-1}2,\frac{z_{.}{\hat\sigma^2}}2)\quad \square
\end{aligned}\]

---



1. Draw $(z_i|\mu,\sigma^2,y) \sim Gamma(\frac{\nu+1}2,\frac{(y_i-\mu)^2}{2\sigma^2}+\frac{\nu}2)$.

   $\hat\mu=\frac{1}{z_{.}}\sum_{j=1}^mz_jy_j$,  
   
   ${\hat\sigma^2}=\frac{1}{z_{.}}\sum_{j=1}^mz_j(y_j-\hat\mu)^2$

2. Draw $({\sigma^2}|z,y) \sim IG(\frac{m-1}2,\frac{z_{.}{\hat\sigma^2}}2)$.

3. Draw $(\mu|{\sigma^2},z,y) \sim N(\hat\mu,\frac{{\sigma^2}}{z_{.}})$.



A more general DA algorithm developed by Meng and van Dyk (1999) 

---

```{r,echo=F}
g.tls<-function (n,nu,y,step=20){
  m       <- length(y)
  z       <- rgamma(m,nu/2,nu/2)
  z.      <- sum(z)
  mu      <- sum(z*y)/z.
sigma.sq  <- sum((y-mu)^2*z)/z.
theta     <- matrix(ncol = 2, nrow = n)  
theta[1,] <- c(mu,sigma.sq)  
  for (i in 1:n) {
          z  <- rgamma(m,(nu+1)/2,rate = ((y-mu)^2/sigma.sq+nu)/2)  
          z. <- sum(z)
      hat.mu <- sum(z*y)/z.
hat.sigma.sq <- sum((y-hat.mu)^2*z)/z.   
    sigma.sq <- 1/rgamma(1, (m-1)/2, rate=(hat.sigma.sq*z./2))    
          mu <- rnorm(1, hat.mu, sqrt(sigma.sq/z.))
  theta[i, ] <- c(mu,sigma.sq)  
  }
  thinned=seq(round(n*0.2),n,step)
  theta[thinned,]
}
```

```{r,eval=T, echo=F}
# mu.j <- sigma.sq.j <- NA
  # for (j in 1:m) {mu.j[j]=z[j]*y[j]}
  # mu=sum(mu.j)/z.
  # for (j in 1:m) {  
  # sigma.sq.j[j]=(y[j]-mu)^2*z[j]
  # }
  # sigma.sq=sum(sigma.sq.j)/z.
  #   for (j in 1:m) {
  #     z[j] <- rgamma(1,(nu+1)/2,((y[j]-mu)^2/sigma.sq+nu)/2)  
  #   }
    # for (j in 1:m) {mu.j[j]=z[j]*y[j]}
    # mu=sum(mu.j)/z.
    # 
    # for (j in 1:m) {  
    # sigma.sq.j[j]=(y[j]-mu)^2*z[j]
    # }
    # sigma.sq=sum(sigma.sq.j)/z.
nu=4
set.seed(123)
y <- ggdist::rstudent_t(30,nu,1,2)
# y <- rnst(1e5, 1000, 5, 13)
# rt.scaled(n, 4, mean = 0, sd = 1)
#  Y[1, ] <- y <- rnorm(m,mu,sigma/y)
N <- 20000
t_EM<- g.tls(N,nu,y)
```



```{r,eval=T,echo=F}
par(mfrow=c(1,2))
for(i in 1:2){
  cum.est <- cumean(t_EM[,i])
  cum.sd <- cumSd(t_EM[,i])
plot(cum.est,main=c(expression(mu),expression(sigma^2))[i],
     ylim= range(c((cum.est-2*cum.sd),(cum.est+2*cum.sd))),
     type="l",xlab="index",ylab="running average",col="cornflowerblue")
  abline(h=c(1,4)[i],col="red")
  lines(cum.est-1.96*cum.sd,col="cornflowerblue",lty=3)
  lines(cum.est+1.96*cum.sd,col="cornflowerblue",lty=3)
}
```

# Solutions

The latent variable \(\mathbf{Z}=\mathbf{X}\boldsymbol{\beta+\varepsilon}\)

$y_i|z_i=\begin{cases}1&\text{if }z_i>0\\0&\text{o.w.}\end{cases}$

Probit models: \(\boldsymbol{\varepsilon}\sim N_n(\mathbf{0,I}) \)

Logit models: \(\boldsymbol{\varepsilon}\sim Logistic_n(\mathbf{0,I}) \)


## Probit Model

A Gibb's sampler:

$(1) \mathbf{z^*|y},\boldsymbol{\beta}$~$N_{tr}(\mathbf{x}_i^T\boldsymbol{\beta},1)$ 
truncated by 0 at$\begin{cases}\text{left}&y_i=1\\\text{right}&y_i=0\end{cases}$

\((2) \boldsymbol{\beta}|\mathbf{y,z^*}\sim N_p\left( \underbrace{(\mathbf{x}^T\mathbf{x})^{-1}\mathbf{x}^T\mathbf{z^*}}_{m_\beta},\underbrace{(\mathbf{x}^T\mathbf{x})^{-1}}_{v_\beta} \right)\)

Repeat (1) and (2) long enough.
(Albert and Chib, 1993)

Because
\(\pi(\boldsymbol{\beta,Z|y}) = C\pi(\boldsymbol{\beta})\prod_{i=1}^{m}\left[\mathbf{1}_{Z_i>0}\mathbf{1}_{y_i=1}+\mathbf{1}_{Z_i\le0}\mathbf{1}_{y_i=0}\right]\phi (Z_{i})\)

\(\pi(\boldsymbol{\beta|y,Z}) = C\pi(\boldsymbol{\beta})\prod_{i=1}^{m}\phi (Z_{i};\mathbf{x}_i^T\boldsymbol{\beta},1)\)



```{r,echo=F}
# beta.t <- c(-1,1/2,1/4,-1,1)
# beta.name <- c("Beta0","Beta1","Beta2","Beta3","Beta4")

# X.df <- data.frame(x1=rnorm(m,1,1),
#                    x2=rnorm(m,2,1),
#                    x3=sample(c("a","b","c"),m,replace=T))

# X <- matrix(rnorm(m*p), ncol = p)
# cnty <- ceiling(exp(y)) # count data
# X <- matrix(runif(n*p, 5, 10), n)

# p.true <- pnorm(X%*%beta.t)

# Y <- rbinom(m,1,p.true)

```

---

Simulate the observed data

Assume $\boldsymbol{\beta}=\{-1,0.5,0.25\}$

$\begin{bmatrix}X_1\\X_2\end{bmatrix}\sim N_2(\begin{bmatrix}1\\2\end{bmatrix},\begin{bmatrix}1&0\\0&1\end{bmatrix})$

```{r echo=F}
beta.t <- c(-1,1/2,1/4)
beta.name <- c("Beta0","Beta1","Beta2")
m <- 50; p <- 3;#p <- dim(X)[2] 
step=20; set.seed(123)

X.df <- data.frame(x1=rnorm(m,1,1),
                   x2=rnorm(m,2,1))
X <- model.matrix(~.,X.df)

Y <- ifelse(X%*%beta.t+ rnorm(m) > 0, 1, 0) 

head(cbind(Y,X),4)
colMeans(X)
table(Y)
```


```{r, echo=F}
z.cond <- function(beta){
   ez<-(X%*%beta) 
   u<-runif(m,0,1)
   z <- ez + qnorm(ifelse(Y==1,u+(1-u)*pnorm(0,ez,1),
                               u*pnorm(0,ez,1)))
   return(z)
 }

beta.cond = function(z,V,cholV){
   beta <- V%*%(t(X)%*%z)+cholV%*%rnorm(p)
  return(beta)  
}
```


```{r, echo=F}
g.probit<-function (N,X,Y,m,p,step=20){
iXX<-chol2inv(chol(t(X)%*%X)); V<-iXX*(m/(m+1)); cholV<-chol(V) 

beta.ini <- runif(p,-3,3)
   Z     <- matrix(NA,N,m) 
   z.ini <- z.cond(beta.ini)
Beta     <- matrix(NA,nrow=(N+1),ncol=p)
Beta[1,] <- beta.ini <- beta.cond(z.ini,V,cholV)

for(i in 1:N){
   Z[i,]     <- z.cond(Beta[i,])
Beta[(i+1),] <- beta.cond(Z[i,],V,cholV)
}
thinned=seq(round(N*0.2),N,step)  
Beta[thinned,]
}
```


```{r,echo=F}
N <- 25000
beta.p <- g.probit(N,X,Y,m,p)
```

```{r,echo=F}
#beta.p.marg <- mean(dnorm(beta.p%*%t(X)))*beta.p
```

---

- Gibbs' results

```{r,echo=F}
beta.p.mean <- colMeans(beta.p)
beta.p.median <- apply(beta.p,2,quantile,probs=0.5)
beta.p.ll <- apply(beta.p,2,quantile,probs=0.025)
beta.p.ul <- apply(beta.p,2,quantile,probs=0.975)
beta.p.sd <- c(sd(beta.p[,1]),sd(beta.p[,2]),sd(beta.p[,3]))
par.p<- cbind(beta.p.mean,beta.p.ll,beta.p.ul,beta.p.sd) # ,beta.p.median
rownames(par.p) <- beta.name
pander(round((par.p),4))
```


```{r,echo=F}
biasedSd<-function(data){sqrt(mean((data-mean(data))^2))}
cumSd<-function(data){sapply(Reduce(c,data,accumulate = T), biasedSd)}
cumean<- function(x){cumsum(x)/seq_along(x)} 
```


IRLS' results

```{r,echo=F}
fit.p.glm <- glm(Y~.,X.df,family=binomial(link="probit"))
glm.p<- coef(fit.p.glm)
glm.p.sd <- summary(fit.p.glm)$coef[,2] # sqrt(diag(vcov(fit.probit.glm)))
glm.p.ci <- confint(fit.p.glm)
par.glm.p<- cbind(glm.p,glm.p.sd,glm.p.ci)
rownames(par.glm.p) <- beta.name
pander(round((par.glm.p),4))
```

```{r, echo=F}
glm.p.marg <-mean(dnorm(X %*% glm.p))*glm.p
glm.p.ci.marg <-mean(dnorm(X %*% glm.p))*glm.p.ci

beta.p.marg <-mean(dnorm(X %*% beta.p.mean))*beta.p.mean
beta.p.ci.marg <-cbind(ll=mean(dnorm(X %*% beta.p.mean))*beta.p.ll, ul= mean(dnorm(X %*% beta.p.mean))*beta.p.ul)
```


---


```{r,echo=F}
par(mfrow=c(2,p),mar=c(3,3.2,1,.5),mgp=c(1.70,.70,0))
for(s in 1:p){
  plot(beta.p[,s],
       main=bquote(beta[.(s-1)]),type="p",pch=1,cex=0.1,col="cornflowerblue")
  abline(h=beta.t[s],col="red")
  abline(h=glm.p[s],col="green")
}
for(s in 1:p){
  cum.est <- cumean(beta.p[,s])
  cum.sd <- cumSd(beta.p[,s])  
  plot(cum.est,ylim= range(c((cum.est-2*cum.sd),(cum.est+2*cum.sd))),
       #c(beta.p.ll[s]-.01,beta.p.ul[s]+.01),
       main=bquote(beta[.(s-1)]),type="l",xlab="index",ylab="running average",col=1)
  abline(h=beta.t[s],col="red")
  abline(h=glm.p[s],col="green")
  # abline(h=beta.p.ll[s],col="cornflowerblue")
  # abline(h=beta.p.ul[s],col="cornflowerblue")
  lines(cum.est-1.96*cum.sd,col="cornflowerblue",lty=3)
  lines(cum.est+1.96*cum.sd,col="cornflowerblue",lty=3)
}
```



## Logit Model

- Simple, exact, one layer (Polson etal, 2013)


$(1)\quad \boldsymbol{\omega_i^*|\mathbf{y},\beta}\sim PG(n_i,|\mathbf{x}_i^T\boldsymbol{\beta}|)$

\((2)\quad \boldsymbol{\beta|\mathbf{y},\omega^*}\sim N_p\left(\mathbf{m_\omega,v_\omega}\right)\)

where 

the prior $\boldsymbol{\beta}\sim N_p(\mathbf{b,B})$

$\boldsymbol{\Omega}=diag_n(\mathbf{\omega_i})$,

$\mathbf{v_\omega}={\mathbf{(x'\boldsymbol{\Omega} x+B^{-1})^{-1}}}$,

$\mathbf{m_\omega}=\mathbf{v_\omega(x'(y-\frac12)+B^{-1}b)}$, 

Repeat (1) and (2) long enough.


```{r,eval=F, echo=F}
set.seed(1234)
Y <- rbinom(m,1,1/(1+exp(-X%*%beta.t)))
table(Y)
```

---

Introduce Pólya–Gamma

\[f(\omega|\beta)=\cosh{(\frac12\mathbf{x}^T\boldsymbol{\beta})}\exp\left[-\frac12(\mathbf{x}^T\boldsymbol{\beta})^2\omega\right]g(\omega)\]

where $\cosh{(c)}=\frac12(e^c+e^{-c})=\frac{1+e^2c}{2e^{c}}$, 

$g(\omega)$ is free of $\beta$ s.t.

\[g(\omega)=\sum_n^\infty(-1)^n\frac{2n+1}{\sqrt{2\pi\omega^3}}\exp[-\frac{(2n+1)^2}{8\omega}]\mathbb{I}_{(0,\infty)}(\omega)\]

Since $y$ is observed data, \(\pi(\omega|\beta,y)=f(\omega|\beta)\).

---


Assign the prior $\boldsymbol{\beta}$ $\sim N_p(\mathbf{b,B})$

\[\scriptsize{\begin{aligned} 
\pi(\boldsymbol{\beta,\omega|y})&= \frac{\pi(\boldsymbol{\beta})}{C(y)}\prod_{i=1}^{N}[p(y_i|\beta)]f(\omega_i|\beta)\\
&= \frac{\pi(\boldsymbol{\beta})}{C(y)}\prod_{i=1}^{N}\left[\frac{e^{y_i\mathbf{x}_i^T\boldsymbol{\beta}}}{1+e^{\mathbf{x}_i^T\boldsymbol{\beta}}}\right]\cosh{(\frac12\boldsymbol{x_i^T\beta})}\exp\left[-\frac12(\mathbf{x}_i^T\boldsymbol{\beta})^2\omega_i\right]g(\omega_i)\\
&\propto\phi_p(\mathbf{b,B})\prod_{i=1}^{N}\frac{e^{y_i\mathbf{x}_i^T\boldsymbol{\beta}}}{1+e^{\mathbf{x}_i^T\boldsymbol{\beta}}}\cdot\frac{1+e^{\mathbf{x}_i^T\boldsymbol{\beta}}}{2e^{\frac12\mathbf{x}_i^T\boldsymbol{\beta}}}\exp\left[-\frac12(\mathbf{x}_i^T\boldsymbol{\beta})^2\omega\right]\\
&=2^{-n}\phi_p(\mathbf{b,B})\prod_{i=1}^{N}\exp\left[(y_i-\frac12)\mathbf{x}_i^T\boldsymbol{\beta}-\frac12(\mathbf{x}_i^T\boldsymbol{\beta})^2\omega_i\right]\\
\end{aligned}}\]

When $\omega_i$ is known, \(\pi(\boldsymbol{\beta,\omega|y})=\pi(\boldsymbol{\beta|\omega,y})\).

Let $\boldsymbol{\Omega}=diag_n(\mathbf{\omega_i})$; $\boldsymbol{\kappa}=\mathbf{y_{1:n}}-\frac12$

Then we get to conditional pmfs of \(\pi(\boldsymbol{\beta|\omega,y})\)


---


\[\scriptsize{\begin{aligned} 
\pi(\boldsymbol{\beta|\omega,y})&\propto|\mathbf{B}|^{-\frac12}\exp\left\{-\frac12\left[\boldsymbol{\beta'(\mathbf{x'\Omega x+B}^{-1})\beta-2\beta'(\mathbf{x'\kappa+B^{-1}b})}\right]\right\}\\
&\propto|\mathbf{v_\omega}|^{-\frac12}\exp\left\{-\frac12\mathbf{v}_\omega^{-1}[\boldsymbol{\beta'\mathbf{I}\beta-2\beta'\mathbf{v_\omega(x'\kappa+B^{-1}b})]}\right\}\\
&\propto|\mathbf{v_\omega}|^{-\frac12}\exp\left\{-\frac12\boldsymbol{(\beta-\mathbf{m_\omega)'v_\omega^{-1}(\beta-m}_\omega)}\right\}\\
\end{aligned}}\]
where 

\(\mathbf{v_\omega=(x^T\Omega x+B^{-1})^{-1}}\),

\(\mathbf{m_\omega=\underset{(p\cdot p)}{v_\omega}(\underset{(p\cdot n)}{x^T}(\mathbf{y_{1:n}}-\frac12)+B^{-1}b)}\),

Then we confirm that \(\boldsymbol{\beta|\omega,y}\sim N_p(m_\omega,V_\omega)\)


```{r echo=F}
w.cond <- function(beta,m){
   w <- rpg.devroye(num=m, h=1, z=abs(X%*%beta))
   return(w)
 }
```


```{r echo=F}
beta.cond = function(y,w,p,varbeta=100){
   matbetapr= diag(rep(1/varbeta,p))
   OMG      = diag(w);  
   OMGinv   = diag(1/w)
   eta      = OMGinv%*%(y-0.5)
   varmat   = chol2inv(chol(matbetapr+t(X)%*%OMG%*%X)) 
   meanvec  = varmat%*%(t(X)%*%OMG%*%eta)
   beta <- mvtnorm::rmvnorm(1, mean = meanvec, sigma = varmat)
  return(beta)  
 }
```




```{r,echo=F}
g.logit<-function (N,X,Y,step=20){
  p <- dim(X)[2]; m <- dim(X)[1]
  beta.ini <- runif(p,-3,3)
  W        <- matrix(NA,nrow=N,ncol=m)
  w.ini    <- w.cond(beta.ini,m)
  Beta     <- matrix(NA,nrow=(N+1),ncol=p)
  Beta[1,] <- beta.ini <- beta.cond(Y,w.ini,p)
  for(i in 1:N){
    W[i,] = w.cond(Beta[i,],m)
    Beta[(i+1),]=beta.cond(Y,W[i,],p) 
  }
  thinned=seq(round(N*0.2),N,step)  
  Beta[thinned,]
}
```


```{r,echo=F}
set.seed(123)
beta.l <- g.logit(N,X,Y)
```


```{r,eval=F,echo=F}
# beta.l.marg <- mean(dlogis(beta.l[thinned,]%*%t(X)))*(beta.l[thinned,])
# beta.l.marg <- colMeans(apply(beta.l[thinned,]%*%t(X),1,dlogis))*(beta.l[thinned,])
```

---

\scriptsize

Gibbs' results (divided by $\pi/\sqrt{3}$)

```{r,echo=F}
beta.l.mean <- colMeans(beta.l)
beta.l.median <- apply(beta.l,2,quantile,probs=0.5)
beta.l.ll <- apply(beta.l,2,quantile,probs=0.025)
beta.l.ul <- apply(beta.l,2,quantile,probs=0.975)
beta.l.sd <- c(sd(beta.l[,1]),sd(beta.l[,2]),sd(beta.l[,3]))
par.l<- cbind(beta.l.mean, beta.l.sd,beta.l.ll,beta.l.ul)*sqrt(3)/pi #,beta.l.median
rownames(par.l) <- beta.name
pander(round((par.l),4))
```

IRLS' results (divided by $\pi/\sqrt{3}$)

```{r echo=F}
fit.l.glm <- glm(Y~.,X.df,family=binomial(link="logit"))
glm.l<- coef(fit.l.glm)
glm.l.sd <- summary(fit.l.glm)$coef[,2] # sqrt(diag(vcov(fit.l.glm)))
glm.l.ci <- confint(fit.l.glm)
# glm.l.marg <-mean(dlogis(X %*% glm.l))*glm.l
par.glm.l<- cbind(glm.l,glm.l.sd,glm.l.ci)*sqrt(3)/pi
rownames(par.glm.l) <- beta.name
pander(round((par.glm.l),4))
```



```{r echo=F}
glm.l.marg <-mean(dlogis(X %*% glm.l))*glm.l
glm.l.ci.marg <-mean(dlogis(X %*% glm.l))*glm.l.ci

beta.l.marg <-mean(dlogis(X %*% beta.l.mean))*beta.l.mean
beta.l.ci.marg <-cbind(ll=mean(dlogis(X %*% beta.l.mean))*beta.l.ll, ul=mean(dlogis(X %*% beta.l.mean))*beta.l.ul)
```

---

```{r,echo=F}
par(mfrow=c(2,p),mar=c(3,3.2,1,.5),mgp=c(1.70,.70,0))
for(s in 1:p){
  plot(beta.l[,s],
       main=bquote(beta[.(s-1)]),type="p",pch=1,cex=0.1,col="cornflowerblue")
  abline(h=beta.t[s],col="red")
  abline(h=glm.l[s],col="green")
}
for(s in 1:p){
  cum.est <- cumean(beta.l[,s])
  cum.sd <- cumSd(beta.l[,s])  
  plot(cum.est,ylim= range(c((cum.est-2*cum.sd),(cum.est+2*cum.sd))),
       # c(beta.l.ll[s]-.01,beta.l.ul[s]+.01),
       main=bquote(beta[.(s-1)]),type="l",xlab="index",ylab="running average",col=1)
  abline(h=beta.t[s],col="red")
  abline(h=glm.l[s],col="green")
  # abline(h=beta.l.ll[s],col="cornflowerblue")
  # abline(h=beta.l.ul[s],col="cornflowerblue")
  lines(cum.est-1.96*cum.sd,col="cornflowerblue",lty=3)
  lines(cum.est+1.96*cum.sd,col="cornflowerblue",lty=3)
}
```

---

Marginal effects


```{r,echo=F}
mean.all<- cbind(irls.p=glm.p.marg, irls.l=glm.l.marg, gibbs.p=beta.p.marg, gibbs.l=beta.l.marg)
rownames(mean.all) <- beta.name
pander(round((mean.all),4))
```

Confidence Interval

\footnotesize

```{r,echo=F}
sd.all<- cbind(glm.p.ci.marg,glm.l.ci.marg, beta.p.ci.marg,beta.l.ci.marg)
rownames(sd.all) <- beta.name
colnames(sd.all) <- c("i.p.l","i.p.u","i.l.l","i.l.u","g.p.l","g.p.u","g.l.l","g.l.u")
panderOptions('table.split.table', 300)
pander(round((sd.all[,c(1,3,5,7,2,4,6,8)]),4))
```


## literature

Jun S. Liu & Ying Nian Wu (1999) Parameter Expansion for Data Augmentation, Journal of the American Statistical Association, 94:448, 1264-1274, DOI: 10.1080/01621459.1999.10473879



--------------------------------------------------------
random walk                        Markov chain
-------------------               ----------------------
graph                              stochastic process

vertex                             state

strongly connected aperiodic       persistent aperiodic

strongly connected and aperiodic   ergodic

edge weighted undirected graph     time reversible
---------------------------------------------------------

Table 5.1: Correspondence between terminology of random walks and Markov chains

James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer.

---


- Gelman, A. (2014). Bayesian data analysis (Third edition.). CRC Press.

11.7 Bibliographic note

Tanner and Wong (1987) introduced the idea of iterative simulation to many statisticians, using the special case of ‘data augmentation’ to emphasize the analogy to the EM algorithm (see Section 13.4).

  Auxiliary variables
  
12.1 Efficient Gibbs samplers

Gibbs sampler computations can often be simplified or convergence accelerated by adding auxiliary variables, for example indicators for mixture distributions, as described in Chapter 22. The idea of adding variables is also called data augmentation and is often a useful conceptual and computational tool, both for the Gibbs sampler and for the EM algorithm (see Section 13.4).

12.7 Bibliographic note

For the relatively simple ways of improving simulation algorithms mentioned in Sections 12.1 and 12.2, Tanner and Wong (1987) discuss data augmentation and auxiliary variables, and Hills and Smith (1992) and Roberts and Sahu (1997) discuss different parameterizations for the Gibbs sampler. Higdon (1998) discusses some more complicated auxiliary variable methods, and Liu and Wu (1999), van Dyk and Meng (2001), and Liu (2003) present different approaches to parameter expansion. The results on acceptance rates for efficient Metropolis jumping rules appear in Gelman, Roberts, and Gilks (1995); more general results for Metropolis-Hastings algorithms appear in Roberts and Rosenthal (2001) and Brooks,
Giudici, and Roberts (2003).

18.2 Multiple imputation

Any single imputation provides a complete dataset that can be used by a variety of researchers to address a variety of questions. Assuming the imputation model is reasonable, the results from an analysis of the imputed dataset are likely to provide more accurate estimates than would be obtained by discarding data with missing values.
The key idea of multiple imputation is to create more than one set of replacements for the missing values in a dataset. This addresses one of the difficulties of single imputation in that the uncertainty due to nonresponse under a particular missing-data model can be properly reflected. The data augmentation algorithm that is used in this chapter to obtain posterior inference can be viewed as iterative multiple imputation.

  Computation using EM and data augmentation
  
The process of generating missing data imputations usually begins with crude methods of imputation based on approximate models such as MCAR. The initial imputations are used as starting points for iterative mode-finding and simulation algorithms.

Chapters 11 and 13 describe the Gibbs sampler and the EM algorithm in some detail as approaches for drawing simulations and obtaining the posterior mode in complex problems. As was mentioned there, the Gibbs sampler and EM algorithms formalize a fairly old approach to handling missing data: replace missing data by estimated values, estimate model parameters, and perhaps, repeat these two steps several times. Often, a problem with no missing data can be easier to analyze if the dataset is augmented by some unobserved values, which may be thought of as missing data.

Here, we briefly review the EM algorithm and its extensions using the notation of this chapter. Similar ideas apply for the Gibbs sampler, except that the goal is simulation rather than point estimation of the parameters. The algorithms can be applied whether the missing data are ignorable or not by including the missing-data model in the likelihood, as discussed in Chapter 8. For ease of exposition, we assume the missing-data mechanism is ignorable and therefore omit the inclusion indicator I in the following explanation. The generalization to specified nonignorable models is relatively straightforward. We assume that any augmented data, for example, mixture component indicators, are included as part
of ymis. Converting to the notation of Sections 13.4 and 13.5:


18.7 Bibliographic note

Little and Rubin (2002) is a comprehensive text on statistical analysis with missing data.
Van Buuren (2012) is a recent text with a more computational focus. Tanner and Wong
(1987) describe the use of data augmentation to calculate posterior distributions. Schafer
(1997) and Liu (1995) apply data augmentation for multivariate exchangeable models, including the normal, t, and loglinear models discussed briefly in this chapter. The approximate variance estimate in Section 18.2 is derived from the Satterthwaite (1946) approximation; see Rubin (1987a), Meng, Raghunathan, and Rubin (1991), and Meng and Rubin
(1992). Abayomi, Gelman, and Levy (2008) discuss graphical methods for checking the fit
and reasonableness of missing-data imputations.

---

Imai, K., and van Dyk, D. A. (2005). A Bayesian analysis of the multinomial probit model using marginal data augmentation. Journal of Econometrics. 124, 311–334. https://doi.org/10.1016/j.jeconom.2004.02.002

Rubin, D. B. (1987b). A noniterative sampling/importance resampling alternative to the data augmentation algorithm for creating a few imputations when fractions of missing information are modest: The SIR algorithm. Discussion of Tanner and Wong (1987). Journal of the American Statistical Association 82, 543–546.

Tanner, M. A., and Wong, W. H. (1987). The calculation of posterior distributions by data augmentation (with discussion). Journal of the American Statistical Association 82, 528–550. 

van Dyk, D. A., and Meng, X. L. (2001). The art of data augmentation (with discussion). Journal of Computational and Graphical Statistics 10, 1–111.



## Ex2: Simple Slice Sampler (Neal, 2003)

($f_X(x)=3x^2I_{(0,1)}(x)$; $E[X]=\int_{0}^1 xf_X(x)dx=0.75$)

[$f(x,y)=3xI(0<y<x<1)$; $f_X(x)=\int_{0}^x f(x,y)dy$]

$f_Y(y)=\int_{y}^1 f(x,y)dx=\frac32(1-y^2)$

$f_{Y|X}(y|x)=\frac1xI(0<y<x)$

$f_{X|Y}(x|y)=\frac{2x}{1-y^2}I(y<x<1)$

$\int_{y}^xf_{X|Y}(x|y)dx=\frac{x^2-y^2}{1-y^2}I(0<y<x<1)\sim Unif(0,1)$

---

1. Draw $(Y|X=x) \sim Unif(0,x)$ and $U \sim Unif(0,1)$.

2. Update $(X|Y=y,U=u) =\sqrt{u(1-y^2)+y^2}$.


```{r}
g.ex2<-function (n,step=20){
  x <- .5
  y <- .5
  X <- NA
  for (i in 1:n) {
    y <- runif(1,0,x)
    u <- runif(1,0,1)
 X[i] <- x <- sqrt(u*(1-y^2)+y^2)
      }
 thinned=seq(round(n*0.2),n,step)  
 X[thinned]
}
```

```{r,eval=T, echo=F}
ex2<- g.ex2(N)
```

---


```{r,eval=T,echo=F}
par(mfrow=c(1,2))
plot(cumean(ex2),main="",
     type="l",xlab="index",ylab="running average",col="cornflowerblue")
abline(h=3/4,col="red")
hist(ex2,40,freq = F, xlab="",main= "")
curve(3*x^2,add = T,col=2)
```


## Ex3: t: Normal-Gamma

($X\sim t_4$, $f_X(x)=\frac38(1+\frac{x^2}4)^{-\frac52}$)

[$f(x,y)=\frac{4}{\sqrt{2\pi}}y^{\frac32}\exp\{-y(\frac{x^2}{2}+2)\} I_{(0,\infty)}(y)$ ]

1. Draw $(Y|X=x) \sim Gamma(\frac52,\frac{x^2}2+2)$.

2. Draw $(X|Y=y) \sim N(0,y^{-1})$.

```{r}
g.t<-function (n,step=20){
  x <- 1;y <- 1 ; X <- NA
  for (i in 1:n) {
    y <- rgamma(1,5/2,(x^2/2+2))    
    X[i] <- x <- rnorm(1, 0, 1/sqrt(y))
  }
  thinned=seq(round(n*0.2),n,step)  
  X[thinned]
}
```

```{r, eval=T,echo=F}
t_N.G<- g.t(N)
# 2*var(t_N.G)/(var(t_N.G)-1)
```


---


```{r,eval=T,echo=F}
par(mfrow=c(1,2))
plot(cumean(t_N.G),main="",
     type="l",xlab="index",ylab="running average",col="cornflowerblue")
  abline(h=0,col="red")
hist(t_N.G,40,freq = F, xlab="X",main= "",xlim = c(-5,5))
curve(dt(x,4),add = T,col=2)
```


