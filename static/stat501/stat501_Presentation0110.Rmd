---
title: 'A Brief Introduction to Data Augmentation'
author: "Shen Qu"
subtitle: 'STAT 501: Statistical Literature and Problems'
# runtime: shiny
output:
  ioslides_presentation: 
    fig_width: 8
    fig_height: 5
    incremental: no
  beamer_presentation: 
  slidy_presentation: 
    duration: 45
header-includes:
- \usepackage{amssymb}
- \usepackage{amsmath}
---




```{r setup, include=F}
knitr::opts_chunk$set(message=F, warning=F, echo=T,cache = F,collapse = T)
options(width = 2000)
options(scipen=6)
options(digits=4)
if (!require(pacman)) {install.packages("pacman"); library(pacman)}
p_load(mixtools,scales,TTR,BayesLogit,mvtnorm,pander) #,EnvStats
# mixtools, for ellipse # scales::alpha #shiny, shinythemes ,
# html_document: 
```

# Question

- Binary response regression models

Observed $Y_1,..,Y_n\sim Bern(p_i)$, i=1,..,n.

Covariates $X=X_1,...X_p$

Desired $\beta=\beta_0,\beta_1,...,\beta_p$


## Frequentist's Method

The link function: $Pr(Y_i=1|\beta)=H(\mathbf{x}_i^T\boldsymbol{\beta})$

H is a CDF
<!-- v.s.  Maximum Likelihood Estimation -->
<!-- v.s. asymptotic expansions or numerical integration -->

Iteratively Reweighted Least Squares (IRLS) Algorithm

## Bayesian methods


v.s. Multi-layers or approximation


The latent variable \(\mathbf{Z}=\mathbf{X}\boldsymbol{\beta+\varepsilon}\)

$y_i|z_i=\begin{cases}1&\text{if }z_i>0\\0&\text{o.w.}\end{cases}$



- In case of Probit models,
\(\boldsymbol{\varepsilon}\sim N_n(\mathbf{0,I}) \)


- In case of Logit models,
\(\boldsymbol{\varepsilon}\sim Logistic_n(\mathbf{0,I}) \)

Can we have a simple, exact algorithm?




# A Data-Augmentation schemes


A well-behaved Markov chain Monte Carlo (MCMC)

A underlying variable $Z$ simulated from the proper distribution

Generating the missing data

## Motivation

Assume pdf $f_X(x):\mathbb{R}^p \to [0,\infty)$, and estimate $E[g(x)]$.

When $E_{f_X}[g(x)]= \int_{\mathbb{R}^p} g(x) f_X(x) dx$ is hard to numerical integral or analytical approximate,

## Monte Carlo Sampling

Regardless of the distribution, if we have $g(X_1),g(X_2),\ldots,g(X_m) \overset{iid}{\sim} f_X(x)$ 

then $\frac{1}{m}\sum_{i=1}^m g(X_i)$ is a good estimator for $E(g)$.

Consistency: If $E[|g|]<\infty$, then $\frac{1}{n}\sum_{i=1}^n g(X_i) \overset{a.s.}\longrightarrow E[g]$

Unbiasedness: $E[\frac{1}{S}\sum Y_k]=E(Y)$


## Monte Carlo Markov chain (MCMC)

Unknown distribution, if we can constructe a Markov chain



## DA algorithm

When it is impossible to simulate from $f_X(x)$, require the conditions:

1. the invariant x-marginal pdf/pmf: 

$f_X(x)=\int_{\mathbb{R}^q} f(x,y)dy$

2. the applicable conditional pdf/pmf 

$f_{X|Y}(x|y)=\frac{f_{X,Y}(x,y)}{f_Y(y)}$ and $f_{Y|X}(y|x)$.

---

- Constructing a Markov chain, one iteration includes:

1. Draw $Y \sim f_{Y |X}(·|x)$.

2. Draw $X_{i+1} \sim f_{X|Y} (·|y)$.

Repeat to simulate $f_X(x)$

Tanner and Wong (1987), Swendsen and Wang (1987)

```{r,eval=T, echo=F}
# functions of cumulative standard diviation
# 1 runSD(t_EM[,1], n=1, cumulative=TRUE)
# 2
biasedSd<-function(data){sqrt(mean((data-mean(data))^2))}
cumSd<-function(data){sapply(Reduce(c,data,accumulate = T), biasedSd)}
# cumSd(t_EM[,1])
# sd(t_EM[,1])
cumean<- function(x){cumsum(x)/seq_along(x)} 
```

## Ex1: Bivariate Normal Density

```{r,echo=F,eval=F}
# method 1
rbvn<-function (n, m1, s1, m2, s2, rho){
     X1 <- rnorm(n, mu1, s1)
     X2 <- rnorm(n, mu2 + (s2/s1) * rho *
           (X1 - mu1), sqrt((1 - rho^2)*s2^2))
     cbind(X1, X2)
}
bvn <- rbvn(N,mu1,s1,mu2,s2,rho)
# method 2
bvn <- MASS::mvrnorm(N, mu = mu, Sigma = sigma )
# method 3
M <- t(chol(sigma)) # M %*% t(M)
Z <- matrix(rnorm(2*N),2,N) # 2 rows, N/2 columns
bvn <- t(M %*% Z) + matrix(rep(mu,N), byrow=TRUE,ncol=2)

colnames(bvn) <- c("bvn_X1","bvn_X2")
```


```{r,echo=F}
# Function to draw ellipse for bivariate normal data
ellipse_bvn <- function(bvn, alpha){
  Xbar <- apply(bvn,2,mean)
  S <- cov(bvn)
  ellipse(Xbar, S, alpha = alpha, col="red")
}
```

```{r echo = FALSE}
# Target parameters for univariate normal distributions
N <- 20000 ;mu1 <- 0; s1 <- 1; mu2 <- 0; s2 <- 1; rho <- 1/sqrt(2)
# Parameters for bivariate normal distribution
mu <- c(mu1,mu2); s <- c(s1,s2) # Mean
sigma <- matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),2) # Covariance matrix
```



($X \sim N(0,1)$; $Y \sim N(0,1)$)

[$X,Y \sim N_2(0,1,\frac{1}{\sqrt 2})$; $f_X(x)=\int_{\mathbb{R}^q} f(x,y)dy$]

1. Draw $(Y|X=x) \sim N(\frac{x}{\sqrt 2},\frac12)$.

2. Draw $(X|Y=y) \sim N(\frac{y}{\sqrt 2},\frac12)$.


```{r,echo=F}
g.bvn<-function (n, mu1, s1, mu2, s2, rho,step=20){
  x <- 0; y <- 0; 
  mat      <- matrix(ncol=2,nrow = n)
  mat[1, ] <- c(x, y)
  for (i in 2:n) {
    x <- rnorm(1, mu1+(s1/s2)*rho*(y-mu2), sqrt((1-rho^2)*s1^2))
    y <- rnorm(1, mu2+(s2/s1)*rho*(x-mu1), sqrt((1-rho^2)*s2^2))
    mat[i, ] <- c(x, y)} 
  colnames(mat) <- c("X","Y"); 
  thinned=seq(round(n*0.2),n,step)  
  mat[thinned,]
  }
```



---

```{r,eval=T,echo=F}
bvn <- g.bvn(N, mu1, s1, mu2, s2, rho)
```


```{r,eval=T,echo=F}
par(mfrow=c(2,2))

plot(bvn,type = "n", xlab="X",ylab="Y")# 
points(bvn,col=alpha("steelblue", 0.4),pch=1,cex = 0.1)
ellipse_bvn(bvn,.05)
#ellipse_bvn(bvn,.5)
# plot(bvn,type="l",col=alpha("steelblue", 0.4),lwd=.1)
plot(cumean(bvn[,1]),main=bquote(mu[.(1)]),
     type="l",xlab="index",ylab="running average",col="cornflowerblue")
  abline(h=mu1,col="red")
hist(bvn[,1],40,freq = F, xlab="X",main= "") # xlab=bquote(X[.(1)])
curve(dnorm(x,mu1,s1),add = T,col=2)
# lines(density(bvn[,i]), col = "steelblue", lwd = 2)
hist(bvn[,2],40,freq = F, xlab="Y",main= "") # xlab=bquote(X[.(1)])
curve(dnorm(x,mu2,s2),add = T,col=2)
```


# Conditions and Properties

Harris ergodic, which satisfies three properties:
irreducible, aperiodic, and recurrent.

A sufficient condition for Harris ergodicity is

$$\mathcal{K}:k(x'|x)>0\quad\forall x', x\in\mathbf{X}$$

## Definition

A Markov chain, $X = \{X_i\}^\infty_{i=0}$,with state space X. 
If the current state of the chain is $X = x$, then the density of the next state, $X'$, is $k(x'|x)$.
The Markov transition density (Mtd) is

$$k(x'|x)=\int_Y f_{X|Y}(x'|y)f_{Y|X}(y|x) dy$$

---

Check $k(x'|x)$ is a pdf:

$$\begin{aligned}
\int_Xk(x'|x)dx'&=\int_X\left[\int_Y f_{X|Y}(x'|y)f_{Y|X}(y|x) dy\right] dx'\\
&=\int_Yf_{Y|X}(y|x) \left[\int_X f_{X|Y}(x'|y)dx'\right] dy\\
&=\int_Yf_{Y|X}(y|x)dy=1
\end{aligned}$$

## Invariant (stationarity)

$f_X$ is an invariant density for $K$ when


$$f_{X}(x') =\int_X k(x'|x)f_{X}(x)dx$$

Then the Markov chain is time homogeneous and the "recurrent" property holds. ???

## Detailed balance (updat when server resume)

Symmetric $k(x'|x)=k(x|x')$

$$k(x',x)=k(x'|x)f_X(x)=\int_Y \frac{f(x',y)f(y,x)}{f_Y(y)} dy$$
$$k(x,x')=k(x|x')f_X(x')=\int_Y \frac{f(x,y)f(y,x')}{f_Y(y)} dy$$

## Convergence

For a well-behaved Markov chain X, the marginal density of $X_i$
will *converge* to the invariant density $f_X$ no matter how the chain is started.

$$\frac{1}{m}\sum_{i=1}^m g(X_i)\overset{a.s}{=}E_{f_X}[g(x)]$$
s.t. $P\left(\lim\limits_{n\to\infty}|\frac{1}{m}\sum_{i=1}^m g(X_i)-E_{f_X}[g(x)]|<\varepsilon\right)=1$

"irreducible"?

##  Geometric ergodicity and CLT


## Ex4:  Location-scale student's t

EM algorithm (Dempster et al., 1977). 

($Z_i\sim t_{\nu=4,\mu,\sigma^2}$,i=1,..,m, $f_Z(z)=\frac{\Gamma(\frac{\nu+1}2)}{\sqrt{\nu\sigma^2}\Gamma(\frac{\nu}2)}(1+\frac{(z-\mu)^2}{\nu\sigma^2})^{-\frac{\nu+1}2}$)

$p(z,y|\mu,\sigma^2)=\prod_{i=1}^m\underbrace{\frac{\sqrt{y_i}}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{y_i}{2\sigma^2}(z_i-\mu)^2\right]}_{N(\mu,\sigma^2/y_i)}\underbrace{\frac{(\frac{\nu}2)^{(\frac{\nu}2)}}{\Gamma(\frac{\nu}2)}y_i^{\frac{\nu}2-1}\exp\left[-\frac{\nu}{2}y_i\right]}_{Gamma(\nu/2,\nu/2)}$

$$\begin{aligned}\int_Y p(z,y|\mu,\sigma^2)dy=&\prod_{i=1}^mp(z_i|y_i,\mu,\sigma^2)p(y_i|\mu,\sigma^2)dy_i\\
=&\prod_{i=1}^m\frac{\Gamma(\frac{\nu+1}2)}{\sqrt{\nu\sigma^2}\Gamma(\frac{\nu}2)}(1+\frac{(z_i-\mu)^2}{\nu\sigma^2})^{-\frac{\nu+1}2}\end{aligned}$$

---

$p((\mu,\sigma^2),y|z)\propto\pi(\mu,\sigma^2)p(z,y|\mu,\sigma^2)=\frac{1}{\sigma^2}p(z,y|\mu,\sigma^2)$


1. Draw $(Y_i|\mu,\sigma^2,z) \sim Gamma(\frac{\nu+1}2,\frac{1}2(\frac{(z_i-\mu)^2}{\sigma^2}+\nu))$.

   $\hat\mu=\frac{1}{y_{.}}\sum_{j=1}^mz_jy_j$, $\hat\sigma^2=\frac{1}{y_{.}}\sum_{j=1}^my_j(z_j-\hat\mu)^2$

2. Draw $(\sigma^2|y,z) \sim IG(\frac{m+1}2,\frac{y_{.}\hat\sigma^2}2)$.

3. Draw $(\mu|\sigma^2,y,z) \sim N(\hat\mu,\frac{\sigma^2}{y_{.}})$.

A more general DA algorithm developed by Meng and van Dyk (1999) 

---

```{r}
g.tls<-function (n,nu,z,step=20){
  m       <- length(z)
  y       <- rgamma(m,nu/2,nu/2)
  y.      <- sum(y)
  mu      <- sum(z*y)/y.
sigma.sq  <- sum((z-mu)^2*y)/y.
theta     <- matrix(ncol = 2, nrow = n)  
theta[1,] <- c(mu,sigma.sq)  
  for (i in 1:n) {
          y  <- rgamma(m,(nu+1)/2,rate = ((z-mu)^2/sigma.sq+nu)/2)  
          y. <- sum(y)
      hat.mu <- sum(z*y)/y.
hat.sigma.sq <- sum((z-hat.mu)^2*y)/y.   
    sigma.sq <- 1/rgamma(1, (m+1)/2, rate=(hat.sigma.sq*y./2))    
          mu <- rnorm(1, hat.mu, sqrt(sigma.sq/y.))
  theta[i, ] <- c(mu,sigma.sq)  
  }
  thinned=seq(round(n*0.2),n,step)
  theta[thinned,]
}
```

```{r,eval=T, echo=F}
# mu.j <- sigma.sq.j <- NA
  # for (j in 1:m) {mu.j[j]=z[j]*y[j]}
  # mu=sum(mu.j)/y.
  # for (j in 1:m) {  
  # sigma.sq.j[j]=(z[j]-mu)^2*y[j]
  # }
  # sigma.sq=sum(sigma.sq.j)/y.
  #   for (j in 1:m) {
  #     y[j] <- rgamma(1,(nu+1)/2,((z[j]-mu)^2/sigma.sq+nu)/2)  
  #   }
    # for (j in 1:m) {mu.j[j]=z[j]*y[j]}
    # mu=sum(mu.j)/y.
    # 
    # for (j in 1:m) {  
    # sigma.sq.j[j]=(z[j]-mu)^2*y[j]
    # }
    # sigma.sq=sum(sigma.sq.j)/y.
nu=4
set.seed(123)
z <- ggdist::rstudent_t(30,nu,1,2)
# z <- rnst(1e5, 1000, 5, 13)
# rt.scaled(n, 4, mean = 0, sd = 1)
#  Z[1, ] <- z <- rnorm(m,mu,sigma/y)
N <- 20000
t_EM<- g.tls(N,nu,z)
```


---



```{r,eval=T,echo=F}
par(mfrow=c(1,2))
for(i in 1:2){
  cum.est <- cumean(t_EM[,i])
  cum.sd <- cumSd(t_EM[,i])
plot(cum.est,main=c(expression(mu),expression(sigma^2))[i],
     ylim= range(c((cum.est-2*cum.sd),(cum.est+2*cum.sd))),
     type="l",xlab="index",ylab="running average",col="cornflowerblue")
  abline(h=c(1,4)[i],col="red")
  lines(cum.est-1.96*cum.sd,col="cornflowerblue",lty=3)
  lines(cum.est+1.96*cum.sd,col="cornflowerblue",lty=3)
}
```

# Solutions

## Probit Model

A Gibb's sampler:

$(1) \mathbf{z^*|y},\boldsymbol{\beta}$~$N_{tr}(\mathbf{x}_i^T\boldsymbol{\beta},1)$ 
truncated by 0 at$\begin{cases}\text{left}&y_i=1\\\text{right}&y_i=0\end{cases}$

\((2) \boldsymbol{\beta}|\mathbf{y,z^*}\sim N_p\left( \underbrace{(\mathbf{x}^T\mathbf{x})^{-1}\mathbf{x}^T\mathbf{z^*}}_{m_\beta},\underbrace{(\mathbf{x}^T\mathbf{x})^{-1}}_{v_\beta} \right)\)

Repeat (1) and (2) long enough.
(Albert and Chib, 1993)

Because
\(\pi(\boldsymbol{\beta,Z|y}) = C\pi(\boldsymbol{\beta})\prod_{i=1}^{m}\left[\mathbf{1}_{Z_i>0}\mathbf{1}_{y_i=1}+\mathbf{1}_{Z_i\le0}\mathbf{1}_{y_i=0}\right]\phi (Z_{i})\)

\(\pi(\boldsymbol{\beta|y,Z}) = C\pi(\boldsymbol{\beta})\prod_{i=1}^{m}\phi (Z_{i};\mathbf{x}_i^T\boldsymbol{\beta},1)\)



```{r,echo=F}
# beta.t <- c(-1,1/2,1/4,-1,1)
# beta.name <- c("Beta0","Beta1","Beta2","Beta3","Beta4")

# X.df <- data.frame(x1=rnorm(m,1,1),
#                    x2=rnorm(m,2,1),
#                    x3=sample(c("a","b","c"),m,replace=T))

# X <- matrix(rnorm(m*p), ncol = p)
# cnty <- ceiling(exp(y)) # count data
# X <- matrix(runif(n*p, 5, 10), n)

# p.true <- pnorm(X%*%beta.t)

# Y <- rbinom(m,1,p.true)

```

---

Simulate the observed data

```{r}
beta.t <- c(-1,1/2,1/4)
beta.name <- c("Beta0","Beta1","Beta2")
m <- 50; p <- 3;#p <- dim(X)[2] 
step=20; set.seed(123)

X.df <- data.frame(x1=rnorm(m,1,1),
                   x2=rnorm(m,2,1))
X <- model.matrix(~.,X.df)

Y <- ifelse(X%*%beta.t+ rnorm(m) > 0, 1, 0) 

table(Y)
```


```{r, echo=F}
z.cond <- function(beta){
   ez<-(X%*%beta) 
   u<-runif(m,0,1)
   z <- ez + qnorm(ifelse(Y==1,u+(1-u)*pnorm(0,ez,1),
                               u*pnorm(0,ez,1)))
   return(z)
 }

beta.cond = function(z,V,cholV){
   beta <- V%*%(t(X)%*%z)+cholV%*%rnorm(p)
  return(beta)  
}
```


```{r, echo=F}
g.probit<-function (N,X,Y,m,p,step=20){
iXX<-chol2inv(chol(t(X)%*%X)); V<-iXX*(m/(m+1)); cholV<-chol(V) 

beta.ini <- runif(p,-3,3)
   Z     <- matrix(NA,N,m) 
   z.ini <- z.cond(beta.ini)
Beta     <- matrix(NA,nrow=(N+1),ncol=p)
Beta[1,] <- beta.ini <- beta.cond(z.ini,V,cholV)

for(i in 1:N){
   Z[i,]     <- z.cond(Beta[i,])
Beta[(i+1),] <- beta.cond(Z[i,],V,cholV)
}
thinned=seq(round(N*0.2),N,step)  
Beta[thinned,]
}
```


```{r,echo=F}
N <- 25000
beta.p <- g.probit(N,X,Y,m,p)
```

```{r,echo=F}
#beta.p.marg <- mean(dnorm(beta.p%*%t(X)))*beta.p
```

- Gibbs' results

```{r,echo=F}
beta.p.mean <- colMeans(beta.p)
beta.p.median <- apply(beta.p,2,quantile,probs=0.5)
beta.p.ll <- apply(beta.p,2,quantile,probs=0.025)
beta.p.ul <- apply(beta.p,2,quantile,probs=0.975)
beta.p.sd <- c(sd(beta.p[,1]),sd(beta.p[,2]),sd(beta.p[,3]))
par.p<- cbind(beta.p.mean,beta.p.median,beta.p.ll,beta.p.ul,beta.p.sd)
rownames(par.p) <- beta.name
pander(round((par.p),4))
```


```{r,echo=F}
biasedSd<-function(data){sqrt(mean((data-mean(data))^2))}
cumSd<-function(data){sapply(Reduce(c,data,accumulate = T), biasedSd)}
cumean<- function(x){cumsum(x)/seq_along(x)} 
```

---

Iteratively Reweighted Least Squares (IRLS) Algorithm

Fisher Scoring??? Newton-Raphson algorithm "gradient descent level II"???

```{r,echo=F}
fit.p.glm <- glm(Y~.,X.df,family=binomial(link="probit"))
glm.p<- coef(fit.p.glm)
glm.p.sd <- summary(fit.p.glm)$coef[,2] # sqrt(diag(vcov(fit.probit.glm)))
glm.p.ci <- confint(fit.p.glm)
glm.p.marg <-mean(dnorm(X %*% glm.p))*glm.p
glm.p.ci.marg <-mean(dnorm(X %*% glm.p.ci))*glm.p.ci
par.glm.p<- cbind(glm.p,glm.p.sd,glm.p.ci,glm.p.marg,glm.p.ci.marg)
rownames(par.glm.p) <- beta.name
pander(round((par.glm.p),4))
```


---


```{r,echo=F}
par(mfrow=c(2,p),mar=c(3,3.2,1,.5),mgp=c(1.70,.70,0))
for(s in 1:p){
  plot(beta.p[,s],
       main=bquote(beta[.(s-1)]),type="p",pch=1,cex=0.1,col="cornflowerblue")
  abline(h=beta.t[s],col="red")
  abline(h=glm.p[s],col="green")
}
for(s in 1:p){
  cum.est <- cumean(beta.p[,s])
  cum.sd <- cumSd(beta.p[,s])  
  plot(cum.est,ylim= range(c((cum.est-2*cum.sd),(cum.est+2*cum.sd))),
       #c(beta.p.ll[s]-.01,beta.p.ul[s]+.01),
       main=bquote(beta[.(s-1)]),type="l",xlab="index",ylab="running average",col=1)
  abline(h=beta.t[s],col="red")
  abline(h=glm.p[s],col="green")
  # abline(h=beta.p.ll[s],col="cornflowerblue")
  # abline(h=beta.p.ul[s],col="cornflowerblue")
  lines(cum.est-1.96*cum.sd,col="cornflowerblue",lty=3)
  lines(cum.est+1.96*cum.sd,col="cornflowerblue",lty=3)
}
```



## Logit Model

$P_i(Y=1)=\frac1{1+\exp(-\mathbf{x}_i^T\boldsymbol{\beta})}$; 

$H(p_i)^{-1}=\ln\frac{p_i}{1-p_i}$ log odds of success = $\mathbf{x}_i^T\boldsymbol{\beta}$

\(\boldsymbol{\varepsilon}\sim Logistic_n(\mathbf{0,I}) \)

$(1)\quad \boldsymbol{\omega_i^*|\mathbf{y},\beta}\sim PG(n_i,|\mathbf{x}_i^T\boldsymbol{\beta}|)$

\((2)\quad \boldsymbol{\beta|\mathbf{y},\omega^*}\sim N_p\left(\mathbf{m_\omega,v_\omega}\right)\)

where 

$\mathbf{m_\omega}=\mathbf{v_\omega(x'(y-\frac12)+B^{-1}b)}$, 

$\mathbf{v_\omega}={\mathbf{(x'\boldsymbol{\Omega} x+B^{-1})^{-1}}}$

$\boldsymbol{\Omega}=diag_n(\mathbf{\omega_i})$,

The prior $\boldsymbol{\beta}\sim N_p(\mathbf{b,B})$

Repeat (1) and (2) long enough.

(Polson etal, 2013)


```{r,eval=T, echo=T}
set.seed(1234)
Y <- rbinom(m,1,1/(1+exp(-X%*%beta.t)))
table(Y)
```


---

Derive the full conditional pdf

$\pi(\omega|\beta,y)$

$\pi(\beta|\omega,y)$

(update when the server resume)

---

$\boldsymbol{\omega_i^*|\mathbf{y},\beta}\sim PG(n_i,|\mathbf{x}_i^T\boldsymbol{\beta}|)$



```{r}
w.cond <- function(beta,m){
   w <- rpg.devroye(num=m, h=1, z=abs(X%*%beta))
   return(w)
 }
```

\(\boldsymbol{\beta|\mathbf{y},\omega^*}\sim N_p\left(\mathbf{m_\omega,v_\omega}\right)\)

```{r}
beta.cond = function(y,w,p,varbeta=100){
   matbetapr= diag(rep(1/varbeta,p))
   OMG      = diag(w);  
   OMGinv   = diag(1/w)
   eta      = OMGinv%*%(y-0.5)
   varmat   = chol2inv(chol(matbetapr+t(X)%*%OMG%*%X)) 
   meanvec  = varmat%*%(t(X)%*%OMG%*%eta)
   beta <- mvtnorm::rmvnorm(1, mean = meanvec, sigma = varmat)
  return(beta)  
 }
```


---

- Gibbs' sampler

```{r}
g.logit<-function (N,X,Y,step=20){
  p <- dim(X)[2]; m <- dim(X)[1]
  beta.ini <- runif(p,-3,3)
  W        <- matrix(NA,nrow=N,ncol=m)
  w.ini    <- w.cond(beta.ini,m)
  Beta     <- matrix(NA,nrow=(N+1),ncol=p)
  Beta[1,] <- beta.ini <- beta.cond(Y,w.ini,p)
  for(i in 1:N){
    W[i,] = w.cond(Beta[i,],m)
    Beta[(i+1),]=beta.cond(Y,W[i,],p) 
  }
  thinned=seq(round(N*0.2),N,step)  
  Beta[thinned,]
}
```


```{r,echo=F}
set.seed(123)
beta.l <- g.logit(N,X,Y)
```


---

```{r,eval=F,echo=F}
# beta.l.marg <- mean(dlogis(beta.l[thinned,]%*%t(X)))*(beta.l[thinned,])
# beta.l.marg <- colMeans(apply(beta.l[thinned,]%*%t(X),1,dlogis))*(beta.l[thinned,])
```

- Gibbs' results

```{r,echo=F}
beta.l.mean <- colMeans(beta.l)
beta.l.median <- apply(beta.l,2,quantile,probs=0.5)
beta.l.ll <- apply(beta.l,2,quantile,probs=0.025)
beta.l.ul <- apply(beta.l,2,quantile,probs=0.975)
beta.l.sd <- c(sd(beta.l[,1]),sd(beta.l[,2]),sd(beta.l[,3]))
par.l<- cbind(beta.l.mean,beta.l.median,beta.l.ll,beta.l.ul, beta.l.sd)
rownames(par.l) <- beta.name
pander(round((par.l),4))
```


---

Iteratively Reweighted Least Squares (IRLS) Algorithm

```{r}
fit.l.glm <- glm(Y~.,X.df,family=binomial(link="logit"))
glm.l<- coef(fit.l.glm)
glm.l.sd <- summary(fit.l.glm)$coef[,2] # sqrt(diag(vcov(fit.l.glm)))
glm.l.ci <- confint(fit.l.glm)
# glm.l.marg <-mean(dlogis(X %*% glm.l))*glm.l
par.glm.l<- cbind(glm.l,glm.l.sd,glm.l.ci)
rownames(par.glm.l) <- beta.name
pander(round((par.glm.l),4))
```

---

```{r,echo=F}
par(mfrow=c(2,p),mar=c(3,3.2,1,.5),mgp=c(1.70,.70,0))
for(s in 1:p){
  plot(beta.l[,s],
       main=bquote(beta[.(s-1)]),type="p",pch=1,cex=0.1,col="cornflowerblue")
  abline(h=beta.t[s],col="red")
  abline(h=glm.l[s],col="green")
}
for(s in 1:p){
  cum.est <- cumean(beta.l[,s])
  cum.sd <- cumSd(beta.l[,s])  
  plot(cum.est,ylim= range(c((cum.est-2*cum.sd),(cum.est+2*cum.sd))),
       # c(beta.l.ll[s]-.01,beta.l.ul[s]+.01),
       main=bquote(beta[.(s-1)]),type="l",xlab="index",ylab="running average",col=1)
  abline(h=beta.t[s],col="red")
  abline(h=glm.l[s],col="green")
  # abline(h=beta.l.ll[s],col="cornflowerblue")
  # abline(h=beta.l.ul[s],col="cornflowerblue")
  lines(cum.est-1.96*cum.sd,col="cornflowerblue",lty=3)
  lines(cum.est+1.96*cum.sd,col="cornflowerblue",lty=3)
}
```

---

Means

```{r,echo=F}
mean.all<- cbind(irls.p=glm.p,gibbs.p=beta.p.mean,irls.l=glm.l,gibbs.l=beta.l.mean)
rownames(mean.all) <- beta.name
pander(round((mean.all),4))
```

Standard deviations

```{r,echo=F}
sd.all<- cbind(irls.p=glm.p.sd,gibbs.p=beta.p.sd,irls.l=glm.l.sd,gibbs.l=beta.l.sd)
rownames(sd.all) <- beta.name
pander(round((sd.all),4))
```

## literature

Jun S. Liu & Ying Nian Wu (1999) Parameter Expansion for Data Augmentation, Journal of the American Statistical Association, 94:448, 1264-1274, DOI: 10.1080/01621459.1999.10473879



--------------------------------------------------------
random walk                        Markov chain
-------------------               ----------------------
graph                              stochastic process

vertex                             state

strongly connected aperiodic       persistent aperiodic

strongly connected and aperiodic   ergodic

edge weighted undirected graph     time reversible
---------------------------------------------------------

Table 5.1: Correspondence between terminology of random walks and Markov chains

James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer.

---


- Gelman, A. (2014). Bayesian data analysis (Third edition.). CRC Press.

11.7 Bibliographic note

Tanner and Wong (1987) introduced the idea of iterative simulation to many statisticians, using the special case of ‘data augmentation’ to emphasize the analogy to the EM algorithm (see Section 13.4).

  Auxiliary variables
  
12.1 Efficient Gibbs samplers

Gibbs sampler computations can often be simplified or convergence accelerated by adding auxiliary variables, for example indicators for mixture distributions, as described in Chapter 22. The idea of adding variables is also called data augmentation and is often a useful conceptual and computational tool, both for the Gibbs sampler and for the EM algorithm (see Section 13.4).

12.7 Bibliographic note

For the relatively simple ways of improving simulation algorithms mentioned in Sections 12.1 and 12.2, Tanner and Wong (1987) discuss data augmentation and auxiliary variables, and Hills and Smith (1992) and Roberts and Sahu (1997) discuss different parameterizations for the Gibbs sampler. Higdon (1998) discusses some more complicated auxiliary variable methods, and Liu and Wu (1999), van Dyk and Meng (2001), and Liu (2003) present different approaches to parameter expansion. The results on acceptance rates for efficient Metropolis jumping rules appear in Gelman, Roberts, and Gilks (1995); more general results for Metropolis-Hastings algorithms appear in Roberts and Rosenthal (2001) and Brooks,
Giudici, and Roberts (2003).

18.2 Multiple imputation

Any single imputation provides a complete dataset that can be used by a variety of researchers to address a variety of questions. Assuming the imputation model is reasonable, the results from an analysis of the imputed dataset are likely to provide more accurate estimates than would be obtained by discarding data with missing values.
The key idea of multiple imputation is to create more than one set of replacements for the missing values in a dataset. This addresses one of the difficulties of single imputation in that the uncertainty due to nonresponse under a particular missing-data model can be properly reflected. The data augmentation algorithm that is used in this chapter to obtain posterior inference can be viewed as iterative multiple imputation.

  Computation using EM and data augmentation
  
The process of generating missing data imputations usually begins with crude methods of imputation based on approximate models such as MCAR. The initial imputations are used as starting points for iterative mode-finding and simulation algorithms.

Chapters 11 and 13 describe the Gibbs sampler and the EM algorithm in some detail as approaches for drawing simulations and obtaining the posterior mode in complex problems. As was mentioned there, the Gibbs sampler and EM algorithms formalize a fairly old approach to handling missing data: replace missing data by estimated values, estimate model parameters, and perhaps, repeat these two steps several times. Often, a problem with no missing data can be easier to analyze if the dataset is augmented by some unobserved values, which may be thought of as missing data.

Here, we briefly review the EM algorithm and its extensions using the notation of this chapter. Similar ideas apply for the Gibbs sampler, except that the goal is simulation rather than point estimation of the parameters. The algorithms can be applied whether the missing data are ignorable or not by including the missing-data model in the likelihood, as discussed in Chapter 8. For ease of exposition, we assume the missing-data mechanism is ignorable and therefore omit the inclusion indicator I in the following explanation. The generalization to specified nonignorable models is relatively straightforward. We assume that any augmented data, for example, mixture component indicators, are included as part
of ymis. Converting to the notation of Sections 13.4 and 13.5:


18.7 Bibliographic note

Little and Rubin (2002) is a comprehensive text on statistical analysis with missing data.
Van Buuren (2012) is a recent text with a more computational focus. Tanner and Wong
(1987) describe the use of data augmentation to calculate posterior distributions. Schafer
(1997) and Liu (1995) apply data augmentation for multivariate exchangeable models, including the normal, t, and loglinear models discussed briefly in this chapter. The approximate variance estimate in Section 18.2 is derived from the Satterthwaite (1946) approximation; see Rubin (1987a), Meng, Raghunathan, and Rubin (1991), and Meng and Rubin
(1992). Abayomi, Gelman, and Levy (2008) discuss graphical methods for checking the fit
and reasonableness of missing-data imputations.

---

Imai, K., and van Dyk, D. A. (2005). A Bayesian analysis of the multinomial probit model using marginal data augmentation. Journal of Econometrics. 124, 311–334. https://doi.org/10.1016/j.jeconom.2004.02.002

Rubin, D. B. (1987b). A noniterative sampling/importance resampling alternative to the data augmentation algorithm for creating a few imputations when fractions of missing information are modest: The SIR algorithm. Discussion of Tanner and Wong (1987). Journal of the American Statistical Association 82, 543–546.

Tanner, M. A., and Wong, W. H. (1987). The calculation of posterior distributions by data augmentation (with discussion). Journal of the American Statistical Association 82, 528–550. 

van Dyk, D. A., and Meng, X. L. (2001). The art of data augmentation (with discussion). Journal of Computational and Graphical Statistics 10, 1–111.



## Ex2: Simple Slice Sampler (Neal, 2003)

($f_X(x)=3x^2I_{(0,1)}(x)$; $E[X]=\int_{0}^1 xf_X(x)dx=0.75$)

[$f(x,y)=3xI(0<y<x<1)$; $f_X(x)=\int_{0}^x f(x,y)dy$]

$f_Y(y)=\int_{y}^1 f(x,y)dx=\frac32(1-y^2)$

$f_{Y|X}(y|x)=\frac1xI(0<y<x)$

$f_{X|Y}(x|y)=\frac{2x}{1-y^2}I(y<x<1)$

$\int_{y}^xf_{X|Y}(x|y)dx=\frac{x^2-y^2}{1-y^2}I(0<y<x<1)\sim Unif(0,1)$

---

1. Draw $(Y|X=x) \sim Unif(0,x)$ and $U \sim Unif(0,1)$.

2. Update $(X|Y=y,U=u) =\sqrt{u(1-y^2)+y^2}$.


```{r}
g.ex2<-function (n,step=20){
  x <- .5
  y <- .5
  X <- NA
  for (i in 1:n) {
    y <- runif(1,0,x)
    u <- runif(1,0,1)
 X[i] <- x <- sqrt(u*(1-y^2)+y^2)
      }
 thinned=seq(round(n*0.2),n,step)  
 X[thinned]
}
```

```{r,eval=T, echo=F}
ex2<- g.ex2(N)
```

---


```{r,eval=T,echo=F}
par(mfrow=c(1,2))
plot(cumean(ex2),main="",
     type="l",xlab="index",ylab="running average",col="cornflowerblue")
abline(h=3/4,col="red")
hist(ex2,40,freq = F, xlab="",main= "")
curve(3*x^2,add = T,col=2)
```


## Ex3: t: Normal-Gamma

($X\sim t_4$, $f_X(x)=\frac38(1+\frac{x^2}4)^{-\frac52}$)

[$f(x,y)=\frac{4}{\sqrt{2\pi}}y^{\frac32}\exp\{-y(\frac{x^2}{2}+2)\} I_{(0,\infty)}(y)$ ]

1. Draw $(Y|X=x) \sim Gamma(\frac52,\frac{x^2}2+2)$.

2. Draw $(X|Y=y) \sim N(0,y^{-1})$.

```{r}
g.t<-function (n,step=20){
  x <- 1;y <- 1 ; X <- NA
  for (i in 1:n) {
    y <- rgamma(1,5/2,(x^2/2+2))    
    X[i] <- x <- rnorm(1, 0, 1/sqrt(y))
  }
  thinned=seq(round(n*0.2),n,step)  
  X[thinned]
}
```

```{r, eval=T,echo=F}
t_N.G<- g.t(N)
# 2*var(t_N.G)/(var(t_N.G)-1)
```


---


```{r,eval=T,echo=F}
par(mfrow=c(1,2))
plot(cumean(t_N.G),main="",
     type="l",xlab="index",ylab="running average",col="cornflowerblue")
  abline(h=0,col="red")
hist(t_N.G,40,freq = F, xlab="X",main= "",xlim = c(-5,5))
curve(dt(x,4),add = T,col=2)
```


