---
title: "STAT 501"
subtitle: Statistical Literature and Problems Part II
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document: default
header-includes:
- \usepackage{amssymb}
- \usepackage{amsmath}
---




```{r setup, include=F}
knitr::opts_chunk$set(message=F, warning=F, echo=T,cache = T,collapse = T)
options(width = 2000)
options(repos="https://cran.rstudio.com")
options(scipen=6)
options(digits=4)
if (!require(pacman)) {install.packages("pacman"); library(pacman)}
p_load(stargazer, pscl, mlmRev,mvtnorm, MASS,brms, ggplot2,tidyverse,mlogit,BayesLogit,robcbi,kableExtra,truncnorm,msm ,lme4) 

# likelihoodAsy, coda,devtools,loo,dagitty,rethinking, msm::rtnorm 
library(nimble, warn.conflicts = FALSE)
```


#  {.tabset .tabset-fade .tabset-pills}



## (Polson etal, 2013)

Nicholas G. Polson, James G. Scott & Jesse Windle (2013) Bayesian Inference for Logistic Models Using Pólya–Gamma Latent Variables, Journal of the American Statistical Association, 108:504, 1339-1349, [DOI: 10.1080/01621459.2013.829001](https://www.tandfonline.com/doi/full/10.1080/01621459.2013.829001)

### Introduction

[Home page of James Scott](https://jgscott.github.io/)

[R package BayesLogit and Thesis](https://github.com/jwindle/BayesLogit)


```{r,eval=F,include=F}
h = c(1, 2, 3);
z = c(4, 5, 6);
## Devroye-like method -- only use if h contains integers, preferably small integers.
(X = rpg.devroye(100, h, z));
h = c(1.2, 2.3, 3.2);
z = c(4, 5, 6);
## Sum of gammas method -- this is slow.
(X = rpg.gamma(100, h, z));
h = c(1, 4, 2.3);
z = c(4, 5, 6);
## Hybrid method -- automatically chooses best procedure.
(X = rpg(100, h, z));
```


*Definition 1*. A random variable X has a Pólya–Gamma distribution with parameters b > 0 and , denoted as $X\sim PG(b, c),$ if

\[X\overset{\mathcal{D}}{=}\frac1{2\pi^2}\sum_{k=1}^\infty\frac{g_k}{((k-\frac12)^2 + \frac{c^2}{4\pi^2})}\quad (1)\]

where the $g_k ~ Ga(b, 1)$ are independent gamma random variables, and where  indicates equality in distribution.


### 2 The Polya-Gamma distribution

#### 2.1. The Case $PG(b,0)$

The $PG(b,0)$ class of distributions is closely related to a subset of distributions that are surveyed by Biane, Pitman, and Yor (2001). This family of distributions, which we denote by $J^\star(b), b>0$, has close connections with <span style="color: red;">the Jacobi Theta and Riemann Zeta functions, and with Brownian excursions</span>. Its Laplace transform is

\[E[\exp(-tJ^\star(b))]=\cosh^{-b}\sqrt{2t}\quad (4)\]

implying that \(PG(b,0)\overset{\mathcal{D}}{=}\frac14J^\star(b)\)

#### 2.2. The General $PG(b,c)$ Class

\[p(x|b,c)=\frac{\exp(-\frac{c^2}2x) p(x|b,0)}{E[\exp x(-\frac{c^2}2\omega)]}\quad (5)\]

where $p(x|b,0)$ is the density of an $\omega\sim PG(b,0)$ random variable.

### 3 A data-augmentation strategy

A data-augmentation scheme for binomial likelihoods

#### 3.1 Main Result

|   | Polson et al. (2013)  | Albert and Chib (1993)  |
|---|---|---|
Gaussian  | scale mixture  | location mixture |
Latent variables | Polya-Gamma | truncated normals |

The number of successes $y_i\sim Bino(n_i,\frac{1}{\{1+e^{-\psi_i}\}})$, where $n_i$ is the number of trials, $\psi_i=x_i^T\boldsymbol{\beta}$ are the log odds of success. $x_i=(x_{i1},..,x_{ip} )$ the vector of regressors for observation $i\in\{1,..,N\}$. $\boldsymbol{\beta}\sim N(b,B)$

To sample from the posterior distribution using the Pólya–Gamma method, simply iterate two steps:

\[\begin{align}
(\omega_i|\boldsymbol{\beta})&\sim PG(n_i,x_i^T\boldsymbol{\beta})\\
(\boldsymbol{\beta}|y,\omega)&\sim N(m_\omega,V_\omega);\quad m_\omega=V_\omega(X^T\kappa+B^{-1}b);V_\omega=(X^T\Omega X+B^{-1})^{-1}
\end{align}\]

where \(\kappa=(y_1-\frac{n_1}2,..,y_N-\frac{n_N}2)\), and $\Omega$ is the diagonal matrix of $\omega_i$'s.

#### 3.2 Existing Data-Augmentation Schemes

The outcomes $y_i$ are assumed to be thresholded versions of an underlying continuous quantity $z_i$

Assume $n_i=1$, \(y_i=\begin{cases}1&\text{if } z_i>0\\0&\text{if }z_i\le0\end{cases}\)

\(z_i=x_i^T\boldsymbol{\beta}+\epsilon_i\), \(\epsilon_i\sim Logistic(1)\)

The standard approach has been to add another layer of auxiliary variables to handle the logistic error model on the latent-utility scale. One strategy is to represent the logistic distribution as a normal-scale mixture

\[(\epsilon_i|\phi_i)\sim N(0,\phi_i);\quad \phi_i=(2\lambda_i)^2; \lambda_i\sim KS(1)\quad\text{Kolmogorov–Smirnov distribution}\]

Alternatively, one may approximate the logistic error term as a discrete mixture of normals.

\[(\epsilon_i|\phi_i)\sim N(0,\phi_i);\quad \phi_i=\sum_{k=1}^K\omega_k\delta_{\phi^{(k)}}\]

where $\delta_{\phi}$ indicates a <span style="color: red;">*Dirac measure*</span> at $\phi$. The weights $\omega_k$ and the points $\phi^{(k)}$ in the discrete mixture are fixed for a given choice of $k$ so that *<span style="color: red;">the Kullback–Leibler divergence</span>* from the true distribution of the random utilities is minimized. Frühwirth-Schnatter and Frühwirth (2010) found that the choice of $K=10$ leads to a good approximation.

The discrete mixture of normals is an approximation, but it outperforms the scale mixture of normals in terms of effective sampling rate, as it is much faster.

One may also arrive at the hierarchy above by manipulating the random utility derivation of McFadden (1974)

The dRUM. One must use a table of different weights and variances representing different normal mixtures, to approximate a finite collection of type-III logistic distributions, and interpolate within this table to approximate the entire family.

Another approximation: the use of a Student-t link function as a close substitute for the logistic link. This also introduces a second layer of latent variables, in that the Student-t error model for $z_i$ is represented as a scale mixture of normals.

Our data-augmentation scheme differs from each of these approaches in several ways. 

1. it does not appeal directly to the random-utility interpretation of the logit model. Instead, it represents the logistic CDF as a mixture with respect to an infinite convolution of gammas. 

2. the method is exact, in the sense of making draws from the correct joint posterior distribution, rather than an approximation to the posterior that arises out of an approximation to the link function. 

3. like the Albert and Chib (1993) method, it requires only a single layer of latent variables.


Directed acyclic graphs depicting two latent-variable constructions for the logistic-regression model: the difference of random-utility model versus direct data-augmentation scheme. ![](uasa_a_829001_o_f0001g.jpeg)

#### 3.3. Mixed Model Example

The real advantage of data augmentation, and the Pólya–Gamma technique in particular, is that it becomes easy to construct and fit more complicated models. For instance, the Pólya–Gamma method trivially accommodates *mixed models, factor models, and models with a spatial or dynamic structure*. For most problems in this class, good MH samplers are difficult to design, and at the very least will require ad hoc tuning to yield good performance.


\[\begin{align}
y_{ij}&\sim Bino(1,p_{ij}),\quad p_{ij}=\frac{e^{\psi_{ij}}}{1+e^{\psi_{ij}}}\\
\psi_{ij}&=m+\delta_j+x'_{ij}\beta,\quad\delta_j\sim N(0,1/\phi);\quad m\sim N(0,\kappa^2/\phi)
\end{align}\]

where $i$ and $j$ correspond to the $i^{th}$ observation from the $j^{th}$ district. The fixed effect $\beta$ is given an $N(0, 100I)$ prior, while the precision parameter $\phi$ is given $Ga(1, 1)$ prior. We take $\kappa\to\infty$ to recover an improper prior for the global intercept $m$.


Bangladesh Fertility Survey, 1989

```{r,eval=T,echo=T,collapse=T}
data(Contraception)
```

As seen in the negative binomial examples below, one may also painlessly incorporate a more complex prior structure using the Pólya–Gamma technique. For instance, if given information about *the geographic location* of each district, one could place a *spatial process prior* upon the random offsets $\{\delta_j\}$.

### 4 Simulating Polya-Gamma random Variables

a method for simulating from the Pólya–Gamma distribution, which we have implemented as a stand-alone sampler in the BayesLogit R package.

#### 4.1. The $PG(1,z)$ Sampler

 An exponentially tilted Jacobi distribution $J^\star(1,z)$ via the density

\[f(x|z)=\cosh(z)\exp\left(-\frac{z^{2}x}2\right)f(x)\quad(9)\]
\[PG(1,z)=\frac14J^\star(1,\frac z2)\quad (10)\]


When \(f(x)=\sum_{n=0}^\infty(-1)^na_n(x)\) and the coefficients $a_n(x)$ are decreasing for all , for fixed $x$ in the support of $f$, then the partial sums, \(S_n(x)=\sum_{i=0}^n(-1)^ia_i(x)\), satisfy 

\[S_0(x)>S_2(x)>..>f(x)>..>S_3(x)>S_1(x)\quad (11)\]

For the $J^\star(1,z)$ distribution the algorithm will accept with high probability upon checking $U\le S_1(X)$.

The Jacobi density has two alternating-sum representations, \(\sum_{n=0}^\infty(-1)^na_i^L(x)\) and \(\sum_{n=0}^\infty(-1)^na_i^R(x)\), neither of which satisfy Equation (11) for all $x$ in the support of $f$. However, each satisfies Equation (11) on an interval. These two intervals, respectively, denoted as $I_L$ and $I_R$ , satisfy \(I_L\cup I_R = (0,\infty)\) and \(I_L\cap I_R\neq\emptyset\) Thus, one may pick \(t\in I_L\cap I_R\)and define the piecewise coefficients.

\[a_n(x)=\pi(n+\frac{1}2)\begin{cases}(\frac{2}{\pi x})^{\frac32}\exp\left(-\frac{2(n+\frac{1}2)^2}{x}\right) & 0<x\le t&(12)\\ \exp\left(-(\frac{\pi^2(n+\frac{1}2)^{2}}2)x\right) & x>t&(13)\end{cases}\]

so that \(f(x)=\sum_{n=0}^\infty(-1)^na_n(x)\) satisfies the partial sum criterion (11) for $x>0$. Devroye shows that the best choice of $t$ is near 0.64.

The $J^\star(1,z)$ density can be written as an infinite, alternating sum \(f(x|z)=\sum_{n=0}^\infty(-1)^na_n(x|z)\), where

\[a_n(x|z)=\cosh(z)\exp\left(-\frac{z^{2}x}2\right)a_n(x)\]

This satisfies Equation (11), as \(\frac{a_{n+1}(x|z)}{a_n(x|z)}=\frac{a_{n+1}(x)}{a_n(x)}\). 


Since \(a_0(x|z)\ge f(x|z)\), the first term of the series provides a natural proposal:

\[c(z) g(x|z)=\frac{\pi}2\cosh(z)\begin{cases}(\frac{2}{\pi x})^{\frac32}\exp\left(-\frac{z^{2}x}2-\frac{1}{2x}\right) & 0<x\le t\\ \exp\left(-(\frac{z^{2}}2+\frac{\pi^{2}}8)x\right) & x>t\end{cases}(14)\]

$X\sim g(x|z)$ may be sampled from a mixture of an inverse-Gaussian and an exponential:

\[X\sim \begin{cases}IG(|z|^{-1},1)\mathbf{1}_{(0,t]} & \text{with prob } \frac{p}{p+q}\\ Expo(-\frac{z^{2}}2+\frac{\pi^{2}}8)\mathbf{1}_{(t,\infty)} & \text{with prob } \frac{q}{p+q}\end{cases}\]

where \(p(z) =\int^t_0 c(z) g(x|z)dx\) and \(q(z) =\int_t^\infty c(z) g(x|z)dx\). Note that we are implicitly suppressing the dependence of p, q, c, and g upon t.


<span style="color: red;">\(Expo(-(\frac{z^{2}}2+\frac{\pi^{2}}8))\)???</span>

sampling $J^\star(1,z)$ proceeds as follows:

1. Generate a proposal $X\sim g(x|z)$.

2. Generate $U\sim Unif(0,c(z)g(X|z))$.

3. Iteratively calculate $S_n (X|z)$, starting at $S_1(X|z)$, until $U\le S_n(X|z)$ for an odd $n$ or until $U>S_n (X|z)$ for an even $n$.

4. Accept $X$ if $n$ is odd; return to step 1 if $n$ is even.

To sample $Y\sim PG(1, z)$, draw $X\sim J^\star(1,z/2)$ and then let $Y=X/4$


#### 4.2. Analysis of Acceptance Rate

**Proposition 1** Define

\[p(z,t)=\int^t_0 \frac\pi2\cosh(z)\exp\left(-\frac{z^{2}x}2\right)a_0^L(x)dx\]
\[q(z,t)=\int^\infty_t \frac\pi2\cosh(z)\exp\left(-\frac{z^{2}x}2\right)a_0^R(x)dx\]

1. The best truncation point $t^\star$ is independent of $z\ge0$.

2. For a fixed truncation point $t$, $p(z,t)$ and $q(z,t)$ are continuous, $p(z,t)$ decreases to zero as $z$ diverges, and $q(z, t)$ converges to 1 as $z$ diverges. Thus, $c(z,t)= p(z,t) + q(z,t)$ is continuous and converges to 1 as $z$ diverges.

3. For fixed $t$, the average probability of accepting a draw, $1/c(z,t)$, is bounded below for all $z$. For $t^\star$, this bound to five digits is $0.99919$, which is attained at $z\simeq1.378.$


#### 4.3. Analysis of Tail Probabilities

**Proposition 2**: When sampling $X\sim J^\star(1, z)$, the probability of deciding to accept or reject upon checking the $n^{th}$ partial sum $S_n$, $n\ge1$, is

\[\frac1{c(z)}\int^\infty_0 [a_{n-1}(x|z)-a_n(x|z)]dx\]

#### 4.4. The General $PG(b, z)$ Case

The effective sample size (ESS) for the ith parameter in the model is

\[ESS_i=\frac{M}{1+2\sum_{j=1}^k\rho_i(j)}\]

where $M$ is the number of post-burn-in samples, and $\rho_i(j)$ is the $j^{th}$ autocorrelation of the chain corresponding to $\beta_i$

### 5 Experiment

presents the results of an extensive benchmarking study comparing the efficiency of our method to other data-augmentation schemes. 

The eight datasets


- In binary logit models.  

First, the Pólya–Gamma is more efficient than all previously proposed data-augmentation schemes. 

Second, the Pólya–Gamma method always had a higher effective sample size than the two default Metropolis samplers we tried. 

Finally, the Pólya–Gamma method truly shines when the model has a complex prior structure.

- In negative-binomial models.  

The Pólya–Gamma method consistently yields the best effective sample sizes in negative-binomial regression. However, its effective sampling rate suffers when working with a large count or a nonintegral overdispersion parameter. 

Using either the Pólya–Gamma or the Frühwirth-Schnatter et al. (2009) techniques, one arrives at a multivariate Gaussian conditional for $\psi$ whose covariance matrix involves latent variables. Producing a random variate from this distribution is expensive, as one must calculate the Cholesky decomposition of a relatively large matrix at each iteration. Therefore, the overall sampler spends relatively less time drawing auxiliary variables. Since the Pólya–Gamma method leads to a higher effective sample size, it wastes fewer of the expensive draws for the main parameter.

### 6 Discussion

concludes with a discussion of some open issues related to our proposal.

### Technical supplement

#### 1 Details of Polya-Gamma sampling algorithm

#### 2 Benchmarks: overview

#### 3 Benchmarks: binary logistic regression

##### 3.1 Data Sets

Nodal: part of the boot R package (Canty and Ripley, 2012). The response indicates if cancer has spread from the prostate to surrounding lymph nodes. There are 53
observations and 5 binary predictors.

Pima Indian: There are 768 observations and 8 continuous predictors. It is noted on [the UCI website](http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) that there are many predictor values coded as 0, though the physical measurement should be non-zero. We have removed all of those entries to generate a data set with 392 observations. The marginal mean incidence of diabetes is roughly 0.33 before and after removing these data points.

[Heart](http://archive.ics.uci.edu/ml/datasets/Statlog+(Heart)): The response represents either an absence or presence of heart disease. There are 270 observations and 13 attributes, of which 6 are categorical or binary and 1 is ordinal. The ordinal covariate has been stratified by dummy variables.

[Australian Credit](http://archive.ics.uci.edu/ml/datasets/Statlog+(Australian+Credit+Approval)): The response represents either accepting or rejecting a credit card application.3 The meaning of each predictor was removed to protect the propriety of the original data. There are 690 observations and 14 attributes, of which 8 are categorical or binary. There were 37 observations with missing attribute values. These
missing values were replaced by the mode of the attribute in the case of categorical data and the mean of the attribute for continuous data. This dataset is linearly
separable and results in some divergent regression coefficients, which are kept in check by the prior.

[German Credit 1 and 2](http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)): The response represents either a good or bad credit risk.4
There are 1000 observations and 20 attributes, including both continuous and categorical data. We benchmark two scenarios. In the first, the ordinal covariates have
been given integer values and have not been stratified by dummy variables, yielding
a total of 24 numeric predictors. In the second, the ordinal data has been stratified
by dummy variables, yielding a total of 48 predictors.

Synthetic 1: Simulated data with 150 outcomes and 10 predictors. The design points were chosen to be orthogonal. The data are included as a supplemental file.

Synthetic 2: Simulated data with 500 outcomes and 20 predictors. The design points were simulated from a Gaussian factor model, to yield pronounced patterns of collinearity.
The data are included as a supplemental file.




##### 3.2 Methods

##### 3.3 Results

#### 4 Benchmarks: logit mixed models

#### 5 Benchmarks: negative-binomial models

#### 6 Extensions


## (Choi & Hobert, 2013)

Choi, H. M., & Hobert, J. P. (2013). The Polya-Gamma Gibbs sampler for Bayesian logistic regression is uniformly ergodic. [Electronic Journal of Statistics, 7, 2054-2064.](https://projecteuclid.org/euclid.ejs/1377005819)

#### 1 Introduction

#### 2 Polson, Scott and Windle’s algorithm

## (Taylor-Rodríguez et al, 2017)

Taylor-Rodríguez, D., Womack, A., Fuentes, C., & Bliznyuk, N. (2017). Intrinsic Bayesian Analysis for Occupancy Models. [Bayesian Anal., 12(3), 855-877.](https://projecteuclid.org/euclid.ba/1473431536)

### 1 Introduction

### 2 Inference for a single model

#### 2.1 The occupancy model with Probit link

#### 2.2 An objective prior for $\alpha,\lambda$

## (Johnson etal, 2019)





## Resources

[Charlie Geyer's Personal Home Page](http://users.stat.umn.edu/~geyer/)

[McElreath 2020. Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/)

[R package rethinking](https://github.com/rmcelreath/rethinking)

```{r,eval=F,echo=F,collapse=T}
library(devtools)
devtools::install_github("rmcelreath/rethinking")
install.packages("V8")
library(rethinking)
data(bangladesh)
#woman  ID number for each woman in sample
#district  Number for each district
#use.contraception  0/1 indicator of contraceptive use
#living.children  Number of living children
#age.centered  Centered age
#urban  0/1 indicator of urban context
```
