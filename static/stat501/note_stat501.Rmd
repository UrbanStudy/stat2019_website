---
title: "STAT 501"
subtitle: Statistical Literature and Problems 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document: default
header-includes:
- \usepackage{amssymb}
- \usepackage{amsmath}
---




```{r setup, include=F}
knitr::opts_chunk$set(message=F, warning=F, echo=T,cache =T,collapse = T)
options(width = 2000)
options(repos="https://cran.rstudio.com")
options(scipen=10)
options(digits=10)
if (!require(pacman)) {install.packages("pacman"); library(pacman)}
p_load( ggplot2,tidyverse,stargazer, pscl,kableExtra, MASS,
        brms,mlogit,lme4,
       mfx,foreign,erer ,nnet, VGAM,
       bamlss,bayesm,
      BayesLogit,robcbi,truncnorm,msm, mlmRev,mvtnorm ) 

# likelihoodAsy, coda,devtools,loo,dagitty,rethinking, msm::rtnorm 
library(nimble, warn.conflicts = FALSE)
```


#  {.tabset .tabset-fade .tabset-pills}


## (Albert &  Chib, 1993)

James H. Albert & Siddhartha Chib (1993) Bayesian Analysis of Binary and Polychotomous Response Data, Journal of the American Statistical Association, 88:422, 669-679, [DOI: 10.1080/01621459.1993.10476321](https://doi.org/10.1080/01621459.1993.10476321)

### 1 Introduction

\(Y_1,..,Y_n\sim Bern(p_i)\). \(\beta_{k\times1}\) unknown vector of parameters. \(X_i^T=(X_{i1},..,X_{ik})\) known covariates.

\(p_i=H(\mathbf{x}_i^T\boldsymbol{\beta})\). \(H(\cdot)\) is a known CDF with linear structure \(\mathbf{x}_i^T\boldsymbol{\beta}\)

If \(H(\cdot)\) is standard normal CDF, it obtains the probit model,

if \(H(\cdot)\) is logistic CDF, it obtains the logit model.

\(\pi(\boldsymbol{\beta})\) is the prior density.

\(
\pi(\boldsymbol{\beta}|data) = \frac{\pi(\boldsymbol{\beta})\prod_{i=1}^{k}H (\mathbf{x}_i^T\boldsymbol{\beta})^{y_i}(1-H(\mathbf{x}_i^T\boldsymbol{\beta}))^{1-y_i}}{\int\pi(\boldsymbol{\beta})\prod_{i=1}^{k}H (\mathbf{x}_i^T\boldsymbol{\beta})^{y_i}(1-H(\mathbf{x}_i^T\boldsymbol{\beta}))^{1-y_i}d\boldsymbol{\beta}}
\)
is intractable.


For small number of parameter, a Bayesian approach summarized the posterior using numerical integration.

For large models (k large), posterior moments by Monte Carlo integration with a `multivariate Student's t importance function`.

This is a simulation-based approach for computing the exact posterior distribution of $\beta$.

The key idea is to introduce $N$ independent latent variables $Z_1,..Z_N\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1)$ into the problem. \(Y_i=\begin{cases}1&\text{if } Z_i>0\\0&\text{if }Z_i\le0\end{cases}\)

This approach is very simular to the data augmentation/Gibbs sampling framework used in censored regression models.

### 2. The Gibbs Sampler

\(
\theta_1^{(1)}\quad\text{from}\quad\pi(\theta_1|\{\theta_j^{(0)},j\neq1\})\\
\theta_2^{(1)}\quad\text{from}\quad\pi(\theta_2|\{\theta_1^{(1)},j>2\})\\
\vdots\\
\theta_p^{(1)}\quad\text{from}\quad\pi(\theta_p|\{\theta_j^{(1)},j<p\})\\
\)

One cycle is iterated $t$ times. For sufficiently large $t^*$, $\theta^{(t^*)}$ can be regarded as one simulated value from the posterior of $\theta$. Replicating this process $m$ times $\{\theta_{1j}^{(t^*)},\theta_{2j}^{(t^*)},..,\theta_{pj}^{(t^*)},j=1,..,m\}$

Two practical drawbacks to the replication approach:

1. The method is inefficient. the samples $\{\theta_j^{(t)}\}$, for $t<t^*$ are discarded.

2. After the initial run it may be necessary to repeat the simulation with a larger number of replication to get accurate density estimates.

- A "one-run" Gibbs sampling scheme is efficient in that few observations are discarded.

1. One should collect the values staring at the cycle $t$. The value of $t$ is samll (10-40) relative to the total number of values collected. <span style="color: red;">(burn-in???)</span>

2.  If one wishes to obtain an approximate independent sample of the $\theta$, the simulated values of $\theta$ could be collected at cycles $t,t+n_1,t+2n_1,..$, where $n_1$ is the spacing between cycles where $\theta^{(t)}$ and $\theta^{(t)}$ are believed to be approximately independent. *But it is not necessary to obtain an independent sample of $\theta$ to obtain, say, a marginal posterior density estimate of $\theta_k$* <span style="color: red;">???</span>

- One goal of this article is to obtain estimates of the densities of the individual parameters or their functional. 

One can estimate the density of this function using a *kernel density* estimate of the simulated values of \(g(\theta_k)\{\theta_k^{(i)},i=1,..,m\}\). A slightly preferable estimate of this marginal posterior density is given by \(\hat\pi(g(\theta_k))\approx\frac1m\sum_{i=1}^m\pi(g(\theta_k)\{\theta_r^{(i)},r\neq k\})\) <span style="color: red;">???</span>

In practice, we collect values of $\theta$ in batches of 100-200 until all the marginal density estimates for the components of $\theta$ stabilize.

- A second goal is estimation of posterior expectations.

To compute this standard error from this correlated simulation sample, we apply *the well-known batch means method*. We batch or section the sample into subsamples of equal size. When the lag one autocorrelation of the batch means is under .05, the simulation standard error is computed as the standard deviation of the batch means divided by the square root of the number of batches.
\(se=\frac{sd}{\sqrt{B}}\) <span style="color: red;"> subsamples of equal size???</span>

### 3 Data augmentation and Gibbs sampling

#### 3.1 Introduction

\[
\begin{align} 
\pi(\boldsymbol{\beta|y,Z}) = C\pi(\boldsymbol{\beta})\prod_{i=1}^{N}\phi (Z_{i};\mathbf{x}_i^T\boldsymbol{\beta},1); &\quad\mathbf{Z}=\mathbf{X}\boldsymbol{\beta+\varepsilon}; &\quad \boldsymbol{\varepsilon}\sim N_N(0,\mathbf{I})&&(1)\\
\\
\boldsymbol{\beta}|\mathbf{y,Z}\sim N_k(\boldsymbol{\hat\beta_Z},(\mathbf{X}^T\mathbf{X})^{-1}); &\quad\boldsymbol{\hat\beta_Z}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Z}&&& (2)\\
\\
Z_i|\boldsymbol{y,\beta}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1) &\quad\text{truncated at the left by 0} & \text{if } y_i=1 &&\\
 &\quad\text{truncated at the right by 0} & \text{if } y_i=0 &&(3)
\end{align}
\]

The starting value $\beta^{(0)}$ may be taken to be the maximum likelihood (ML) estimate, or least squares (LS) estimate $(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$

#### 3.2 The $t$ Link

```{r}
x <- seq(0.001,.999,0.001)
plot(qlogis(x,location=0,scale=1),qt(x,df=4),col=4,type = "l")
lines(qlogis(x,location=0,scale=1),qt(x,df=8), col = 2, lty = 2, lwd = 2, add = TRUE)
lines(qlogis(x,location=0,scale=1),qt(x,df=16), col = 3, lty = 4, lwd = 2, add = TRUE)
abline(a=0,b=1, lty = 3, add = TRUE)
legend(4,-2,c("t(4)","t(8)","t(16)"),col = c(4,2,3),lty = c(1,2,4))
```


\(\mathbf{Y}\sim Bern(p_i)\) have an underlying \(N(Z)\); \(\boldsymbol{\beta}|\mathbf{Z}\sim N_k()\); generalize mixtures of Normal distribution.

$H()=t$ *investigate the sensitivity* of the fitted probabilities to the choice of link function.

The most popular link function for binary data is the logit, which corresponds to a choice of a logistic distribution for $H$

Logistic quantiles are approximately a linear function of $t(8)$ quantiles. The logistic distribution has the same kurtosis as a $t$ distribution with 9 df.

\(Z_i\sim t_{\nu}(\mathbf{x}_i^T\boldsymbol{\beta},1)\) equivalently, \(Z_i|\lambda_i\sim N(\mathbf{x}_i^T\boldsymbol{\beta},\lambda_i^{-1})\); \(\lambda_i\sim Gamma(\frac{\nu}{2},\frac{2}{\nu})\propto\lambda_i^{\frac{\nu}{2}-1}\exp(-\frac{\nu\lambda_i}{2})\); Suppose \(\beta\sim Unif()\)

\[
\begin{align} 
\boldsymbol{\beta|y,Z,\lambda,\nu}\sim N_k(\boldsymbol{\hat\beta_{Z,\lambda}},(\mathbf{X'WX})^{-1}); &\quad\boldsymbol{\hat\beta_{Z,\lambda}}=(\mathbf{X'WX})^{-1}\mathbf{X'WZ},\ \mathbf{W}=\mathrm{diag}(\lambda_i) &&& (4)\\
\\
\boldsymbol{Z_i|y,\beta,\lambda,\nu}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},\lambda_i^{-1}) &\quad\text{truncated at the left by 0} & \text{if } y_i=1 &&\\
 &\quad\text{truncated at the right by 0} & \text{if } y_i=0 &&(5)\\
\\
\boldsymbol{\lambda_{1:N}|y,Z,\beta,\nu}\sim Gamma(\frac{\nu+1}{2},\frac{2}{\nu+(Z_i-\mathbf{x}_i^T\boldsymbol{\beta})^2})&\quad\text{independent with } \lambda_i&&&(6)\\
\boldsymbol{\nu|y,Z,\beta,\lambda}\propto\pi(\nu)\prod_{i=1}^N(c(\nu)\lambda_i^{\frac{\nu}{2}-1}e^{-\frac{\nu\lambda_i}{2}})&\quad\text{in a finite set}&&&(7)\\
\end{align}
\]

$\beta^{(0)}=$  least squares (LS) estimate under the probit model, set $\lambda_i=1,\forall i$

\[\hat\pi(\boldsymbol{\beta})\approx\frac1m\sum_{i=1}^m\pi(\boldsymbol{\beta|Z^{(i)},\lambda^{(i)}})\]

\(p_k=\Phi(\lambda_k^{\frac12}\mathbf{x}_k^T\boldsymbol{\beta})\) by a transformation of the conditional density of $\beta$

\[\hat\pi(p_k)=\frac1m\sum_{i=1}^m\frac{\phi(\Phi(p_k);\mu,\sigma^2)}{\phi(\Phi(p_k);0,1)}\]

\(\mu=\sqrt{\lambda_k^{(i)}}\mathbf{x}_k^T\boldsymbol{\hat\beta_{Z,\lambda}^{mu3(i)}}\) <span style="color: red;">????</span>

\(\sigma^2=\lambda_k^{(i)}\mathbf{x}_k^T(\mathbf{X'WX})^{-1}\mathbf{x}_k\)


#### 3.3 Hierarchical Analysis

(1) \(\mathbf{Z}\sim N(\boldsymbol{X\beta,I})\), (2) \(\boldsymbol{\beta}\sim N(\boldsymbol{A\beta^{(0)},\sigma^2I})\), (3) prior density \(\pi(\boldsymbol{\beta^{(0)},\sigma^2})\qquad\)  (8)

The hyperparameters \(\boldsymbol{\beta^{(0)}}\sim Unif()\), $\sigma^2$ given a noninformative prior <span style="color: red;">????</span>

 The posterior density of the regression vector $\beta$ compromises between least squares estimates from the "full" k-dimensional model and the "reduced" p-dimensional model where $\boldsymbol{\beta=A\beta^{(0)}}$

\[
\begin{align} 
\boldsymbol{\beta|Z,\sigma^2}&\sim N_k(\boldsymbol{\mu,V}) &&(9)\\ 
\boldsymbol{\mu}&=\boldsymbol{W_1\hat\theta_1+(I-W_1)A\hat\theta_2} \\
\boldsymbol{\hat\theta_1}&=(\mathbf{X'X})^{-1}\mathbf{X'Z} \\
\boldsymbol{\hat\theta_1}&=(\mathbf{X'X})^{-1}\mathbf{X'Z} \\
\boldsymbol{V}&=\boldsymbol{((I-W_1)A)[A^TX^T(I+XX^T\sigma^2)^{-1}XA]^{-1}((I-W_1)A)^T+[X^TX+I/\sigma^2]^{-1}} \\
\boldsymbol{\sigma^2|Z}&\propto c(\mathbf{Z)\frac{|(I+XX^T\sigma^2)^{-1}|^{\frac12}}{|A^TX^T(I+XX^T\sigma^2)^{-1}XA|^{\frac12}}}\exp{\left\{\frac12Q(\boldsymbol{Z,XA\hat\theta_2,I+XX^T\sigma^2})\right\}}\pi(\sigma^2) &&(10)\\ 
\end{align}
\]

where \(Q(\boldsymbol{Z,\mu,\Sigma})=\boldsymbol{(Z-\mu)^T\Sigma^{-1}(Z-\mu)}\) and \(c(\mathbf{Z)}\) is a proportionality constant.

one starts with initial guesses at $\boldsymbol{\beta}$ and $\sigma^2$, simulates the $Z_i$ from (3), and then simulates $\boldsymbol{\beta}$ and $\sigma^2$ from the distributions (9) and (10)

### 4 Generalizations to a Multinomial response

#### 4.1 Ordered Categories

\(Y_1,..,Y_N\) are observed. \(Y_i\) takes one of $J$ ordered categories. \(p_{ij}=P[Y_i=j]\), we define the cumulative probabilities \(\eta_{ij}=\sum_{k=1}^jp_{ij},j=1,..,J-1\) <span style="color: red;">k????</span>

One popular regression model is given by \(\eta_{ij}=\Phi(\gamma_i-\mathbf{x}_i^T\boldsymbol{\beta}),i=1,..,N; j=1,..,J-1\)

A latent continuous variable $Z_i\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1)$. \(Y_i=j\text{ if } \gamma_{j-1}<Z_i\le\gamma_{j}, \gamma_{0}=-\infty,\gamma_{J}=\infty\)

 The posterior distribution of $\beta$ conditional on $y$ and $Z$ is given by the multivariate normal form (2)

\[
\begin{align} 
\pi(\boldsymbol{\beta,\gamma|y})&= C\pi(\boldsymbol{\beta})\prod_{i=1}^{N}\sum_{j=1}^{J}\mathbf{1}_{(y_i=j)}[\Phi (\gamma_{j}-\mathbf{x}_i^T\boldsymbol{\beta})-\Phi (\gamma_{j-1}-\mathbf{x}_i^T\boldsymbol{\beta})]\\
\pi(\boldsymbol{\beta,\gamma,Z|y})&= C\prod_{i=1}^{N}\left[\frac{1}{\sqrt{2\pi}}\exp(-\frac12(Z_i-\mathbf{x}_i^T\boldsymbol{\beta})^2)(\sum_{j=1}^{J}\mathbf{1}_{(Y_i=j)}\mathbf{1}_{(\gamma_{j-1}<Z_i\le\gamma_{j})})\right]&&(11)\\
Z_i|\boldsymbol{\beta,\gamma},y_i&\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1) \quad\text{truncated at the left(right) by } \gamma_{j-1}(\gamma_{j}) &&(12)\\
\gamma_j|\boldsymbol{Z,y,\beta,},\{\gamma_k,k\neq j\}&\propto  \prod_{i=1}^{N}\left[\mathbf{1}_{(Y_i=j)}\mathbf{1}_{(\gamma_{j-1}<Z_i\le\gamma_{j})})+\mathbf{1}_{(Y_i=j+1)}\mathbf{1}_{(\gamma_{j}<Z_i\le\gamma_{j+1})})\right]&&(13)\\
\end{align}
\]

(13) can be seen to be uniform on the interval \([\max \{\max \{Z_i: Y_i = j \}, \gamma_{j-1} \}, \min \{\min \{Z_i: Y_i = j + 1\}, \gamma_{j+1} \}]\). 

To implement the Gibbs sampler here, start with ($\beta,\gamma$) set equal to the MLE and simulate from the distributions (13), (12), and (1), in that order.

#### 4.2 Unordered Categories With a Latent Multinormal Distribution

We introduce independent latent variables \(Z_i=(Z_{i1},..,Z_{iJ})(J>2)\) and define \(Z_{ij}=\mathbf{x}_{ij}^T\boldsymbol{\beta}+\varepsilon_{ij},\ i=1,..,N;j=1,..,J\), whrre \(\varepsilon_i=(\varepsilon_{i1},..,\varepsilon_{iJ})^T\sim N_J(\mathbf{0,\Sigma_{J\times J}})\)

\(\mathbf{\Sigma}\) is parameterized in terms of a parameter vector $\theta$ of dimension not exceeding \(\frac12J(J-1)\).

On unit $i$ we observe one of $J$ possible outcomes with respective probabilities $p_{i1},..,p_{ij}$. Category $j$ is observed if $Z_{ij}>Z_{ik}$ for all $k\neq j$.

The multinomial logit model can be derived in this setup if and only if the errors \(\varepsilon_{ij}\) are a random sample from a Type I extreme value distribution. The multinomial probabilities are given by \(p_{ij}=P[\mathbf{x}_{ij}^T\boldsymbol{\beta}+\varepsilon_{ij}>\mathbf{x}_{ik}^T\boldsymbol{\beta}+\varepsilon_{ik}, \forall k\neq j]\)

\(\mathbf{Z=X}\boldsymbol{\beta+\varepsilon}\) where \(\varepsilon=(\varepsilon_{1}^T,..,\varepsilon_{N}^T)^T\sim N_{NJ}(\mathbf{0,I_N\otimes\Sigma})\)

\[
\begin{align} 
\boldsymbol{\beta|Z_{1:N},Y,\theta}&\sim N_k(\boldsymbol{\hat\beta_{Z}},(\mathbf{X'\Omega^{-1} X})^{-1}); \quad\boldsymbol{\hat\beta_{Z}}=(\mathbf{X'\Omega^{-1} X})^{-1}\mathbf{X'\Omega^{-1} Z}\\
\boldsymbol{Z_i|y,\beta,\theta,\{Z_i\}}&\sim N(\mathbf{x}_i^T\boldsymbol{\beta},\Sigma) \quad\text{such that
 the }y_i^{th}\text{ component of }Z_i\text{ is the maximum}  &&\\
\boldsymbol{\theta|Z_{1:N},Y,\beta}&\propto \boldsymbol{\pi(\theta)|\Omega|^{-\frac12}} \exp\left[-\frac12\boldsymbol{(Z-X\beta)^T\Omega^{-1}(Z-X\beta)}\right]&&(14)\\
\end{align}
\]

\(\mathbf{\Omega}^{-1}\) is a block diagonal matrix with \(\mathbf{\Sigma}^{-1}\) as the typical block. <span style="color: red;"> ???</span>


### 5.1 Finney Data



```{r,echo=T,collapse=T}
data(Finney)
Vol <- Finney$Vol; Rate <- Finney$Rate; Resp <- Finney$Resp
lVol <-log(Vol); lRate <- log(Rate)
# plotFdat <- Finney$plotFdat
# plotFdat(Rs=Resp,lV=lVol,lR=lRate,zc,zr,rob=F,cont=F)
plot(Vol,Rate,type="n",xlab="Vol",ylab="Rate")
points(Vol[Resp==0],Rate[Resp==0],pch=5, cex=1.2)
points(Vol[Resp==1],Rate[Resp==1],pch=16,cex=1.2)

finney <- data.frame(Finney[1:3])
finney[,3] <- as.factor(finney[,3])
table(finney[3])
n <- nrow(finney)
```



### Compare with the results after log transform

What is the benifit of log transform?

```{r,eval=F,echo=T,collapse=T}
fit.probit.lm.l <- lm(Resp~lVol+lRate)
fit.probit.glm.l <- glm(Resp~lVol+lRate,family=binomial(link="probit"))
fit.logit.glm.l <- glm(Resp~lVol+lRate,family=binomial(link="logit"))
comp <- fits.compare(fit.probit.lm.l, fit.probit.glm.l, fit.logit.glm.l)
comp
# plot(comp)
```

```{r,eval=F,echo=F}
summary(z.logit)
correl(z.logit)
covar(z.logit)
Rank(z.logit)
rscale(z.logit)
# kable(t(data_frame(weights(z.logit))))
```



```{r,eval=F,echo=F}
# From lecture5_sample;# response must have 3 or more levels
# fit.probit.plr <- polr(Resp~Vol+Rate, method = "probit",finney) 
```




```{r,echo=F,out.width='50%'}
#z.cub <- glm(Resp~lVol+lRate,family=binomial,method="cubinf", ufact=3.2)
# summary(z.cub)
# plot(z.cub, smooth=TRUE, ask=TRUE)
```



```{r,eval=F,echo=F,out.width='50%'}
## Adds a QQ-line for the values in x in the current plot.
x <- residuals(z.probit, type="deviance")
qqnorm(x, ylab = "Deviance Residuals")
QQline(x, lty = 2)
# Predictions provided by a model fit when method is "cubinf".
rVol <- runif(20,0.4,3.7); rRate <- runif(20,0.3,3.75)
newdat <- data.frame(lVol=log(rVol),lRate=log(rRate))
predict(z.probit, newdat, type="response")

```


### 5.2 Election Data


### 5.3 A Trivariate Probit Example




## Probit Binary

\[\Phi^{-1}(p_i)=\beta_0+\beta_1x_{1i}+\beta_2x_{2i},\ i=1,..,39\]

where $x_{1i}$ is the volume of air inspired, $x_{2i}$ is the rate of air inspired, and the binary outcome observed is the occurrence or nonoccurrence on a transient vasorestriction on the skin of the digits. $\beta\sim Unif$ prior is placed on the regression parameter

\(\Omega=(\mathbf{X}^T\mathbf{X})^{-1})\)

```{r}
# from Hoff (2009, Ch.12)
X <- cbind(1, Vol, Rate) # Design Matrix
p <- dim(X)[2]           # Parameter Dimension
iXX<-solve(t(X)%*%X)  # Inverse Matrix
n <- nrow(X)
V<-iXX*(n/(n+1))         # Hoff, p.156, 9.2.2: . A unit information prior is one that contains the same amount of information as that would be contained in only a single observation. ; 
# Note of Regresion Analysis p.23
cholV<-chol(V)     
```



The starting value $\beta^{(0)}$ is the least square estimate or MLE

$\beta^{(0)}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$. Assume y are numeric values

```{r, eval=T,collapse=T}
z<-qnorm(rank(Resp,ties.method="random")/(n+1)) # random initial Z
(beta <- as.vector(solve( t(X) %*% X ) %*% t(X) %*% z))
```

\(p_i=\Phi(\mathbf{x}_i^T\boldsymbol{\beta})\)

```{r,echo=T,collapse=T}
(prob <- as.vector(pnorm(t(X %*% beta),0,1)))
```

Expected value of Z \(\hat Z=\mathbf{x}_i^T\boldsymbol{\beta}\)

```{r, echo=T,collapse=T}
(ez <- as.vector(X %*% beta))
```

Generating Z

\(Z_i|\boldsymbol{y,\beta}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1)\) truncated by 0 at the \(\begin{cases}\text{left if}&y_i=1\\\text{right if}&y_i=0\end{cases}\)

<https://adzemski.github.io/r/statistics/R-package-truncated-normal/>

- Method 1

```{r,eval=T, echo=T,collapse=T}
set.seed(123); u<-runif(n,0,1)
(z<- ez + qnorm(ifelse(Resp==1,u+(1-u)*pnorm(0,ez,1),u*pnorm(0,ez,1))))
```

- Method 2

```{r,eval=T, echo=T,collapse=T}
tr_norm<-function(n,mu,v,truc_side='left'){ # Truncated Normal function
 u<-runif(n,0,1)
 if(truc_side=='left'){
 tr_norm_value <- u+(1-u)*pnorm(0,mu,sqrt(v))}
 if(truc_side=='right'){
 tr_norm_value<-  u*pnorm(0,mu,sqrt(v))}
  qnorm(tr_norm_value,mu,sqrt(v))
}
set.seed(123)
(z<- ifelse(Resp==1, tr_norm(1,ez,1,'left'),tr_norm(1,ez,1,'right')))
```

- Method 3

```{r,eval=T, echo=T,collapse=T}
for(j in 1:n){
  set.seed(123)
ifelse(Resp[j]==1,
z[j] <- rtruncnorm(1,a=0,b=Inf,mean=ez[j],sd=1),
z[j] <- rtruncnorm(1,a=-Inf,b=0,mean=ez[j],sd=1)
)}
z
```
- Method 4

```{r}
for(j in 1:n){
  set.seed(123)
ifelse(Resp[j]==1,
z[j] <- rtnorm(n, mean=ez[j], sd=1, lower=0, upper=Inf),
z[j] <- rtnorm(n, mean=ez[j], sd=1, lower=-Inf, upper=0)
)}
z
```


- Update $\boldsymbol{\hat\beta_Z}$

\(\boldsymbol{\hat\beta_Z}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Z}\)


```{r, echo=T,collapse=T}
solve(t(X)%*%X)%*%t(X)%*%z # New Beta
```



### Gibbs sampling 1

```{r}
M <- 25000; burnin <- 5000; step <- 20; size <- (M-burnin)/step
set.seed(123)

gibbs1 <- matrix(NA,ncol=p,nrow=size); Z<-matrix(NA,size,n) 
for(m in 1:M){  #Gibbs Sampler
  
# Update beta 1
# beta <- mvrnorm(1, solve(t(X)%*%X)%*%(t(X)%*%z), solve(t(X)%*%X)) # without n/(n+1)
# Update beta 2
# beta <- mvrnorm(1, V%*%(t(X)%*%z), V) # draw beta from Normal
# Update beta 3
beta<- V%*%(t(X)%*%z)+cholV%*%rnorm(p)

  #update z
ez<-(X%*%beta)  
# Truncated 1
u<-runif(n,0,1)
z<- ez + qnorm(ifelse(Resp==1,u+(1-u)*pnorm(0,ez,1),u*pnorm(0,ez,1)))

# Truncated 2
# z<- ifelse(Resp==1, tr_norm(1 ,ez, 1,'left'),tr_norm(1 ,ez, 1 ,'right'))# draw Z from truncated Normal

# Truncated 3
# for(j in 1:n){
# ifelse(Resp[j]==1,
# z[j] <- rtruncnorm(1,a=0,b=Inf,mean=ez[j],sd=1),
# z[j] <- rtruncnorm(1,a=-Inf,b=0,mean=ez[j],sd=1)
# )}

# Truncated 4    # slow
# for(j in 1:n){
# ifelse(Resp[j]==1,
# z[j] <- rtnorm(1, mean=ez[j], sd=1, lower=0, upper=Inf),
# z[j] <- rtnorm(1, mean=ez[j], sd=1, lower=-Inf, upper=0)
# )}

if(m%%step==0 & m>burnin) { gibbs1[(m-burnin)/step,] <- t(beta); Z[(m-burnin)/step,]<- z} 
}
```



Plot Gibbs 1

```{r,echo=F,include=T, fig.width=9, fig.height=6, fig.align='center'}
lab<-c(expression(beta[0]),expression(beta[1]),expression(beta[2]))
laby<-c("density","","")

par(mfrow=c(3,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
for(j in 1:p){
plot(gibbs1[,j],  ylab=lab[j],main="",pch=1,cex=0.1,
     xlab="Gibbs iteration (k)",col="cornflowerblue")  
plot(cumsum(gibbs1[,j])/(1:size),  ylab=lab[j],main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
#hist(gibbs.mat[-burnin,j],freq =F,xlab=paste("distribution of est. for beta",j-1),main="",col="cornflowerblue")
plot(density(gibbs1[,j],adj=2),lwd=2,main="",col="cornflowerblue",
    xlab=lab[j],ylab="density")
abline(v=quantile(gibbs1[,j],c(0.025,0.975)),col="gray",lwd=1)
}
```

### Gibbs sampling 2 , refer to (Hoff, 2009, Ch.12)


```{r,echo=T,include=T}
#### probit regression
## setup
 set.seed(123)
 beta<-rep(0,p) 
 z<-qnorm(rank(Resp,ties.method="random")/(n+1)) # random initial Z
# mu<-0 ; sigma<-100  
## MCMC
gibbs2<-matrix(NA,size,p) ; Z<-matrix(NA,size,n) ; ac<-0
for(m in 1:M) {
  #update beta
  beta<- V%*%( t(X)%*%z ) + cholV%*%rnorm(p)  # ???

  #update z
  ez<-X%*%beta
  u<-runif(n,0,1)
  z<- ez + qnorm(ifelse(Resp==1,u+(1-u)*pnorm(0,ez,1),u*pnorm(0,ez,1)))
    
  #help mixing
c<-rnorm(1,0,n^(-1/3))  #  sd responding the sample size ????
zp<- z+c 
lhr <-  sum(dnorm(zp,ez,100,log=T) - dnorm(z,ez,100,log=T) ) 
if(log(runif(1))<lhr) { z<-zp ; ac<-ac+1 }              
  if(m%%step==0 & m>burnin) 
  { 
    gibbs2[(m-burnin)/step,]<-  beta
    Z[(m-burnin)/step,]<- z
  }
} 
```

Plot Gibbs 2

```{r,echo=F,include=T, fig.width=9, fig.height=6, fig.align='center'}
par(mfrow=c(3,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
for(j in 1:p){
plot(gibbs2[,j],  ylab=lab[j],main="",pch=1,cex=0.1,
     xlab="Gibbs iteration (k)",col="cornflowerblue")  
plot(cumsum(gibbs2[,j])/(1:size),  ylab=lab[j],main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
#hist(gibbs.mat[-burnin,j],freq =F,xlab=paste("distribution of est. for beta",j-1), main="",col="cornflowerblue")
plot(density(gibbs2[,j],adj=2),lwd=2,main="",col="cornflowerblue",
    xlab=lab[j],ylab="density")
abline(v=quantile(gibbs2[,j],c(0.025,0.975)),col="gray",lwd=1)
}
```

```{r,echo=F,eval=F, message=F, warning=F, fig.width=6, fig.height=3, fig.align='center'}
par(mfrow=c(2,3),mar=c(3,3,.5,.5),mgp=c(1.70,.70,0))
plot(gibbs.mat[,(n+1)],  ylab=expression(beta^{(k)}),main="",pch=20,cex=0.3,
     xlab="Gibbs iteration (k)",col="cornflowerblue")
plot(cumsum(gibbs.mat,(n+1)])/(1:(S/2)),  ylab=expression(E(beta)),main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
hist(gibbs.mat[,(n+1)],  xlab=expression(paste("distribution of est. for ",beta)),
     main="",col="cornflowerblue")
abline(v=quantile(gibbs.mat[,(n+1)],c(0.025,0.975)),col="red",lwd=1)

u <- seq(0.01,4,length=1000)
d <- dgamma(u,shape=alpha,rate=pumps.par["50%","Beta"])
plot(u,d,type="l",xlab=expression(lambda),ylab="density",col="cornflowerblue",lwd=2,pch=20,cex=0.7)
abline(v=pumps.quant["50%",1:10],col="grey",lwd=1)

par(las=1,mar=c(3,5,.5,.5))
plot(c(0,4),c(1,10),type="n",xlab=expression(lambda),ylab=" ",axes=FALSE)
axis(side=1)
axis(side=2,at=1:10,labels=paste("Pump #",10:1,sep=""))
segments(pumps.quant["2.5%",1:10],10:1,pumps.quant["97.5%",1:10],10:1,col="cornflowerblue",lwd=2,pch=20,cex=0.7)
segments(pumps.quant["50%",1:10],(10:1)-0.25,pumps.quant["50%",1:10],(10:1)+0.25,col="cornflowerblue",lwd=2,pch=20,cex=0.7)
```



```{r,eval=F,include=F, message=F, warning=F, fig.width=9, fig.height=3, fig.align='center'}
par(mfrow=c(1,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))

for(j in 1:p){
plot(density(gibbs2[,j],adj=2),lwd=2,main="",#xlim=c(-10,5),
    xlab=lab[j],ylab=laby[j],col="cornflowerblue")
sd<-sqrt(  solve(t(X)%*%X/n)[j,j] )
E <- V%*%(t(X)%*%z)
x<-seq(min(gibbs2[,j]),max(gibbs2[,j]),length=100)
lines(x,dnorm(x,E[j],sd),lwd=2,col="gray")
if(j==3) {legend(2.1,1,legend=c("prior","posterior"),lwd=c(2,2),col=c("gray","cornflowerblue"),bty="n")}
}
```




## t link

\(\boldsymbol{\beta|y,Z,\lambda,\nu}\sim N_k(\boldsymbol{\hat\beta_{Z,\lambda}},(\mathbf{X'WX})^{-1}); \quad\boldsymbol{\hat\beta_{Z,\lambda}}=(\mathbf{X'WX})^{-1}\mathbf{X'WZ},\ \mathbf{W}=\mathrm{diag}(\lambda_i)\)

```{r}
lambda <- rep(1,n)
w <- diag(lambda)
z<-qnorm(rank(Resp,ties.method="random")/(n+1)) # random initial Z
(beta <- as.vector(solve(t(X)%*%w%*%X)%*%t(X)%*%w%*%z)) 
(ez <- as.vector(X %*% beta))
```

Generating Z

\(Z_i|\boldsymbol{y,\beta,\lambda}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},\lambda_i^{-1})\) truncated by 0 at the \(\begin{cases}\text{left if}&y_i=1\\\text{right if}&y_i=0\end{cases}\)

```{r,eval=T, echo=T,collapse=T}
set.seed(123); u<-runif(n,0,1)
(z<- ez + qnorm(ifelse(Resp==1,u+(1-u)*pnorm(0,ez,lambda^(-1)),u*pnorm(0,ez,lambda^(-1)))))
```

\(\boldsymbol{\lambda_{1:N}|y,Z,\beta,\nu}\sim Gamma(\frac{\nu+1}{2},\frac{2}{\nu+(Z_i-\mathbf{x}_i^T\boldsymbol{\beta})^2})\)

```{r}
nu <- 8; set.seed(123)
for(i in 1:n){
lambda[i] <- rgamma(1,shape = (nu+1)/2,rate = ((nu+(z-X %*% beta)^2)/2)[i] )
}
w <- diag(lambda)
```


\(\boldsymbol{\hat\beta_Z}=(\mathbf{X}^TW\mathbf{X})^{-1}\mathbf{X}^TW\mathbf{Z}\)


```{r, echo=T,collapse=T}
t(solve(t(X)%*%w%*%X)%*%t(X)%*%w%*%z) # New E[Beta]
solve(t(X)%*%w%*%X)                   #     V[Beta]
```


```{r}
lambdavec <- rgamma(n,1,1)
prod.nu <- function(nu,lambdavec){
  n <- length(lambdavec)
  cnu <- (gamma(nu/2)*((nu/2)^(nu/2)))^(-1) # ^(-n)
  prod(cnu * lambdavec^(nu/2-1) * exp(-nu*lambdavec/2)) #  
}

prod.nuvec <- Vectorize(prod.nu,vectorize.args = "nu")
nuprobs <- prod.nuvec(1:20,lambda)

sample(1:20,1,prob =nuprobs[1:20])

```


```{r,eval=T}
M <- 25000; burnin <- 5000; step <- 20; size <- (M-burnin)/step
set.seed(123)

gibbs3 <- matrix(NA,ncol=p,nrow=size); Z<-matrix(NA,size,n) 
for(m in 1:M){  #Gibbs Sampler

  #update z
ez<-(X%*%beta)  
# Truncated 1
u<-runif(n,0,1)
z<- ez + qnorm(ifelse(Resp==1,u+(1-u)*pnorm(0,ez,lambda^(-1/2)),u*pnorm(0,ez,lambda^(-1/2))))  
  
# Update beta 1
beta <- mvrnorm(1, t(solve(t(X)%*%w%*%X)%*%t(X)%*%w%*%z)*n/(n+1), solve(t(X)%*%w%*%X)*n/(n+1)) # without n/(n+1)
# Update beta 2
# beta <- mvrnorm(1, V%*%(t(X)%*%z), V) # draw beta from Normal
# Update beta 3
# beta<- V%*%(t(X)%*%z)+cholV%*%rnorm(p)

  #update lambda
for(i in 1:n){
lambda[i] <- rgamma(1,shape = (nu+1)/2 , rate = ((nu+(z-X %*% beta)^2)/2)[i] )
}
# lambda <- rgamma(n,shape = (nu+1)/2 , rate = ((nu+(z-X %*% beta)^2)/2) )

w <- diag(lambda)

 # Update nu 

if(m%%step==0 & m>burnin) { gibbs3[(m-burnin)/step,] <- t(beta); Z[(m-burnin)/step,]<- z} 
}
```

Plot Gibbs 3

```{r,echo=F,include=T, fig.width=9, fig.height=6, fig.align='center'}
par(mfrow=c(3,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
for(j in 1:p){
plot(gibbs3[,j],  ylab=lab[j],main="",pch=1,cex=0.1,
     xlab="Gibbs iteration (k)",col="cornflowerblue")  
plot(cumsum(gibbs3[,j])/(1:size),  ylab=lab[j],main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
#hist(gibbs.mat[-burnin,j],freq =F,xlab=paste("distribution of est. for beta",j-1), main="",col="cornflowerblue")
plot(density(gibbs3[,j],adj=2),lwd=2,main="",col="cornflowerblue",
    xlab=lab[j],ylab="density")
abline(v=quantile(gibbs3[,j],c(0.025,0.975)),col="gray",lwd=1)
}
```




\(\boldsymbol{\hat\beta}\approx\frac1m\sum_{i=1}^m(\boldsymbol{\beta|Z^{(i)}})\)

```{r,echo=T,collapse=T}
(g1 <- colMeans(gibbs1,na.rm = T))
(g2<-apply(gibbs2,2,mean))
(g3<-apply(gibbs3,2,mean))
g1.sd<- apply(gibbs1,2,sd,na.rm = T)
g2.sd<-apply(gibbs2,2,sd)
g3.sd<-apply(gibbs3,2,sd)
```

- Marginal Effects

\(p_i=\mathbf{x}_i^T\boldsymbol{\beta}\)

\(\beta_{marg}=\overline{\phi(p_i)}\boldsymbol{\beta}\)

```{r,echo=T,collapse=T}
(g1.marg <- mean(dnorm(X %*% t(gibbs1)))*g1)
(g2.marg <- mean(dnorm(X %*% t(gibbs2)))*g2)
(g3.marg <- mean(dnorm(X %*% t(gibbs3)))*g3)
```



### Rank likelihood regression 


```{r,echo=T,collapse=T}
source("rlreg.R")
rfit<-treg(Resp,X)
```

```{r}
(rl <- apply(rfit$BETA,2,mean)) # function(x) c(mean(x),sd(x)))
(rl.sd <- apply(rfit$BETA,2,sd))

(rl.marg <- mean(dnorm(X %*% t(rfit$BETA)))*rl)
```




### GLM


```{r,eval=T,echo=T,collapse=T}
fit.probit.glm <- glm(Resp~Vol+Rate,family=binomial(link="probit"))
glm.p<- coef(fit.probit.glm)
glm.p.sd <- summary(fit.probit.glm)$coef[,2] # sqrt(diag(vcov(fit.probit.glm)))
# fit.probit.lm <- lm(Resp~Vol+Rate)
# fit.logit.glm <- glm(Resp~Vol+Rate,family=binomial(link="logit"))
#comp <- fits.compare(fit.probit.lm, fit.probit.glm, fit.logit.glm)
#comp
#plot(comp)
(glm.p.marg <- mean(dnorm(X %*% glm.p))*glm.p)
```



### nimble

- Code 1

```{r}
code <- nimbleCode({
  beta0 ~ dnorm(0, sd = 100)
  beta1 ~ dnorm(0, sd = 100)
  beta2 ~ dnorm(0, sd = 100)
#  sigma ~ dunif(0, 100)        
  for(i in 1:n) {
    y[i] ~ dbern(p[i])    
    probit(p[i])  <- beta0 + beta1*x1[i] + beta2*x2[i] # sd = sigma

  }
})
## extract data for two predictors and center for better MCMC performance
x1 <- Vol #- mean(Vol)
x2 <- Rate # - mean(Rate)

constants <- list(n = n)
data <- list(y = Resp, x1 = x1, x2 = x2)
inits <- list(beta0 = -0.5, beta1 = 0.1, beta2 = 0.1) # beta0 =mean(Resp), sigma = 1
model <- nimbleModel(code, constants = constants, data = data, inits = inits) # build model
```

```{r}
mcmc.out <- nimbleMCMC(code = code, constants = constants,
                       data = data, inits = inits,
                       nchains = 4, niter = 2000,
                       summary = TRUE, WAIC = T,
                       monitors = c('beta0','beta1','beta2')) #,'sigma'
names(mcmc.out)
```

```{r}
nimble.p <- mcmc.out$summary$all.chains[,1]
nimble.p.sd <- mcmc.out$summary$all.chains[,3]
```

```{r}
# nimble.sample<- rbind(mcmc.out$samples$chain1[1901:2000,],mcmc.out$samples$chain2[1901:2000,],
#                       mcmc.out$samples$chain3[1901:2000,],mcmc.out$samples$chain4[1901:2000,])
(nimble.p.marg <- mean(dnorm(X %*% (nimble.p)))*nimble.p)
```


- Code 2 Use `inprod`

```{r}
code <- nimbleCode({
  beta0 ~ dnorm(0, sd = 100)
  for(j in 1:k)
    beta[j] ~ dnorm(0, sd = 100)
#  sigma ~ dunif(0, 100)  
  for(i in 1:n) {
  probit(p[i]) <- beta0 + inprod(beta[1:k], x[i, 1:k]) #, sd = sigma)
    y[i] ~ dbern(p[i])
  }
})

k <- 2
# cX <- sweep(finney[,1:2], 2, colMeans(finney[,1:2]))  # center for better MCMC performance

constants <- list(n = n, k = 2, x =finney[,1:2]) # cX
data <- list(y = Resp)
inits <- list(beta0 = -0.5, beta = rep(0, k), sigma = 0.5)
model <- nimbleModel(code, constants = constants, data = data, inits = inits)
```



```{r}
mcmc.out <- nimbleMCMC(code = code, constants = constants,
                       data = data, inits = inits,
                       nchains = 2, niter = 1000,
                       summary = TRUE, WAIC = T,
                       monitors = c('beta0','beta')) # ,'beta2','sigma'
names(mcmc.out)
```

```{r}
mcmc.out$summary$all.chains
mcmc.out$WAIC 
```



- Code 3 Use Matrix Algebra

```{r,eval=F}
code <- nimbleCode({
     beta0 ~ dnorm(0, sd = 100)
     beta[1:k] ~ dmnorm(zeros[1:k], omega[1:k, 1:k])
#     sigma ~ dunif(0, 100)
     linpred[1:n] <- (x[1:n, 1:k] %*% beta[1:k])[1:n,1]
     for(i in 1:n) {
     probit(p[i]) <-  beta0 + linpred[i,1]   #~ dnorm(, sd = sigma)
     y[i] ~ dbern(p[i])
       }
})

constants <- list(n = n, k = k, x = finney[,1:2], zeros = rep(0, k), omega = 0.0001 * diag(k))
data <- list(y = Resp)
inits <- list(beta0 = -0.5, beta = rep(0, k), sigma = 0.5)
model <- nimbleModel(code, constants = constants, data = data, inits = inits)
```

> Error in replaceConstantsRecurse(x, constEnv, constNames) : 'list' object cannot be coerced to type 'double'

```{r}
mcmcConf <- configureMCMC(model)
mcmcConf$printSamplers()
```




### brms

```{r,results=F}
brms.p <- brm(Resp ~ Vol + Rate, data =finney, family = bernoulli("probit"),silent = T, refresh=0) # probit_approx
```

```{r}
summary(brms.p)
brm.p <- fixef(brms.p)[,1]
brm.p.sd <- fixef(brms.p)[,2]
```

```{r}
(brm.p.marg <- mean(dnorm(X %*% brm.p))*brm.p)
```



### Comparing the results

```{r,echo=T,collapse=T}
par.p<- cbind(rl,g1,g3,glm.p,g2,nimble.p,brm.p)
rownames(par.p) <- c("Beta0","Beta1","Beta2")
pander::pander(round((par.p),4))
```




```{r,eval=T,echo=T}
par.p.sd<- rbind(g1.sd,g2.sd,g3.sd,rl.sd,glm.p.sd, nimble.p.sd,brm.p.sd)
colnames(par.p.sd) <- c("Beta0","Beta1","Beta2")
pander::pander(round(t(par.p.sd),4))
```

```{r,echo=F,include=T, fig.width=9, fig.height=3, fig.align='center'}
par(mfrow=c(1,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
ymx<-c(0.3,0.75,1.25)
lab<-c(expression(beta[1]),expression(beta[2]),expression(beta[3]))
laby<-c("density","","")
for(j in 1:3) {
plot(density(rfit$BETA[,j],adj=2),lwd=2,main="",
 xlab=lab[j],col="gray",ylim=c(0,ymx[j]),ylab=laby[j])
lines(density(gibbs1[,j],adj=2),col="cornflowerblue",lwd=2,lty=1)
lines(density(gibbs2[,j],adj=2),col="cyan",lwd=2)
lines(density(gibbs3[,j],adj=2),col="steelblue",lwd=2)
if(j==3) {
 legend(1.75,1.25,legend=c("Rank","Gibbs1","Gibbs2","Gibbs3"), lty=c(1,1,1,1),
       lwd=c(2,2,2,2),col=c("gray","cornflowerblue","cyan","steelblue"))
          } 
               }
```



```{r,echo=F,include=T,fig.show='hold', fig.width=9, fig.height=6, fig.align='center'}
par(mar=c(5,3,1,1),mgp=c(1.75,.75,0)); par(mfrow=c(4,3))
lab<-c(expression(beta[0]),expression(beta[1]),expression(beta[2]))
for(j in 1:3) {
acf(rfit$BETA[,j],ci.col="dodgerblue4",xlab=lab[j])}  
for(j in 1:3) {
acf(gibbs1[,j],ci.col="dodgerblue4",xlab=lab[j])} 
for(j in 1:3) {
acf(gibbs2[,j],xlab=lab[j],ci.col="dodgerblue4")}
for(j in 1:3) {
acf(gibbs3[,j],xlab=lab[j],ci.col="dodgerblue4")}
mtext("autocorrelation functions with Rank Reg(above),Gibbs1(middle) ,Gibbs2(below)",side=1,line=-1,outer=T)
```


## Logit Binary

### brms

```{r,results=F}
brms.l <- brm(Resp ~ Vol + Rate, data =finney, family = bernoulli("logit"),silent = T, refresh=0)
```

```{r}
summary(brms.l)
brm.l <- fixef(brms.l)[,1]
brm.l.sd <- fixef(brms.l)[,2]
```


```{r}
brm.sample <- data.frame()
for (i in 1:4) {
brm.sample <- rbind(brm.sample,data.frame(brms.l$fit@sim$samples[[i]])[1901:2000,1:3])
}
prob.l<-  mean(X %*% t(brm.sample))
(brm.l.marg <-prob.l*(1-prob.l)*brm.l)
(brm.l.marg <- mean(dlogis(X %*% t(brm.sample)))*brm.l)
(brm.l.marg <- mean(dlogis(X %*% brm.l))*brm.l)
```


```{r,eval=F}
inv_logit <- function(x) 1 / (1 + exp(-x))
fit.brms2 <- brm(bf(Resp ~ inv_logit(eta), 
                     eta ~ Vol + Rate, nl = T),
  data = finney, family = bernoulli("identity"), 
  prior = prior(normal(0, 100), nlpar = "eta"),silent = T, refresh=0
)

fit.brms3 <- brm(
  bf(Resp ~ guess + (1 - guess) * inv_logit(eta), 
    eta ~  Vol + Rate, guess ~ 1, nl = T),
  data = finney, family = bernoulli("identity"), 
  prior = c(
    prior(normal(0, 100), nlpar = "eta"),
    prior(beta(1, 1), nlpar = "guess", lb = 0, ub = 1)
  ),silent = T, refresh=0
)
summary(fit.brms2)
summary(fit.brms3)
```



### nimble

```{r}
## define the model
code <- nimbleCode({
    beta0 ~ dnorm(0, sd = 100)
    beta1 ~ dnorm(0, sd = 100)
    beta2 ~ dnorm(0, sd = 100)    
#    sigma_RE ~ dunif(0, 1000)
    for(i in 1:n) {
#        beta2[i] ~ dnorm(0, sd = sigma_RE)
        logit(p[i]) <- beta0 + beta1 * x1[i] + beta2 *x2[i]
        y[i] ~ dbern(p[i])
    }
})
## constants, data, and initial values
x1 <- Vol # - mean(Vol)
x2 <- Rate # - mean(Rate)
constants <- list(n = n, x1 = x1, x2 = x2)
data <- list(y = Resp)
inits <- list(beta0 = -0.5, beta1 = 0.1, beta2 = .1)

## create the model object
Rmodel <- nimbleModel(code=code, constants=constants, data=data, inits=inits, check = FALSE)
```

```{r,eval=F}
Rmcmc <- buildMCMC(Rmodel)
#Compile the model and MCMC algorithm
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)
#Execute MCMC algorithm and extract samples
Cmcmc$run(1000)
samples <- as.matrix(Cmcmc$mvSamples)
```

```{r,eval=F}
mcmcConf <- configureMCMC(Rmodel)
mcmcConf$printSamplers()
```

```{r}
mcmc.out <- nimbleMCMC(code = code, constants = constants,
                       data = data, inits = inits,
                       nchains = 4, niter = 2000,
                       summary = TRUE, WAIC = T,
                       monitors = c('beta0','beta1','beta2')) #,'sigma''sigma_RE',
```

```{r}
nimble.l <- mcmc.out$summary$all.chains[,1]
nimble.l.sd <- mcmc.out$summary$all.chains[,3]
```

```{r}
nimble.sample<- rbind(mcmc.out$samples$chain1[1901:1000,],mcmc.out$samples$chain2[1901:1000,],
                      mcmc.out$samples$chain3[1901:1000,],mcmc.out$samples$chain4[1901:1000,])
prob.l<-  mean(X %*% t(nimble.sample))
(glm.l.marg <-prob.l*(1-prob.l)*nimble.l)
(nimble.l.marg <- mean(dlogis(X %*% nimble.l))*nimble.l)
```



### GLM


```{r,eval=T,echo=T,collapse=T}
fit.l.glm <- glm(Resp~Vol+Rate,family=binomial(link="logit"))
glm.l<- coef(fit.l.glm)
glm.l.sd <- summary(fit.l.glm)$coef[,2] # sqrt(diag(vcov(fit.probit.glm)))
```

```{r}
prob.l<-  mean(X %*% glm.l)
(glm.l.marg <-prob.l*(1-prob.l)*glm.l)

(glm.l.marg <-mean(dlogis(X %*% glm.l))*glm.l)
```



- Another method

```{r}
library(mfx)
(mfx.l <- logitmfx(Resp~Vol+Rate,finney))
mfx.l$mfxest[,1]/2
```


### mlogit

```{r,eval=T,echo=T}
finney.mlogit <-dfidx(finney, choice = "Resp", shape='wide') # varying = c(1:2), 
#finney.mlogit <- mlogit.data(finney, choice='Resp', shape='wide', varying=1:2) #
fit.mlogit <- mlogit(Resp~1|(Vol+Rate),finney.mlogit, probit=F) #|0 ,reflevel=5,seed=20,R=100
# fit.mlogit <- mlogit(Resp~Vol+Rate,finney.mlogit, probit=T) 
mlogit<- coef(fit.mlogit)
mlogit.sd <- sqrt(diag(vcov(fit.mlogit)))
mlogit.marg <- rep(NA,3)
mlogit.marg[2] <- colMeans(effects(fit.mlogit, covariate = "Vol", data = finney.mlogit))[2] #type = "ar", 
mlogit.marg[3] <- colMeans(effects(fit.mlogit, covariate = "Rate",  data = finney.mlogit))[2]
```




### Comparing the results

```{r,echo=F,collapse=T}
par.l<- cbind(brm.l,nimble.l,glm.l,mlogit)
rownames(par.l) <- c("Beta0","Beta1","Beta2")
pander::pander(round((par.l),4))
```

```{r,eval=T,echo=F}
par.l.sd<- rbind(brm.l.sd,nimble.l.sd,glm.l.sd,mlogit.sd)
colnames(par.l.sd) <- c("Beta0","Beta1","Beta2")
pander::pander(round(t(par.l.sd),4))
```

\(p_i=\mathbf{x}_i^T\boldsymbol{\beta}\)

\(\beta_{marg}=\overline{\phi(p_i)}\boldsymbol{\beta}\)

\(\beta_{marg}=\overline{logit(p_i)}\boldsymbol{\beta}\)

\(\beta_{marg}=\bar p(1-\bar p))\boldsymbol{\beta}\)

```{r,echo=F,collapse=T}
par.marg<- rbind(rl.marg,nimble.p.marg,g1.marg,g2.marg,g3.marg,glm.p.marg,glm.l.marg,mlogit.marg,nimble.l.marg,brm.p.marg,brm.l.marg)
colnames(par.marg) <- c("Beta0","Beta1","Beta2")
pander::pander(round((par.marg),4))
```


### `glmer`

```{r,eval=T,echo=T}
# From https://data.princeton.edu/
glmer.p <- glmer(Resp~Vol+(1|Rate), family=binomial(link = "probit"))#, nAGQ = 12, REML=FALSE
summary(glmer.p)
```

```{r,eval=T,echo=T}
glmer.l <- glmer(Resp~Vol+(1|Rate), family=binomial(link = "logit")) 
summary(glmer.l)
```



## Ordinal-Panel

```{r}
library(foreign)
Panel101 <- read.dta("https://dss.princeton.edu/training/Panel101.dta")
```


```{r}
kable(summary(Panel101))
```



### `nnet`

```{r}
nnet::multinom(opinion ~ x1 + x2 + x3, data=Panel101)%>%summary() 
```


### `brms`

https://kevinstadler.github.io/blog/bayesian-ordinal-regression-with-random-effects-using-brms/

```{r,results=F}
# brm(self ~ age + (1|id), data=d, family=cumulative("logit"), threshold="flexible") #
# Families cumulative, cratio(continuation ratio), sratio(stopping ratio), and acat(adjacent category) 
#          support logit, probit, probit_approx, cloglog, and cauchit.
# Families categorical, multinomial, and dirichlet support logit.
# Panel101$y_ord<-match(Panel101[,8],levels(Panel101[,8]))
brm.ord <- brm(opinion ~ x1 + x2 + x3, data=Panel101, family=categorical,silent = T, refresh=0)# , refcat ="Str disag"
```

```{r}
summary(brm.ord)
brm.ord.l <- fixef(brm.ord)[,1]
brm.ord.l.sd <- fixef(brm.ord)[,2]
```

### `polr`

```{r}
polr.p <- polr(opinion ~ x1 + x2 + x3, data=Panel101, Hess=TRUE,method = "probit")
summary(polr.p)
library(erer)
ocME(polr.p)# , x.mean=TRUE
```

```{r}
polr.l <- polr(opinion ~ x1 + x2 + x3, data=Panel101, Hess=TRUE,method = "logistic")
summary(polr.l)
ocME(polr.l)
```

### Gibbs

```{r}
X<-sapply(Panel101[,5:7], as.numeric)
X <- cbind(1,X)
y<-Panel101[,8]
name.rank<- levels(Panel101[,8])
y<-match(Panel101[,8],name.rank)
uranks<-sort(unique(y))
n<-dim(X)[1] ; p<-dim(X)[2]
iXX<-solve(t(X)%*%X)  ; V<-iXX*(n/(n+1)) ; cholV<-chol(V)
```


```{r}
## setup
set.seed(1)
beta<-rep(0,p) 
z<-qnorm(rank(y,ties.method="random")/(n+1))
g<-rep(NA,length(uranks)-1)
K<-length(uranks)
mu<-rep(0,K-1) ; sigma<-rep(100,K-1) 

## MCMC
M<-25000; burnin <- 5000; step <- 20; size <- (M-burnin)/step
BETA<-matrix(NA,size,p) ; Z<-matrix(NA,size,n) ; ac<-0
for(m in 1:M) 
{
  #update g 
  for(k in 1:(K-1)) 
  {
  a<-max(z[y==k])
  b<-min(z[y==k+1])
  u<-runif(1, pnorm( (a-mu[k])/sigma[k] ),
              pnorm( (b-mu[k])/sigma[k] ) )
  g[k]<- mu[k] + sigma[k]*qnorm(u)
  }
  #update beta
  E<- V%*%( t(X)%*%z )
  beta<- cholV%*%rnorm(p) + E
  #update z
  ez<-X%*%beta
  a<-c(-Inf,g)[ match( y-1, 0:K) ]
  b<-c(g,Inf)[y]  
  u<-runif(n, pnorm(a-ez),pnorm(b-ez) )
  z<- ez + qnorm(u)
  #help mixing
  c<-rnorm(1,0,n^(-1/3))  
  zp<-z+c ; gp<-g+c
  lhr<-  sum(dnorm(zp,ez,1,log=T) - dnorm(z,ez,1,log=T) ) + 
         sum(dnorm(gp,mu,sigma,log=T) - dnorm(g,mu,sigma,log=T) )
  if(log(runif(1))<lhr) { z<-zp ; g<-gp ; ac<-ac+1 }
if(m%%step==0 & m>burnin) {BETA[(m-burnin)/step,] <- beta; Z[(m-burnin)/step,]<- z} 
} 
```

```{r}
(g4 <- apply(BETA, 2, mean))
(g4.marg <-mean(dnorm(X %*% g4))*g4)
```

```{r,echo=F,include=T, fig.width=9, fig.height=6, fig.align='center'}
lab<-c(expression(beta[0]),expression(beta[1]),expression(beta[2]),expression(beta[3]))
laby<-c("density","","")

par(mfrow=c(4,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
for(j in 1:p){
plot(BETA[,j],  ylab=lab[j],main="",pch=1,cex=0.1,
     xlab="Gibbs iteration (k)",col="cornflowerblue")  
plot(cumsum(BETA[,j])/(1:size),  ylab=lab[j],main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
#hist(gibbs.mat[-burnin,j],freq =F,xlab=paste("distribution of est. for beta",j-1),main="",col="cornflowerblue")
plot(density(BETA[,j],adj=2),lwd=2,main="",col="cornflowerblue",
    xlab=lab[j],ylab="density")
abline(v=quantile(BETA[,j],c(0.025,0.975)),col="gray",lwd=1)
}
```

<!--
### Social mobility data

```{r}
load("socmob.RData") 
yincc<-match(socmob$INC,sort(unique(socmob$INC)))
ydegr<-socmob$DEGREE+1
yage<-socmob$AGE
ychild<-socmob$CHILD
ypdeg<-1*(socmob$PDEG>2)
tmp<-lm(ydegr~ychild+ypdeg+ychild:ypdeg)
```


```{r}
#par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
#par(mfrow=c(1,2))
plot(table(socmob$DEG+1)/sum(table(socmob$DEG+1)),
  lwd=2,type="h",xlab="DEG",ylab="probability")
plot(table(socmob$CHILD)/sum(table(socmob$CHILD)),lwd=2,type="h",xlab="CHILD",ylab="probability" )
```


```{r}
X<-cbind(ychild,ypdeg,ychild*ypdeg)
y<-ydegr
keep<- (1:length(y))[ !is.na( apply( cbind(X,y),1,mean) ) ]
X<-X[keep,] ; y<-y[keep]
ranks<-match(y,sort(unique(y))) ; uranks<-sort(unique(ranks))
n<-dim(X)[1] ; p<-dim(X)[2]
iXX<-solve(t(X)%*%X)  ; V<-iXX*(n/(n+1)) ; cholV<-chol(V)
```


```{r}
## setup
set.seed(1)
beta<-rep(0,p) 
z<-qnorm(rank(y,ties.method="random")/(n+1))
g<-rep(NA,length(uranks)-1)
K<-length(uranks)
mu<-rep(0,K-1) ; sigma<-rep(100,K-1) 

## MCMC
M<-25000; burnin <- 5000; step <- 20; size <- (M-burnin)/step
BETA<-matrix(NA,size,p) ; Z<-matrix(NA,size,n) ; ac<-0
for(m in 1:M) 
{

  #update g 
  for(k in 1:(K-1)) 
  {
  a<-max(z[y==k])
  b<-min(z[y==k+1])
  u<-runif(1, pnorm( (a-mu[k])/sigma[k] ),
              pnorm( (b-mu[k])/sigma[k] ) )
  g[k]<- mu[k] + sigma[k]*qnorm(u)
  }

  #update beta
  E<- V%*%( t(X)%*%z )
  beta<- cholV%*%rnorm(p) + E

  #update z
  ez<-X%*%beta
  a<-c(-Inf,g)[ match( y-1, 0:K) ]
  b<-c(g,Inf)[y]  
  u<-runif(n, pnorm(a-ez),pnorm(b-ez) )
  z<- ez + qnorm(u)


  #help mixing
  c<-rnorm(1,0,n^(-1/3))  
  zp<-z+c ; gp<-g+c
  lhr<-  sum(dnorm(zp,ez,1,log=T) - dnorm(z,ez,1,log=T) ) + 
         sum(dnorm(gp,mu,sigma,log=T) - dnorm(g,mu,sigma,log=T) )
  if(log(runif(1))<lhr) { z<-zp ; g<-gp ; ac<-ac+1 }
if(m%%step==0 & m>burnin) {BETA[(m-burnin)/step,] <- beta; Z[(m-burnin)/step,]<- z} 
} 

```

```{r}

plot(X[,1]+.25*(X[,2]),Z[1000,],
 pch=15+X[,2],col=c("gray","black")[X[,2]+1],
 xlab="number of children",ylab="z", ylim=range(c(-2.5,4,Z[1000,])),
    xlim=c(0,9))

beta.pm<-apply(BETA,2,mean)
ZPM<-apply(Z,2,mean)
abline(0,beta.pm[1],lwd=2 ,col="gray")
abline(beta.pm[2],beta.pm[1]+beta.pm[3],col="black",lwd=2 )
legend(5,4,legend=c("PDEG=0","PDEG=1"),pch=c(15,16),col=c("gray","black"))


plot(density(BETA[,3],adj=2),lwd=2,xlim=c(-.5,.5),main="",
    xlab=expression(beta[3]),ylab="density")
sd<-sqrt(  solve(t(X)%*%X/n)[3,3] )
x<-seq(-.7,.7,length=100)
lines(x,dnorm(x,0,sd),lwd=2,col="gray")
legend(-.5,6.5,legend=c("prior","posterior"),lwd=c(2,2),col=c("gray","black"),bty="n")
```

```{r}
beta.pm<-apply(BETA,2,mean)
beta.pm[1]+beta.pm[3]
quantile(BETA[,3],prob=c(.025,.0975))
quantile(BETA[,3],prob=c(0.025,0.975))
```

-->





## Ordinal-Trolley

```{r,eval=F}
data(Trolley)
```

```{r,eval=F}
d<- Trolley %>%
  group_by(response) %>% 
  count() %>%
  mutate(pr_k     = n / nrow(d)) %>% 
  ungroup() %>% 
  mutate(cum_pr_k = cumsum(pr_k)) %>% 
  filter(response < 7) 
```


```{r,eval=F}
inits <- list(`Intercept[1]` = -2,
              `Intercept[2]` = -1,
              `Intercept[3]` = 0,
              `Intercept[4]` = 1,
              `Intercept[5]` = 2,
              `Intercept[6]` = 2.5)
inits_list <- list(inits, inits)
b11.1 <- 
  brm(data = Trolley, family = cumulative,
      response ~ 1,
      prior(normal(0, 10), class = Intercept),
      iter = 2000, warmup = 1000, cores = 2, chains = 2,
      inits = inits_list,  # here we add our start values
      seed = 11)
```

> Error: Argument 'family' is invalid.

## Multinomial-VA

https://stackoverflow.com/questions/42114194/can-multinomial-models-be-estimated-using-generalized-linear-model

```{r}
data("VA",package = "MASS")
summary(VA)
```


### `nnet`

```{r}
nnet::multinom(cell ~ factor(treat), data=VA)%>%summary() # stime+age +Karn+diag.time
```

### `glm`

```{r}
VA.tab <- table(VA[, c('cell', 'treat')])
summary(glm(Freq ~ cell * treat, data=VA.tab, family=poisson))
```



## Multinomial-hsb2

```{r}
hsb2 <- read.table('https://stats.idre.ucla.edu/stat/data/hsb2.csv', header=TRUE, sep=",")
# Checking the output (dependent) variable
# table(hsb2$ses)
# By default the first category is the reference.
# To change it so ‘middle’ is the reference type
hsb2$ses2 = relevel(factor(hsb2$ses), ref = 2)
summary(hsb2)
```

### `brms`


```{r,results=F }
brm.mlt.hsb2 <- brm(ses2 ~ science + socst + female, data=hsb2, family=categorical("logit"),silent = T, refresh=0)
```

```{r}
summary(brm.mlt.hsb2)
brm.l <- fixef(brm.mlt.hsb2)[,1]
brm.l.sd <- fixef(brm.mlt.hsb2)[,2]
```

### `nnet`

```{r}
multi.hsb2  <-  nnet::multinom(ses2 ~ science + socst + female, hsb2)# 
summary(multi.hsb2)
```

### `mlogit`

```{r}
hsb2.mlogit <- dfidx(hsb2, shape='wide', choice = "ses2")#, varying = c(10,11, 2)
hsb2.mprobit <- mlogit(ses2 ~ 1|science + socst + female, data=hsb2.mlogit, probit = TRUE, reflevel = 2)# 
summary(hsb2.mprobit)
```


### Gibbs

```{r,echo=T}
 X<-sapply(hsb2[,c(10,11)], as.numeric)# ,2
# X <- scale(X)
 X<-cbind(1,X)# ,as.numeric(hsb2[,2])
y<-hsb2$ses2
y<-match(hsb2$ses2,levels(hsb2$ses2))
# ranks<-match(y,sort(unique(y)))
uranks<-sort(unique(y))
n<-dim(X)[1] ; p<-dim(X)[2]
iXX<-solve(t(X)%*%X)  ; V<-iXX*(n/(n+1)) ; cholV<-chol(V)
```

```{r,echo=F}
## setup
set.seed(1)
beta<-rep(0,p) 
z<-qnorm(rank(y,ties.method="random")/(n+1))
#z <- scale(rank(y,ties.method="random"))
g<-rep(NA,length(uranks)-1)
K<-length(uranks)
mu<-rep(0,K-1) ; sigma<-rep(1,K-1) 

## MCMC
M<-25000; burnin <- 5000; step <- 20; size <- (M-burnin)/step
BETA<-matrix(NA,size,p) ; Z<-matrix(NA,size,n) ; ac<-0
for(m in 1:M) 
{
  #update g 
  for(k in 1:(K-1)) 
  {
  a<-max(z[y==k])
  b<-min(z[y==k+1])
  u<-runif(1, pnorm( (a-mu[k])/sigma[k] ),
              pnorm( (b-mu[k])/sigma[k] ) )
  g[k]<- mu[k] + sigma[k]*qnorm(u)
  }
  #update beta
  E<- V%*%( t(X)%*%z )
  beta<- cholV%*%rnorm(p) + E
  #update z
  ez<-X%*%beta
  a<-c(-Inf,g)[ match( y-1, 0:K) ]
  b<-c(g,Inf)[y]  
  u<-runif(n, pnorm(a-ez),pnorm(b-ez) )
  z<- ez + qnorm(u)
  #help mixing
  c<-rnorm(1,0,n^(-1/3))  
  zp<-z+c ; gp<-g+c
  lhr<-  sum(dnorm(zp,ez,1,log=T) - dnorm(z,ez,1,log=T) ) + 
         sum(dnorm(gp,mu,sigma,log=T) - dnorm(g,mu,sigma,log=T) )
  if(log(runif(1))<lhr) { z<-zp ; g<-gp ; ac<-ac+1 }
if(m%%step==0 & m>burnin) {BETA[(m-burnin)/step,] <- beta; Z[(m-burnin)/step,]<- z} 
} 

```

```{r}
(g5 <- apply(BETA, 2, mean))
(g5.marg <-mean(dnorm(X %*% g5))*g5)
```

Note: this method cannot include "female"


### `glmer`

```{r}
glmer(ses2 ~ science + socst + (1|female), hsb2, family=binomial(link = "probit")) 
```

```{r}
glmer(ses2 ~ science + socst + (1|female), hsb2, family=binomial(link = "logit")) 
```


## Multinomial-fish

```{r}
data("Fishing", package = "mlogit")
head(Fishing)
# From Kenneth Train’s exercises using the mlogit package for R
Fish <- dfidx(Fishing, varying = 2:9, choice = "mode", idnames = c("chid", "alt"))
summary(Fish)
```

### `mlogit`

```{r}
Fish.mprobit <- mlogit(mode~price | income | catch, Fish, probit = TRUE, alt.subset=c('beach', 'boat','pier'))
summary(Fish.mprobit)
```

```{r,results=F,eval=T }
fish <- Fish%>% subset(mode=="TRUE",select=c(income,price,catch,idx))
income <- fish$income
price <- fish$price
catch <- fish$catch
alt<- fish$idx$alt
fishing <- data.frame(alt,income,price,catch)
```


### `nnet`

```{r}
multi.fish  <-  nnet::multinom(mode~price + income + catch , data=Fish)
summary(multi.fish)
```

```{r}
nnet::multinom(alt~price + income + catch , data=fishing)%>%summary()
```

### `brms`

Note: Doesn't work

```{r,results=F,eval=F }
brm.mlt.fish <- brm(alt~price + income + catch , data=fishing, family=categorical("logit"),silent = T, refresh=0)
```


```{r,eval=F}
summary(brm.mlt.fish)
brm.l.fish <- fixef(brm.mlt.fish)[,1]
brm.l.fish.sd <- fixef(brm.mlt.fish)[,2]
```



### Gibbs

```{r}
X<-cbind(income,price,catch)
head(X)
X<-cbind(1,scale(X))
name.rank<- names(sort(table(fish$idx$alt)))
y<-match(fish$idx$alt,name.rank)
# ranks<-match(y,sort(unique(y)))
uranks<-sort(unique(y))
n<-dim(X)[1] ; p<-dim(X)[2]
iXX<-solve(t(X)%*%X)  ; V<-iXX*(n/(n+1)) ; cholV<-chol(V)
```

Note: This method require standardize X

```{r,echo=F}
## setup
set.seed(1)
beta<-rep(0,p) 
z<-qnorm(rank(y,ties.method="random")/(n+1))
g<-rep(NA,length(uranks)-1)
K<-length(uranks)
mu<-rep(0,K-1) ; sigma<-rep(100,K-1) 

## MCMC
M<-25000; burnin <- 5000; step <- 20; size <- (M-burnin)/step
BETA<-matrix(NA,size,p) ; Z<-matrix(NA,size,n) ; ac<-0
for(m in 1:M) 
{
  #update g 
  for(k in 1:(K-1)) 
  {
  a<-max(z[y==k])
  b<-min(z[y==k+1])
  u<-runif(1, pnorm( (a-mu[k])/sigma[k] ),
              pnorm( (b-mu[k])/sigma[k] ) )
  g[k]<- mu[k] + sigma[k]*qnorm(u)
  }
  #update beta
  E<- V%*%( t(X)%*%z )
  beta<- cholV%*%rnorm(p) + E
  #update z
  ez<-X%*%beta
  a<-c(-Inf,g)[ match( y-1, 0:K) ]
  b<-c(g,Inf)[y]  
  u<-runif(n, pnorm(a-ez),pnorm(b-ez) )
  z<- ez + qnorm(u)
  #help mixing
  c<-rnorm(1,0,n^(-1/3))  
  zp<-z+c ; gp<-g+c
  lhr<-  sum(dnorm(zp,ez,1,log=T) - dnorm(z,ez,1,log=T) ) + 
         sum(dnorm(gp,mu,sigma,log=T) - dnorm(g,mu,sigma,log=T) )
  if(log(runif(1))<lhr) { z<-zp ; g<-gp ; ac<-ac+1 }
if(m%%step==0 & m>burnin) {BETA[(m-burnin)/step,] <- beta; Z[(m-burnin)/step,]<- z} 
} 

```

```{r}
apply(BETA, 2, mean)
```

```{r,echo=F,include=T, fig.width=9, fig.height=6, fig.align='center'}
lab<-c(expression(beta[0]),expression(beta[1]),expression(beta[2]),expression(beta[3]))
laby<-c("density","","")

par(mfrow=c(4,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
for(j in 1:p){
plot(BETA[,j],  ylab=lab[j],main="",pch=1,cex=0.1,
     xlab="Gibbs iteration (k)",col="cornflowerblue")  
plot(cumsum(BETA[,j])/(1:size),  ylab=lab[j],main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
#hist(gibbs.mat[-burnin,j],freq =F,xlab=paste("distribution of est. for beta",j-1),main="",col="cornflowerblue")
plot(density(BETA[,j],adj=2),lwd=2,main="",col="cornflowerblue",
    xlab=lab[j],ylab="density")
abline(v=quantile(BETA[,j],c(0.025,0.975)),col="gray",lwd=1)
}
```


### `glmer`

```{r}
glmer(mode~price + income + (1|catch) , data=Fishing, family=binomial(link = "probit")) 
```

```{r}
glmer(mode~price + income + (1|catch) , data=Fishing, family=binomial(link = "logit")) 
```


## Multinomial-marital

### 'bamlss'


```{r}
data("marital.nz", package = "VGAM")
```

```{r,eval=F}
library(bamlss)
d <- GAMart()
f <- num ~ s(x1) + s(x2) + s(x3) + te(lon, lat)
b <- bamlss(f, data = d)
```

> Error in value[[3L]](cond) : Package ‘coda’ version 0.19.4 cannot be unloaded: Error in unloadNamespace(package) : namespace ‘coda’ is imported by ‘bamlss’, ‘nimble’, ‘bridgesampling’, ‘brms’ so cannot be unloaded

```{r,eval=F}
## Model formula, each category may have different model terms.
f <- list(mstatus ~ s(age),
                  ~ s(age),
                  ~ s(age))
set.seed(123)
bamlss1 <- bamlss::bamlss(f, family = "multinomial", data = marital.nz,reference = "Married/Partnered",
                              optimizer = bfit, sampler = FALSE)
```




## Dirichlet-multinomial

```{r, eval=F}
test_that('Dirichlet-multinomial conjugacy setup', {
### Dirichlet-multinomial conjugacy

# as of v0.4, exact numerical results here have changed because
# ddirch now sometimes returns NaN rather than -Inf (when an
# alpha is proposed to be negative) -- this changes the RNG
# sequence because NaN values result in no runif() call in decide()

# single multinomial
    set.seed(0)
    n <- 100
    alpha <- c(10, 30, 15, 60, 1)
    K <- length(alpha)
    p <- c(.12, .24, .09, .54, .01)
    y <- rmulti(1, n, p)
    
    code <- function() {
        y[1:K] ~ dmulti(p[1:K], n);
        p[1:K] ~ ddirch(alpha[1:K]);
        for(i in 1:K) {
            alpha[i] ~ dgamma(.001, .001);
        }
    }
    
    inits <- list(p = rep(1/K, K), alpha = rep(K, K))
    data <- list(n = n, K = K, y = y)
    
    test_mcmc(model = code, name = 'Dirichlet-multinomial example', data= data, seed = 0, numItsC = 10000,
              inits = inits,
              results = list(mean = list(p = p)),
              resultsTolerance = list(mean = list(p = rep(.06, K))), avoidNestedTest = TRUE)
})
## bad mixing for alphas; probably explains why posterior estimates for alphas changed so much as of v 0.4
  
## with replication
test_that('Dirichlet-multinomial with replication setup', {
    set.seed(0)
    n <- 100
    m <- 20
    alpha <- c(10, 30, 15, 60, 1)
    K <- length(alpha)
    y <- p <- matrix(0, m, K)
    for(i in 1:m) {
        p[i, ] <- rdirch(1, alpha)
        y[i, ] <- rmulti(1, n, p[i, ])
    }
    
    code <- function() {
        for(i in 1:m) {
            y[i, 1:K] ~ dmulti(p[i, 1:K], n);
            p[i, 1:K] ~ ddirch(alpha[1:K]);
        }
        for(i in 1:K) {
            alpha[i] ~ dgamma(.001, .001);
        }
    }
    
    inits <- list(p = matrix(1/K, m, K), alpha = rep(1/K, K))
    data <- list(n = n, K = K, m = m, y = y)

    ## two tolerance failures are known, for p[39] and p[76]
    test_mcmc(model = code, name = 'Dirichlet-multinomial with replication', data= data,
              seed = 0, numItsC = 1000,
              inits = inits, numItsC_results = 100000,
              results = list(mean = list(p = p, alpha = alpha)),
              resultsTolerance = list(mean = list(p = matrix(.05, m, K),
                                                  alpha = c(5,10,10,20,.5))),
              knownFailures = list('MCMC match to known posterior: p mean 39' = 'KNOWN ISSUE: two samples outside resultsTolerance',
                                  'MCMC match to known posterior: p mean 76' = 'KNOWN ISSUE: two samples outside resultsTolerance'), avoidNestedTest = TRUE)
})
# note alphas mix poorly (and are highly correlated),
# presumably because of cross-level dependence between
# p's and alphas.  cross-level sampler would probably work well here,
# or, of course, integrating over the p's

test_that('Dirichlet-categorical conjugacy setup', {
### Dirichlet-categorical conjugacy

# single multinomial represented as categorical
    set.seed(0)
    n <- 100
    alpha <- c(10, 30, 15, 60, 1)
    K <- length(alpha)
    p <- c(.12, .24, .09, .54, .01)
    y <- rmulti(1, n, p)
    y <- rep(seq_along(y), times = y)
    
    code <- function() {
        for(i in 1:n)
          y[i] ~ dcat(p[1:K])
        p[1:K] ~ ddirch(alpha[1:K])
        for(i in 1:K) {
            alpha[i] ~ dgamma(.001, .001);
        }
    }
    
    inits <- list(p = rep(1/K, K), alpha = rep(K, K))
    data <- list(n = n, K = K, y = y)
    
    test_mcmc(model = code, name = 'Dirichlet-categorical example', data= data, seed = 0, numItsC = 10000,
              inits = inits,
              results = list(mean = list(p = p)),
              resultsTolerance = list(mean = list(p = rep(.06, K))), avoidNestedTest = TRUE)
})

## also note that MCMC results here should be identical to those from
## Dirichlet-multinomial case two tests up from this
```



## Summary


--------------------------------------------------------------------------------------------------------------------
|$y\in$       | link fn | cont'us X   |             |            |            |            |            |discrete X 
|-------------|---------|-------------|-------------|------------|------------|------------|------------|-----------
|             |         |fixed $\beta$|             |            |            |            |random $\beta$|           
|             |         |Manual       |`nimble`     |`brms`    |`glm`       |`mlogit`    |`glmer`     |
|$\{0,1\}$    | $\Phi$  |$\checkmark$ |$\checkmark$ |$\checkmark$|$\checkmark$|            |$\checkmark$|
|             |  Logit  |             |$\checkmark$ |$\checkmark$|$\checkmark$|$\checkmark$|$\checkmark$|
|             |         |             |             |            |`polr`      |            |            |   
|$\{1,2,..\}$ | $\Phi$  |$\checkmark$ |             |            |$\checkmark$|            |$\checkmark$|         
|             |  Logit  |             |             |$\checkmark$|$\checkmark$|            |$\checkmark$|         
|             |         |             |             |            |`nnet`      |            |            |
|\{A,B,..\}   | $\Phi$  |             |             |            |            |$\checkmark$|$\checkmark$|
|             |  Logit  |             |             |$\checkmark$|            |$\checkmark$|$\checkmark$|
|             |  log    |             |             |            |$\checkmark$|            |            |
|\{(A,B),C,.\}| $\Phi$  |             |             |            |            |$\checkmark$|            |
|             |  Logit  |             |             |            |            |$\checkmark$|            |

Nested

Mixed

Hierarchical

## Resources

[Charlie Geyer's Personal Home Page](http://users.stat.umn.edu/~geyer/)

[McElreath 2020. Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/)

[R package rethinking](https://github.com/rmcelreath/rethinking)

```{r,eval=F,echo=F,collapse=T}
library(devtools)
devtools::install_github("rmcelreath/rethinking")
install.packages("V8")
library(rethinking)
data(bangladesh)
#woman  ID number for each woman in sample
#district  Number for each district
#use.contraception  0/1 indicator of contraceptive use
#living.children  Number of living children
#age.centered  Centered age
#urban  0/1 indicator of urban context
```


https://cran.r-project.org/web/packages/bayesm/vignettes/bayesm_Overview_Vignette.html

https://bayesr.r-forge.r-project.org/index.html



