---
title: "Statistical Literature and Problems"
subtitle: "STAT 501"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document: default
header-includes:
 - \usepackage{amssymb}
 - \usepackage{amsmath}
---





```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
options(scipen=6)
options(digits=4)
if (!require(pacman)) {install.packages("pacman"); library(pacman)}
p_load(stargazer, pscl, mvtnorm, MASS, ggplot2,tidyverse,mlogit,BayesLogit,robcbi,kableExtra,truncnorm,lme4) # likelihoodAsy
```


#  {.tabset .tabset-fade .tabset-pills}

## (Albert &  Chib, 1993)

James H. Albert & Siddhartha Chib (1993) Bayesian Analysis of Binary and Polychotomous Response Data, Journal of the American Statistical Association, 88:422, 669-679, DOI: 10.1080/01621459.1993.10476321

### 1 Introduction

\(Y_1,..,Y_n\sim Bern(p_i)\). \(\beta_{k\times1}\) unknown vector of parameters. \(X_i^T=(X_{i1},..,X_{ik})\) known covariates.

\(p_i=H(\mathbf{x}_i^T\boldsymbol{\beta})\). \(H(\cdot)\) is a known CDF with linear structure \(\mathbf{x}_i^T\boldsymbol{\beta}\)

If \(H(\cdot)\) is standard normal CDF, it obtains the probit model,

if \(H(\cdot)\) is logistic CDF, it obtains the logit model.

\(\pi(\boldsymbol{\beta})\) is the prior density.

\(
\pi(\boldsymbol{\beta}|data) = \frac{\pi(\boldsymbol{\beta})\prod_{i=1}^{k}H (\mathbf{x}_i^T\boldsymbol{\beta})^{y_i}(1-H(\mathbf{x}_i^T\boldsymbol{\beta}))^{1-y_i}}{\int\pi(\boldsymbol{\beta})\prod_{i=1}^{k}H (\mathbf{x}_i^T\boldsymbol{\beta})^{y_i}(1-H(\mathbf{x}_i^T\boldsymbol{\beta}))^{1-y_i}d\boldsymbol{\beta}}
\)
is intractable.


For small number of parameter, a Bayesian approach summarized the posterior using numerical integration.

For large models (k large), posterior moments by Monte Carlo integration with a `multivariate Student's t importance function`.

This is a simulation-based approach for computing the exact posterior distribution of $\beta$.

The key idea is to introduce $N$ independent latent variables $Z_1,..Z_N\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1)$ into the problem. \(Y_i=\begin{cases}1&\text{if } Z_i>0\\0&\text{if }Z_i\le0\end{cases}\)

This approach is very simular to the data augmentation/Gibbs sampling framework used in censored regression models.

### 2. The Gibbs Sampler

\(
\theta_1^{(1)}\quad\text{from}\quad\pi(\theta_1|\{\theta_j^{(0)},j\neq1\})\\
\theta_2^{(1)}\quad\text{from}\quad\pi(\theta_2|\{\theta_1^{(1)},j>2\})\\
\vdots\\
\theta_p^{(1)}\quad\text{from}\quad\pi(\theta_p|\{\theta_j^{(1)},j<p\})\\
\)

One cycle is iterated $t$ times. For sufficiently large $t^*$, $\theta^{(t^*)}$ can be regarded as one simulated value from the posterior of $\theta$. Replicating this process $m$ times $\{\theta_{1j}^{(t^*)},\theta_{2j}^{(t^*)},..,\theta_{pj}^{(t^*)},j=1,..,m\}$

Two practical drawbacks to the replication approach:

1. The method is inefficient. the samples $\{\theta_j^{(t)}\}$, for $t<t^*$ are discarded.

2. After the initial run it may be necessary to repeat the simulation with a larger number of replication to get accurate density estimates.

- A "one-run" Gibbs sampling scheme is efficient in that few observations are discarded.

1. One should collect the values staring at the cycle $t$. The value of $t$ is samll (10-40) relative to the total number of values collected. <span style="color: red;">(burn-in???)</span>

2.  If one wishes to obtain an approximate independent sample of the $\theta$, the simulated values of $\theta$ could be collected at cycles $t,t+n_1,t+2n_1,..$, where $n_1$ is the spacing between cycles where $\theta^{(t)}$ and $\theta^{(t)}$ are believed to be approximately independent. *But it is not necessary to obtain an independent sample of $\theta$ to obtain, say, a marginal posterior density estimate of $\theta_k$* <span style="color: red;">???</span>

- One goal of this article is to obtain estimates of the densities of the individual parameters or their functional. 

One can estimate the density of this function using a *kernel density* estimate of the simulated values of \(g(\theta_k)\{\theta_k^{(i)},i=1,..,m\}\). A slightly preferable estimate of this marginal posterior density is given by \(\hat\pi(g(\theta_k))\approx\frac1m\sum_{i=1}^m\pi(g(\theta_k)\{\theta_r^{(i)},r\neq k\})\) <span style="color: red;">???</span>

In practice, we collect values of $\theta$ in batches of 100-200 until all the marginal density estimates for the components of $\theta$ stabilize.

- A second goal is estimation of posterior expectations.

To compute this standard error from this correlated simulation sample, we apply *the well-known batch means method*. We batch or section the sample into subsamples of equal size. When the lag one autocorrelation of the batch means is under .05, the simulation standard error is computed as the standard deviation of the batch means divided by the square root of the number of batches.
\(se=\frac{sd}{\sqrt{B}}\) <span style="color: red;"> subsamples of equal size???</span>

### 3 Data augmentation and Gibbs sampling

#### 3.1 Introduction

\[
\begin{align} 
\pi(\boldsymbol{\beta|y,Z}) = C\pi(\boldsymbol{\beta})\prod_{i=1}^{N}\phi (Z_{i};\mathbf{x}_i^T\boldsymbol{\beta},1); &\quad\mathbf{Z}=\mathbf{X}\boldsymbol{\beta+\varepsilon}; &\quad \boldsymbol{\varepsilon}\sim N_N(0,\mathbf{I})&&(1)\\
\\
\boldsymbol{\beta}|\mathbf{y,Z}\sim N_k(\boldsymbol{\hat\beta_Z},(\mathbf{X}^T\mathbf{X})^{-1}); &\quad\boldsymbol{\hat\beta_Z}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Z}&&& (2)\\
\\
Z_i|\boldsymbol{y,\beta}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1) &\quad\text{truncated at the left by 0} & \text{if } y_i=1 &&\\
 &\quad\text{truncated at the right by 0} & \text{if } y_i=0 &&(3)
\end{align}
\]

The starting value $\beta^{(0)}$ may be taken to be the maximum likelihood (ML) estimate, or least squares (LS) estimate $(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$

#### 3.2 The $t$ Link

\(\mathbf{Y}\sim Bern(p_i)\) have an underlying \(N(Z)\); \(\boldsymbol{\beta}|\mathbf{Z}\sim N_k()\); generalize mixtures of Normal distribution.

$H()=t$ *investigate the sensitivity* of the fitted probabilities to the choice of link function.

The most popular link function for binary data is the logit, which corresponds to a choice of a logistic distribution for $H$

Logistic quantiles are approximately a linear function of $t(8)$ quantiles. The logistic distribution has the same kurtosis as a $t$ distribution with 9 df.

\(Z_i\sim t_{\nu}(\mathbf{x}_i^T\boldsymbol{\beta},1)\) equivalently, \(Z_i|\lambda_i\sim N(\mathbf{x}_i^T\boldsymbol{\beta},\lambda_i^{-1})\); \(\lambda_i\sim Gamma(\frac{\nu}{2},\frac{2}{\nu})\propto\lambda_i^{\frac{\nu}{2}-1}\exp(-\frac{\nu\lambda_i}{2})\); Suppose \(\beta\sim Unif()\)

\[
\begin{align} 
\boldsymbol{\beta|y,Z,\lambda,\nu}\sim N_k(\boldsymbol{\hat\beta_{Z,\lambda}},(\mathbf{X'WX})^{-1}); &\quad\boldsymbol{\hat\beta_{Z,\lambda}}=(\mathbf{X'WX})^{-1}\mathbf{X'WZ},\ \mathbf{W}=\mathrm{diag}(\lambda_i) &&& (4)\\
\\
\boldsymbol{Z_i|y,\beta,\lambda,\nu}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},\lambda_i^{-1}) &\quad\text{truncated at the left by 0} & \text{if } y_i=1 &&\\
 &\quad\text{truncated at the right by 0} & \text{if } y_i=0 &&(5)\\
\\
\boldsymbol{\lambda_{1:N}|y,Z,\beta,\nu}\sim Gamma(\frac{\nu+1}{2},\frac{2}{\nu+(Z_i-\mathbf{x}_i^T\boldsymbol{\beta})^2})&\quad\text{independent with } \lambda_i&&&(6)\\
\boldsymbol{\nu|y,Z,\beta,\lambda}\propto\pi(\nu)\prod_{i=1}^N(c(\nu)\lambda_i^{\frac{\nu}{2}-1}e^{-\frac{\nu\lambda_i}{2}})&\quad\text{in a finite set}&&&(7)\\
\end{align}
\]

$\beta^{(0)}=$  least squares (LS) estimate under the probit model, set $\lambda_i=1,\forall i$

\[\hat\pi(\boldsymbol{\beta})\approx\frac1m\sum_{i=1}^m\pi(\boldsymbol{\beta|Z^{(i)},\lambda^{(i)}})\]

\(p_k=\Phi(\lambda_k^{\frac12}\mathbf{x}_k^T\boldsymbol{\beta})\) by a transformation of the conditional density of $\beta$

\[\hat\pi(p_k)=\frac1m\sum_{i=1}^m\frac{\phi(\Phi(p_k);\mu,\sigma^2)}{\phi(\Phi(p_k);0,1)}\]

\(\mu=\sqrt{\lambda_k^{(i)}}\mathbf{x}_k^T\boldsymbol{\hat\beta_{Z,\lambda}^{mu3(i)}}\) <span style="color: red;">????</span>

\(\sigma^2=\lambda_k^{(i)}\mathbf{x}_k^T(\mathbf{X'WX})^{-1}\mathbf{x}_k\)


#### 3.3 Hierarchical Analysis

(1) \(\mathbf{Z}\sim N(\boldsymbol{X\beta,I})\), (2) \(\boldsymbol{\beta}\sim N(\boldsymbol{A\beta^{(0)},\sigma^2I})\), (3) prior density \(\pi(\boldsymbol{\beta^{(0)},\sigma^2})\qquad\)  (8)

The hyperparameters \(\boldsymbol{\beta^{(0)}}\sim Unif()\), $\sigma^2$ given a noninformative prior <span style="color: red;">????</span>

 The posterior density of the regression vector $\beta$ compromises between least squares estimates from the "full" k-dimensional model and the "reduced" p-dimensional model where $\boldsymbol{\beta=A\beta^{(0)}}$

\[
\begin{align} 
\boldsymbol{\beta|Z,\sigma^2}&\sim N_k(\boldsymbol{\mu,V}) &&(9)\\ 
\boldsymbol{\mu}&=\boldsymbol{W_1\hat\theta_1+(I-W_1)A\hat\theta_2} \\
\boldsymbol{\hat\theta_1}&=(\mathbf{X'X})^{-1}\mathbf{X'Z} \\
\boldsymbol{\hat\theta_1}&=(\mathbf{X'X})^{-1}\mathbf{X'Z} \\
\boldsymbol{V}&=\boldsymbol{((I-W_1)A)[A^TX^T(I+XX^T\sigma^2)^{-1}XA]^{-1}((I-W_1)A)^T+[X^TX+I/\sigma^2]^{-1}} \\
\boldsymbol{\sigma^2|Z}&\propto c(\mathbf{Z)\frac{|(I+XX^T\sigma^2)^{-1}|^{\frac12}}{|A^TX^T(I+XX^T\sigma^2)^{-1}XA|^{\frac12}}}\exp{\left\{\frac12Q(\boldsymbol{Z,XA\hat\theta_2,I+XX^T\sigma^2})\right\}}\pi(\sigma^2) &&(10)\\ 
\end{align}
\]

where \(Q(\boldsymbol{Z,\mu,\Sigma})=\boldsymbol{(Z-\mu)^T\Sigma^{-1}(Z-\mu)}\) and \(c(\mathbf{Z)}\) is a proportionality constant.

one starts with initial guesses at $\boldsymbol{\beta}$ and $\sigma^2$, simulates the $Z_i$ from (3), and then simulates $\boldsymbol{\beta}$ and $\sigma^2$ from the distributions (9) and (10)

### 4 Generalizations to a Multinomial response

#### 4.1 Ordered Categories

\(Y_1,..,Y_N\) are observed. \(Y_i\) takes one of $J$ ordered categories. \(p_{ij}=P[Y_i=j]\), we define the cumulative probabilities \(\eta_{ij}=\sum_{k=1}^jp_{ij},j=1,..,J-1\) <span style="color: red;">k????</span>

One popular regression model is given by \(\eta_{ij}=\Phi(\gamma_i-\mathbf{x}_i^T\boldsymbol{\beta}),i=1,..,N; j=1,..,J-1\)

A latent continuous variable $Z_i\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1)$. \(Y_i=j\text{ if } \gamma_{j-1}<Z_i\le\gamma_{j}, \gamma_{0}=-\infty,\gamma_{J}=\infty\)

 The posterior distribution of $\beta$ conditional on $y$ and $Z$ is given by the multivariate normal form (2)

\[
\begin{align} 
\pi(\boldsymbol{\beta,\gamma|y})&= C\pi(\boldsymbol{\beta})\prod_{i=1}^{N}\sum_{j=1}^{J}\mathbf{1}_{(y_i=j)}[\Phi (\gamma_{j}-\mathbf{x}_i^T\boldsymbol{\beta})-\Phi (\gamma_{j-1}-\mathbf{x}_i^T\boldsymbol{\beta})]\\
\pi(\boldsymbol{\beta,\gamma,Z|y})&= C\prod_{i=1}^{N}\left[\frac{1}{\sqrt{2\pi}}\exp(-\frac12(Z_i-\mathbf{x}_i^T\boldsymbol{\beta})^2)(\sum_{j=1}^{J}\mathbf{1}_{(Y_i=j)}\mathbf{1}_{(\gamma_{j-1}<Z_i\le\gamma_{j})})\right]&&(11)\\
Z_i|\boldsymbol{\beta,\gamma},y_i&\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1) \quad\text{truncated at the left(right) by } \gamma_{j-1}(\gamma_{j}) &&(12)\\
\gamma_j|\boldsymbol{Z,y,\beta,},\{\gamma_k,k\neq j\}&\propto  \prod_{i=1}^{N}\left[\mathbf{1}_{(Y_i=j)}\mathbf{1}_{(\gamma_{j-1}<Z_i\le\gamma_{j})})+\mathbf{1}_{(Y_i=j+1)}\mathbf{1}_{(\gamma_{j}<Z_i\le\gamma_{j+1})})\right]&&(13)\\
\end{align}
\]

(13) can be seen to be uniform on the interval \([\max \{\max \{Z_i: Y_i = j \}, \gamma_{j-1} \}, \min \{\min \{Z_i: Y_i = j + 1\}, \gamma_{j+1} \}]\). 

To implement the Gibbs sampler here, start with ($\beta,\gamma$) set equal to the MLE and simulate from the distributions (13), (12), and (1), in that order.

#### 4.2 Unordered Categories With a Latent Multinormal Distribution

We introduce independent latent variables \(Z_i=(Z_{i1},..,Z_{iJ})(J>2)\) and define \(Z_{ij}=\mathbf{x}_{ij}^T\boldsymbol{\beta}+\varepsilon_{ij},\ i=1,..,N;j=1,..,J\), whrre \(\varepsilon_i=(\varepsilon_{i1},..,\varepsilon_{iJ})^T\sim N_J(\mathbf{0,\Sigma_{J\times J}})\)

\(\mathbf{\Sigma}\) is parameterized in terms of a parameter vector $\theta$ of dimension not exceeding \(\frac12J(J-1)\).

On unit $i$ we observe one of $J$ possible outcomes with respective probabilities $p_{i1},..,p_{ij}$. Category $j$ is observed if $Z_{ij}>Z_{ik}$ for all $k\neq j$.

The multinomial logit model can be derived in this setup if and only if the errors \(\varepsilon_{ij}\) are a random sample from a Type I extreme value distribution. The multinomial probabilities are given by \(p_{ij}=P[\mathbf{x}_{ij}^T\boldsymbol{\beta}+\varepsilon_{ij}>\mathbf{x}_{ik}^T\boldsymbol{\beta}+\varepsilon_{ik}, \forall k\neq j]\)

\(\mathbf{Z=X}\boldsymbol{\beta+\varepsilon}\) where \(\varepsilon=(\varepsilon_{1}^T,..,\varepsilon_{N}^T)^T\sim N_{NJ}(\mathbf{0,I_N\otimes\Sigma})\)

\[
\begin{align} 
\boldsymbol{\beta|Z_{1:N},Y,\theta}&\sim N_k(\boldsymbol{\hat\beta_{Z}},(\mathbf{X'\Omega^{-1} X})^{-1}); \quad\boldsymbol{\hat\beta_{Z}}=(\mathbf{X'\Omega^{-1} X})^{-1}\mathbf{X'\Omega^{-1} Z}\\
\boldsymbol{Z_i|y,\beta,\theta,\{Z_i\}}&\sim N(\mathbf{x}_i^T\boldsymbol{\beta},\Sigma) \quad\text{such that
 the }y_i^{th}\text{ component of }Z_i\text{ is the maximum}  &&\\
\boldsymbol{\theta|Z_{1:N},Y,\beta}&\propto \boldsymbol{\pi(\theta)|\Omega|^{-\frac12}} \exp\left[-\frac12\boldsymbol{(Z-X\beta)^T\Omega^{-1}(Z-X\beta)}\right]&&(14)\\
\end{align}
\]

\(\mathbf{\Omega}^{-1}\) is a block diagonal matrix with \(\mathbf{\Sigma}^{-1}\) as the typical block. <span style="color: red;"> ???</span>


### 5.1 Finney Data

\[\Phi^{-1}(p_i)=\beta_0+\beta_1x_{1i}+\beta_2x_{2i},\ i=1,..,39\]

where $x_{1i}$ is the volume of air inspired, $x_{2i}$ is the rate of air inspired, and the binary outcome observed is the occurrence or nonoccurrence on a transient vasorestriction on the skin of the digits. $\beta\sim Unif$
 prior is placed on the regression parameter

```{r,echo=T,collapse=T}
data(Finney)
Vol <- Finney$Vol; Rate <- Finney$Rate; Resp <- Finney$Resp
lVol <-log(Vol); lRate <- log(Rate)
# plotFdat <- Finney$plotFdat
# plotFdat(Rs=Resp,lV=lVol,lR=lRate,zc,zr,rob=F,cont=F)
plot(Vol,Rate,type="n",xlab="Vol",ylab="Rate")
points(Vol[Resp==0],Rate[Resp==0],pch=5, cex=1.2)
points(Vol[Resp==1],Rate[Resp==1],pch=16,cex=1.2)

finney <- data.frame(Finney[1:3])
finney[,3] <- as.factor(finney[,3])
table(finney[3])
n <- nrow(finney)
```


The starting value $\beta^{(0)}$ is the least square estimate $(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$

```{r, echo=T,collapse=T}
# X <- cbind(Vol, Rate)
# (beta <- solve( t(X) %*% X ) %*% t(X) %*% Resp) #start from LS estimate
X <- cbind(1, Vol, Rate)
(beta <- as.vector(solve( t(X) %*% X ) %*% t(X) %*% Resp))
p <- dim(X)[2]
```

\(\mu=\mathbf{x}_i^T\boldsymbol{\beta}\)

```{r, echo=T,collapse=T}
(mu <- as.vector(X %*% beta)) # Initial mu of Z
```

\(Z_i|\boldsymbol{y,\beta}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1)\) truncated by 0 at the \(\begin{cases}\text{left if}&y_i=1\\\text{right if}&y_i=0\end{cases}\)



```{r,eval=T, echo=T,collapse=T}
# only works for the second probit model
ez<-as.vector(X%*%beta)
set.seed(123); u<-runif(n,0,1)
(z<- ez + qnorm(ifelse(Resp==1,u+(1-u)*pnorm(0,ez,1),u*pnorm(0,ez,1))))
```

```{r, echo=T,collapse=T}
# only works for the first probit model
tr_norm<-function(n,mu,v,truc_side='left'){ # Truncated Normal function
u<-runif(n,min=0,max=1)
  if(truc_side=='left'){
 tr_norm_value<-u-u*pnorm(0,mean=mu,sd=sqrt(v))+pnorm(0,mean=mu,sd=sqrt(v))}
 if(truc_side=='right'){
   tr_norm_value<-u*pnorm(0,mean=mu,sd=sqrt(v))}
  qnorm(tr_norm_value,mu,sqrt(v))
}

set.seed(123)
(z<- ifelse(Resp==1, # Initial Z
tr_norm(1,mu,1),
tr_norm(1,mu,1,'right')
))
```

*other attempts of sampling from truncated normal*

```{r,eval=F, echo=T,collapse=T}

#for(j in 1:n){  
#ifelse(Resp[j]==1,
#z[j] <- rtruncnorm(1,a=0,b=Inf,mean=mu[j],sd=1),
#z[j] <- rtruncnorm(1,a=Inf,b=0,mean=mu[j],sd=1)
#)
#}

# anohter attempt of truncated
#z<- abs(rnorm(n,mean=mu,sd=1)),
#z<- -abs(rnorm(n,mean=mu,sd=1))
```

\(\boldsymbol{\hat\beta_Z}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Z}\)

\(\Omega=(\mathbf{X}^T\mathbf{X})^{-1})\)

```{r, echo=T,collapse=T}
solve(t(X)%*%X)%*%t(X)%*%z # first mu for Beta
solve(t(X)%*%X)            # first omega for Beta

iXX<-solve(t(X)%*%X)  ; V<-iXX*(n/(n+1)) ; cholV<-chol(V) # from Hoff (2009, Ch.12)

```

#### The first Gibbs sampling

```{r}
M <- 25000; set.seed(123)
Gibbs <- matrix(NA,ncol=p,nrow=M); ZZ<-matrix(NA,M,n)      #For full Sample
gibbs <- matrix(NA,ncol=p,nrow=1000); Z<-matrix(NA,1000,n) #For Sub Sample
for(m in 1:M){  #Gibbs Sampler
  
# beta <- mvrnorm(1, solve(t(X)%*%X)%*%(t(X)%*%z), solve(t(X)%*%X)) # without n/(n+1)
beta <- mvrnorm(1, V%*%(t(X)%*%z), V) # draw beta from Normal

z<- ifelse(Resp==1, # draw Z from truncated Normal
tr_norm(1 ,t(X %*% beta), 1),
tr_norm(1 ,t(X %*% beta), 1 ,'right')
)

#ez<-(X%*%beta)
#u<-runif(n,0,1)
#z<- ez + qnorm(ifelse(Resp==1,u+(1-u)*pnorm(0,ez,1),u*pnorm(0,ez,1)))

Gibbs[m,] <- t(beta); ZZ[m,]<- z                                        # full samples
if(m%%(M/1000)==0) { gibbs[m/(M/1000),] <- t(beta); Z[m/(M/1000),]<- z}   # subsamples of equal size
}
```

\(\boldsymbol{\hat\beta}\approx\frac1m\sum_{i=1}^m(\boldsymbol{\beta|Z^{(i)}})\)

```{r,echo=T,collapse=T}
full.mean <- colMeans(Gibbs[,])
burnin <- 24000:25000
tail.mean <- colMeans(Gibbs[burnin,])
sub.mean <- colMeans(gibbs)
par<- cbind(full.mean,tail.mean,sub.mean)
rownames(par) <- c("Beta0","Beta1","Beta2")
pander::pander(round((par),4))
```

```{r,eval=F,echo=T}
full.quant <- apply(Gibbs[burnin,],2,quantile,probs=c(0.025,0.5,0.975)) 
tail.quant <- apply(Gibbs[burnin,],2,quantile,probs=c(0.025,0.5,0.975)) 
sub.quant <- apply(gibbs,2,quantile,probs=c(0.025,0.5,0.975)) 
par<- rbind(full.quant,tail.quant,sub.quant)
colnames(par) <- c("Beta0","Beta1","Beta2")
pander::pander(round(t(par),4))
```

```{r,echo=T,include=T,fig.show='hold', fig.width=9, fig.height=6, fig.align='center'}
par(mar=c(5,3,1,1),mgp=c(1.75,.75,0)); par(mfrow=c(3,3))
lab<-c(expression(beta[0]),expression(beta[1]),expression(beta[2]))
for(j in 1:3) {
acf(Gibbs[,j],ci.col="dodgerblue4",xlab=lab[j])}  
for(j in 1:3) {
acf(Gibbs[burnin,j],ci.col="dodgerblue4",xlab=lab[j])} 
for(j in 1:3) {
acf(gibbs[,j],xlab=lab[j],ci.col="dodgerblue4")}
mtext("autocorrelation functions with Gibbs1.full(above),Gibbs1.tail(middle) ,Gibbs1.sub(below)",side=1,line=-1,outer=T)
```

Choose sub-sampling

```{r,echo=T,include=T, message=F, warning=F, fig.width=9, fig.height=6, fig.align='center'}
par(mfrow=c(3,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
for(j in 1:p){
plot(gibbs[,j],  ylab=lab[j],main="",pch=1,cex=0.1,
     xlab="Gibbs iteration (k)",col="cornflowerblue")  
plot(cumsum(gibbs[,j])/(1:1000),  ylab=lab[j],main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
#hist(gibbs.mat[-burnin,j],freq =F,xlab=paste("distribution of est. for beta",j-1),
#     main="",col="cornflowerblue")
plot(density(gibbs[,j],adj=2),lwd=2,main="",col="cornflowerblue",
    xlab=lab[j],ylab="density")
abline(v=quantile(gibbs[,j],c(0.025,0.975)),col="gray",lwd=1)
}
```




\(p_i=\Phi(\mathbf{x}_i^T\boldsymbol{\beta})\)

```{r,echo=T,collapse=T}
(prob <- pnorm(t(X %*% beta),0,1))
```

```{r,echo=F,eval=F, message=F, warning=F, fig.width=6, fig.height=3, fig.align='center'}
par(mfrow=c(2,3),mar=c(3,3,.5,.5),mgp=c(1.70,.70,0))
plot(gibbs.mat[-burnin,(n+1)],  ylab=expression(beta^{(k)}),main="",pch=20,cex=0.3,
     xlab="Gibbs iteration (k)",col="cornflowerblue")
plot(cumsum(gibbs.mat[-burnin,(n+1)])/(1:(S/2)),  ylab=expression(E(beta)),main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
hist(gibbs.mat[-burnin,(n+1)],  xlab=expression(paste("distribution of est. for ",beta)),
     main="",col="cornflowerblue")
abline(v=quantile(gibbs.mat[-burnin,(n+1)],c(0.025,0.975)),col="red",lwd=1)

u <- seq(0.01,4,length=1000)
d <- dgamma(u,shape=alpha,rate=pumps.par["50%","Beta"])
plot(u,d,type="l",xlab=expression(lambda),ylab="density",col="cornflowerblue",lwd=2,pch=20,cex=0.7)
abline(v=pumps.quant["50%",1:10],col="grey",lwd=1)

par(las=1,mar=c(3,5,.5,.5))
plot(c(0,4),c(1,10),type="n",xlab=expression(lambda),ylab=" ",axes=FALSE)
axis(side=1)
axis(side=2,at=1:10,labels=paste("Pump #",10:1,sep=""))
segments(pumps.quant["2.5%",1:10],10:1,pumps.quant["97.5%",1:10],10:1,col="cornflowerblue",lwd=2,pch=20,cex=0.7)
segments(pumps.quant["50%",1:10],(10:1)-0.25,pumps.quant["50%",1:10],(10:1)+0.25,col="cornflowerblue",lwd=2,pch=20,cex=0.7)
```

#### The second Gibbs sampling, refer to (Hoff, 2009, Ch.12)

```{r}
iXX<-solve(t(X)%*%X)  ; V<-iXX*(n/(n+1)) ; cholV<-chol(V)
```

```{r,echo=T,include=T}
#### probit regression
## setup
set.seed(123)
beta<-rep(0,p) 
z<-qnorm(rank(Resp,ties.method="random")/(n+1)) # random initial Z
BETA<-matrix(NA,1000,p) ; Z<-matrix(NA,1000,n) ; ac<-0
mu<-0 ; sigma<-100  ### Why????

## MCMC
S<-25000
for(s in 1:S) 
{
  #update beta
  E<- V%*%( t(X)%*%z )
  beta<- cholV%*%rnorm(p) + E  # ????????

  #update z
  ez<-X%*%beta
  u<-runif(n,0,1)
  z<- ez + qnorm(ifelse(Resp==1,u+(1-u)*pnorm(0,ez,1),u*pnorm(0,ez,1)))
    
  #help mixing
c<-rnorm(1,0,n^(-1/3))  #  sd responding the sample size ????
zp<-z+c #;g <- 0 ; gp<-g+c
lhr<-  sum(dnorm(zp,ez,1,log=T) - dnorm(z,ez,1,log=T) ) #+ sum(dnorm(c,mu,sigma,log=T) - dnorm(0,mu,sigma,log=T) )
if(log(runif(1))<lhr) { z<-zp ; ac<-ac+1 }              # ; g<-gp

  if(s%%(S/1000)==0) 
  { 
    # cat(s/S,ac/s,"\n")
    BETA[s/(S/1000),]<-  beta
    Z[s/(S/1000),]<- z
  }
} 
```

```{r,echo=T,include=T, message=F, warning=F, fig.width=9, fig.height=3, fig.align='center'}
(beta.pm<-apply(BETA,2,mean))
par(mfrow=c(1,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
laby<-c("density","","")
for(j in 1:p){
plot(density(BETA[,j],adj=2),lwd=2,main="",#xlim=c(-10,5),
    xlab=lab[j],ylab=laby[j],col="cornflowerblue")
sd<-sqrt(  solve(t(X)%*%X/n)[j,j] )
x<-seq(min(BETA[,j]),max(BETA[,j]),length=100)
lines(x,dnorm(x,E[j],sd),lwd=2,col="gray")
if(j==3) {legend(2.1,1,legend=c("prior","posterior"),lwd=c(2,2),col=c("gray","cornflowerblue"),bty="n")}
}
```

```{r,echo=T,collapse=T}
source("rlreg.R")
rfit<-treg(Resp,X)
```

```{r,echo=T,include=T, message=F, warning=F, fig.width=9, fig.height=3, fig.align='center'}
par(mfrow=c(1,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
ymx<-c(0.3,0.75,1.25)
for(j in 1:3) {
plot(density(rfit$BETA[,j],adj=2),lwd=2,main="",
 xlab=lab[j],col="gray",ylim=c(0,ymx[j]),ylab=laby[j])
lines(density(Gibbs[burnin,j],adj=2),col="cornflowerblue",lwd=2)
lines(density(gibbs[,j],adj=2),col="cornflowerblue",lwd=2,lty=2)
lines(density(BETA[,j],adj=2),col="cyan",lwd=2)
if(j==3) {
 legend(1.75,1.25,legend=c("likelihood","Gibbs1.tail","Gibbs1.sub","Gibbs2"), lty=c(1,1,2,1),
       lwd=c(2,2,2,2),col=c("gray","cornflowerblue","cornflowerblue","cyan"))
          } 
               }
```






#### Compare with the results by GLM

Using glm function, the fitted logit model gives the largest value of coefficients and residual scale.

Linear regression treat response as a continuous value <span style="color: red;">????</span> and gives the smallest value of coefficients.

What is the benifit of log transform?

```{r,eval=T,echo=T,collapse=T}
fit.probit.lm <- lm(Resp~Vol+Rate)
fit.probit.glm <- glm(Resp~Vol+Rate,family=binomial(link="probit"))
fit.logit.glm <- glm(Resp~Vol+Rate,family=binomial(link="logit"))
comp <- fits.compare(fit.probit.lm, fit.probit.glm, fit.logit.glm)
comp
plot(comp)
```

#### Compare with the results after log transform

```{r,eval=T,echo=T,collapse=T}
fit.probit.lm.l <- lm(Resp~lVol+lRate)
fit.probit.glm.l <- glm(Resp~lVol+lRate,family=binomial(link="probit"))
fit.logit.glm.l <- glm(Resp~lVol+lRate,family=binomial(link="logit"))
comp <- fits.compare(fit.probit.lm.l, fit.probit.glm.l, fit.logit.glm.l)
comp
# plot(comp)
```

```{r,eval=F,echo=F}
summary(z.logit)
correl(z.logit)
covar(z.logit)
Rank(z.logit)
rscale(z.logit)
# kable(t(data_frame(weights(z.logit))))
```

```{r,eval=F,echo=F}

# From https://data.princeton.edu/
fit.mle.glmer <- glmer(Resp~(1|Vol)+(1|Rate), family=binomial) #, nAGQ = 12, REML=FALSE
summary(fit.mle.glmer)
```

```{r,eval=F,echo=F}
# From Kenneth Train’s exercises using the mlogit package for R

# data("Mode", package="mlogit")
# Mo <- mlogit.data(Mode, choice='choice', shape='wide', varying=c(2:9))
# p1 <- mlogit(choice~cost+time, Mo, seed = 20, R = 100, probit = TRUE)

finney.mlogit <- mlogit.data(finney, choice='Resp', shape='wide') #, varying=c(1:2)
fit.probit.mlogit <- mlogit(Resp~1|Vol+Rate, probit=F,finney.mlogit) #|0 ,reflevel=5,seed=20,R=100
```

```{r,eval=F,echo=F}
# From lecture5_sample;# response must have 3 or more levels
# fit.probit.plr <- polr(Resp~Vol+Rate, method = "probit",finney) 
```







```{r,echo=F,out.width='50%'}
#z.cub <- glm(Resp~lVol+lRate,family=binomial,method="cubinf", ufact=3.2)
# summary(z.cub)
# plot(z.cub, smooth=TRUE, ask=TRUE)
```



```{r,eval=F,echo=F,out.width='50%'}
## Adds a QQ-line for the values in x in the current plot.
x <- residuals(z.probit, type="deviance")
qqnorm(x, ylab = "Deviance Residuals")
QQline(x, lty = 2)
# Predictions provided by a model fit when method is "cubinf".
rVol <- runif(20,0.4,3.7); rRate <- runif(20,0.3,3.75)
newdat <- data.frame(lVol=log(rVol),lRate=log(rRate))
predict(z.probit, newdat, type="response")

```


### 5.2 Election Data


### 5.3 A Trivariate Probit Example



## (Polson etal, 2013)

Nicholas G. Polson, James G. Scott & Jesse Windle (2013) Bayesian Inference for Logistic Models Using Pólya–Gamma Latent Variables, Journal of the American Statistical Association, 108:504, 1339-1349, [DOI: 10.1080/01621459.2013.829001](https://www-tandfonline-com/doi/full/10.1080/01621459.2013.829001)

### Introduction

[Home page of James Scott](https://jgscott.github.io/)

[R package BayesLogit and Thesis](https://github.com/jwindle/BayesLogit)

*Definition 1*. A random variable X has a Pólya–Gamma distribution with parameters b > 0 and , denoted as $X\sim PG(b, c),$ if

where the $g_k ∼ Ga(b, 1)$ are independent gamma random variables, and where  indicates equality in distribution.


### 2 The Polya-Gamma distribution

### 3 A data-augmentation strategy

A data-augmentation scheme for binomial likelihoods

### 4 Simulating Polya-Gamma random Variables

a method for simulating from the Pólya–Gamma distribution, which we have implemented as a stand-alone sampler in the BayesLogit R package.

### 5 Experiment

presents the results of an extensive benchmarking study comparing the efficiency of our method to other data-augmentation schemes. 

### 6 Discussion

concludes with a discussion of some open issues related to our proposal.



