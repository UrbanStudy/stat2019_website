---
title: "Statistical Literature and Problems"
subtitle: "STAT 501"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document: default
header-includes:
 - \usepackage{amssymb}
 - \usepackage{amsmath}
---





```{r setup, include=F}
knitr::opts_chunk$set(message=FALSE, warning=F, echo=TRUE)
options(width = 2000)
options(repos="https://cran.rstudio.com")
options(scipen=6)
options(digits=4)
if (!require(pacman)) {install.packages("pacman"); library(pacman)}
p_load(stargazer, pscl, mvtnorm, MASS, ggplot2,tidyverse,mlogit,robcbi,kableExtra,truncnorm,lme4) # likelihoodAsy
```


#  {.tabset .tabset-fade .tabset-pills}

## (Albert &  Chib, 1993)

James H. Albert & Siddhartha Chib (1993) Bayesian Analysis of Binary and Polychotomous Response Data, Journal of the American Statistical Association, 88:422, 669-679, DOI: 10.1080/01621459.1993.10476321

### 1 Introduction

\(p_i=H(\mathbf{x}_i^T\boldsymbol{\beta})\)


### 3 Data augmentation and Gibbs sampling

#### 3.1 Introduction

\[
\begin{align} 
\pi(\boldsymbol{\beta|y,Z}) = C\pi(\boldsymbol{\beta})\prod_{i=1}^{k}\phi (Z_{i};\mathbf{x}_i^T\boldsymbol{\beta},1); &\quad\mathbf{Z}=\mathbf{X}\boldsymbol{\beta+\varepsilon}; &\quad \boldsymbol{\varepsilon}\sim N_N(0,\mathbf{I})&&(1)\\
\\
\boldsymbol{\beta}|\mathbf{y,Z}\sim N_k(\boldsymbol{\hat\beta_Z},(\mathbf{X}^T\mathbf{X})^{-1}); &\quad\boldsymbol{\hat\beta_Z}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Z}&&& (2)\\
\\
Z_i|\boldsymbol{y,\beta}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1) &\quad\text{truncated at the left by 0} & \text{if } y_i=1 &&\\
 &\quad\text{truncated at the right by 0} & \text{if } y_i=0 &&(3)
\end{align}
\]

The starting value $\beta^{(0)}$ may be taken to be the maximum likelihood (ML) estimate, or least squares (LS) estimate $(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$

#### 3.2 The $t$ Link

\(\mathbf{Y}\sim Bern(p_i)\) have an underlying \(N(Z)\); \(\boldsymbol{\beta}|\mathbf{Z}\sim N_k()\); generalize mixtures of Normal distribution.

$H()=t$ investigate the sensitivity of the fitted probabilities to the choice of link function.

The most popular link function for binary data is the logit, which corresponds to a choice of a logistic distribution for $H$

Logistic quantiles are approximately a linear function of $t(8)$ quantiles. The logistic distribution has the same kurtosis as a $t$ distribution with 9 df.

\(Z_i\sim t_{\nu}(\mathbf{x}_i^T\boldsymbol{\beta},1)\) equivalently, \(Z_i|\lambda_i\sim N(\mathbf{x}_i^T\boldsymbol{\beta},\lambda_i^{-1})\); \(\lambda_i\sim Gamma(\frac{\nu}{2},\frac{2}{\nu})\propto\lambda_i^{\frac{\nu}{2}-1}\exp(-\frac{\nu\lambda_i}{2})\); Suppose \(\beta\sim Unif()\)

\[
\begin{align} 
\boldsymbol{\beta|y,Z,\lambda,\nu}\sim N_k(\boldsymbol{\hat\beta_{Z,\lambda}},(\mathbf{X'WX})^{-1}); &\quad\boldsymbol{\hat\beta_{Z,\lambda}}=(\mathbf{X'WX})^{-1}\mathbf{X'WZ},\ \mathbf{W}=\mathrm{diag}(\lambda_i) &&& (4)\\
\\
\boldsymbol{Z_i|y,\beta,\lambda,\nu}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},\lambda_i^{-1}) &\quad\text{truncated at the left by 0} & \text{if } y_i=1 &&\\
 &\quad\text{truncated at the right by 0} & \text{if } y_i=0 &&(5)\\
\\
\boldsymbol{\lambda_{1:N}|y,Z,\beta,\nu}\sim Gamma(\frac{\nu+1}{2},\frac{2}{\nu+(Z_i-\mathbf{x}_i^T\boldsymbol{\beta})^2})&\quad\text{independent with } \lambda_i&&&(6)\\
\boldsymbol{\nu|y,Z,\beta,\lambda}\propto\pi(\nu)\prod_{i=1}^N(c(\nu)\lambda_i^{\frac{\nu}{2}-1}e^{-\frac{\nu\lambda_i}{2}})&\quad\text{in a finite set}&&&(7)\\
\end{align}
\]

$\beta^{(0)}=$  least squares (LS) estimate under the probit model, set $\lambda_i=1,\forall i$

\[\hat\pi(\boldsymbol{\beta})\approx\frac1m\sum_{i=1}^m\pi(\boldsymbol{\beta|Z^{(i)},\lambda^{(i)}})\]

\(p_k=\Phi(\lambda_k^{\frac12}\mathbf{x}_k^T\boldsymbol{\beta})\) by a transformation of the conditional density of $\beta$

\[\hat\pi(p_k)=\frac1m\sum_{i=1}^m\frac{\phi(\Phi(p_k);\mu,\sigma^2)}{\phi(\Phi(p_k);0,1)}\]

\(\mu=\sqrt{\lambda_k^{(i)}}\mathbf{x}_k^T\boldsymbol{\hat\beta_{Z,\lambda}^{mu3(i)}}\) <span style="color: red;">????</span>

\(\sigma^2=\lambda_k^{(i)}\mathbf{x}_k^T(\mathbf{X'WX})^{-1}\mathbf{x}_k\)


#### 3.3 Hierarchical Analysis

(1) \(\mathbf{Z}\sim N(\boldsymbol{X\beta,I})\), (2) \(\boldsymbol{\beta}\sim N(\boldsymbol{A\beta^{(0)},\sigma^2I})\), (3) prior density \(\pi(\boldsymbol{\beta^{(0)},\sigma^2})\qquad\)  (8)

The hyperparameters \(\boldsymbol{\beta^{(0)}}\sim Unif()\), $\sigma^2$ given a noninformative prior <span style="color: red;">????</span>

 The posterior density of the regression vector $\beta$ compromises between least squares estimates from the "full" k-dimensional model and the "reduced" p-dimensional model where $\boldsymbol{\beta=A\beta^{(0)}}$

\[
\begin{align} 
\boldsymbol{\beta|Z,\sigma^2}&\sim N_k(\boldsymbol{\mu,V}) &&(9)\\ 
\boldsymbol{\mu}&=\boldsymbol{W_1\hat\theta_1+(I-W_1)A\hat\theta_2} \\
\boldsymbol{\hat\theta_1}&=(\mathbf{X'X})^{-1}\mathbf{X'Z} \\
\boldsymbol{\hat\theta_1}&=(\mathbf{X'X})^{-1}\mathbf{X'Z} \\
\boldsymbol{V}&=\boldsymbol{((I-W_1)A)[A^TX^T(I+XX^T\sigma^2)^{-1}XA]^{-1}((I-W_1)A)^T+[X^TX+I/\sigma^2]^{-1}} \\
\boldsymbol{\sigma^2|Z}&\propto c(\mathbf{Z)\frac{|(I+XX^T\sigma^2)^{-1}|^{\frac12}}{|A^TX^T(I+XX^T\sigma^2)^{-1}XA|^{\frac12}}}\exp{\left\{\frac12Q(\boldsymbol{Z,XA\hat\theta_2,I+XX^T\sigma^2})\right\}}\pi(\sigma^2) &&(10)\\ 
\end{align}
\]

where \(Q(\boldsymbol{Z,\mu,\Sigma})=\boldsymbol{(Z-\mu)^T\Sigma^{-1}(Z-\mu)}\) and \(c(\mathbf{Z)}\) is a proportionality constant.

one starts with initial guesses at $\boldsymbol{\beta}$ and $\sigma^2$, simulates the $Z_i$ from (3), and then simulates $\boldsymbol{\beta}$ and $\sigma^2$ from the distributions (9) and (10)

### 4 Generalizations to a Multinomial response

### 5.1 Finney Data

\[\Phi^{-1}(p_i)=\beta_0+\beta_1x_{1i}+\beta_2x_{2i},\ i=1,..,39\]

where $x_{1i}$ is the volume of air inspired, $x_{2i}$ is the rate of air inspired, and the binary outcome observed is the occurrence
 or nonoccurrence on a transient vasorestriction on the skin of the digits. $\beta\sim Unif$
 prior is placed on the regression parameter

```{r,echo=T,collapse=T}
data(Finney)
Vol <- Finney$Vol; Rate <- Finney$Rate; Resp <- Finney$Resp
lVol <-log(Vol); lRate <- log(Rate)
plotFdat <- Finney$plotFdat
plotFdat(Rs=Resp,lV=lVol,lR=lRate,zc,zr,rob=F,cont=F)
finney <- data.frame(Finney[1:3])
finney[,3] <- as.factor(finney[,3])
table(finney[3])
n <- nrow(finney)
```


The starting value $\beta^{(0)}$ is the least square estimate $(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$

```{r, echo=T,collapse=T}
# X <- cbind(Vol, Rate)
# (beta <- solve( t(X) %*% X ) %*% t(X) %*% Resp) #start from LS estimate
X <- cbind(1, Vol, Rate)
(beta <- solve( t(X) %*% X ) %*% t(X) %*% Resp)
```

\(\mu=\mathbf{x}_i^T\boldsymbol{\beta}\)

```{r, echo=T,collapse=T}
mu <- t(X %*% beta) # Initial mu of Z
```

\(Z_i|\boldsymbol{y,\beta}\sim N(\mathbf{x}_i^T\boldsymbol{\beta},1)\) truncated by 0 at the \(\begin{cases}\text{left if}&y_i=1\\\text{right if}&y_i=0\end{cases}\)

```{r, echo=T,collapse=T}
tr_norm<-function(n,mu,v,truc_side='left'){ # Truncated Normal function
 u<-runif(n,min=0,max=1)
  if(truc_side=='left'){
 tr_norm_value<-u-u*pnorm(0,mean=mu,sd=sqrt(v))+pnorm(0,mean=mu,sd=sqrt(v))}
 if(truc_side=='right'){
   tr_norm_value<-u*pnorm(0,mean=mu,sd=sqrt(v))}
  qnorm(tr_norm_value,mu,sqrt(v))
}

z<- ifelse(Resp==1, # Initial Z
tr_norm(1,mu,1),
tr_norm(1,mu,1,'right')
)

# anohter attempt of truncated
#for(j in 1:n){  
#ifelse(Resp[j]==1,
#z[j] <- rtruncnorm(1,a=0,b=Inf,mean=mu[j],sd=1),
#z[j] <- rtruncnorm(1,a=Inf,b=0,mean=mu[j],sd=1)
#)
#}

# anohter attempt of truncated
#z<- abs(rnorm(n,mean=mu,sd=1)),
#z<- -abs(rnorm(n,mean=mu,sd=1))
```

\(\boldsymbol{\hat\beta_Z}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Z}\)

\(\Omega=(\mathbf{X}^T\mathbf{X})^{-1})\)

```{r, echo=T,collapse=T}
solve(t(X)%*%X)%*%t(X)%*%z # first mu for Beta
solve(t(X)%*%X)            # first omega for Beta
```


```{r}
M <- 10000; set.seed(123)
gibbs.mat <- matrix(NA,ncol=length(beta),nrow=M) #Gibbs Sampelr variables
for(m in 1:M){  #Gibbs Sampler
  
beta <- mvrnorm(1, solve(t(X)%*%X)%*%t(X)%*%z, solve(t(X)%*%X)) # draw beta from Normal

z<- ifelse(Resp==1, # draw Z from truncated Normal
tr_norm(1 ,t(X %*% beta), 1),
tr_norm(1 ,t(X %*% beta), 1 ,'right')
)
gibbs.mat[m,] <- t(beta)   # store draws
}
```

\(\boldsymbol{\hat\beta}\approx\frac1m\sum_{i=1}^m(\boldsymbol{\beta|Z^{(i)},\lambda^{(i)}})\)

```{r,echo=T,collapse=T}
burnin <- 1:(M/2)
pumps.quant <- apply(gibbs.mat[-burnin,],2,quantile,probs=c(0.025,0.5,0.975))
mean <- colMeans(gibbs.mat[-burnin,])
pumps.par<- rbind(mean,pumps.quant)
colnames(pumps.par) <- c("Beta0","Beta1","Beta2")
pander::pander(round(t(pumps.par),4))
```

```{r,echo=T,include=T, message=F, warning=F, fig.width=9, fig.height=6, fig.align='center'}
par(mfrow=c(3,3),mar=c(3,3.2,.5,.5),mgp=c(1.70,.70,0))
for(j in 1:3){
plot(gibbs.mat[-burnin,j],  ylab=expression(beta[j]^{(m)}),main="",pch=1,cex=0.3,
     xlab="Gibbs iteration (k)",col="cornflowerblue")  
plot(cumsum(gibbs.mat[-burnin,j])/(1:(M/2)),  ylab=expression(E(beta[j])),main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
hist(gibbs.mat[-burnin,j],freq =F,xlab=paste("distribution of est. for beta",j),
     main="",col="cornflowerblue")
abline(v=quantile(gibbs.mat[-burnin,j],c(0.025,0.975)),col="red",lwd=1)
}
```


\(p_i=\Phi(\mathbf{x}_i^T\boldsymbol{\beta})\)

```{r,echo=T,collapse=T}
(p <- pnorm(t(X %*% beta),0,1))
```

```{r,echo=F,eval=F, message=F, warning=F, fig.width=6, fig.height=3, fig.align='center'}
par(mfrow=c(2,3),mar=c(3,3,.5,.5),mgp=c(1.70,.70,0))
plot(gibbs.mat[-burnin,(n+1)],  ylab=expression(beta^{(k)}),main="",pch=20,cex=0.3,
     xlab="Gibbs iteration (k)",col="cornflowerblue")
plot(cumsum(gibbs.mat[-burnin,(n+1)])/(1:(S/2)),  ylab=expression(E(beta)),main="",
     type="l",col="cornflowerblue",lwd=2,pch=20,cex=0.7,xlab="Gibbs iteration (k)")
hist(gibbs.mat[-burnin,(n+1)],  xlab=expression(paste("distribution of est. for ",beta)),
     main="",col="cornflowerblue")
abline(v=quantile(gibbs.mat[-burnin,(n+1)],c(0.025,0.975)),col="red",lwd=1)

u <- seq(0.01,4,length=1000)
d <- dgamma(u,shape=alpha,rate=pumps.par["50%","Beta"])
plot(u,d,type="l",xlab=expression(lambda),ylab="density",col="cornflowerblue",lwd=2,pch=20,cex=0.7)
abline(v=pumps.quant["50%",1:10],col="grey",lwd=1)

par(las=1,mar=c(3,5,.5,.5))
plot(c(0,4),c(1,10),type="n",xlab=expression(lambda),ylab=" ",axes=FALSE)
axis(side=1)
axis(side=2,at=1:10,labels=paste("Pump #",10:1,sep=""))
segments(pumps.quant["2.5%",1:10],10:1,pumps.quant["97.5%",1:10],10:1,col="cornflowerblue",lwd=2,pch=20,cex=0.7)
segments(pumps.quant["50%",1:10],(10:1)-0.25,pumps.quant["50%",1:10],(10:1)+0.25,col="cornflowerblue",lwd=2,pch=20,cex=0.7)
```







#### Compare with the results by GLM

```{r,eval=T,echo=T,collapse=T}
# Using glm function
fit.probit.glm <- glm(Resp~Vol+Rate,family=binomial(link="probit"))
summary(fit.probit.glm)

# fit.probit.glm.l <- glm(Resp~lVol+lRate,family=binomial(link="probit"))
# fit.logit.glm.l <- glm(Resp~lVol+lRate,family=binomial(link="logit"))
#comp <- fits.compare(fit.probit.glm, fit.probit.glm.l, fit.logit.glm.l)
# comp
# plot(comp)
```

```{r,eval=F,echo=F}
summary(z.logit)
correl(z.logit)
covar(z.logit)
Rank(z.logit)
rscale(z.logit)
# kable(t(data_frame(weights(z.logit))))
```

```{r,eval=F,echo=F}

# From https://data.princeton.edu/
fit.mle.glmer <- glmer(Resp~(1|Vol)+(1|Rate), family=binomial) #, nAGQ = 12, REML=FALSE
summary(fit.mle.glmer)
```

```{r,eval=F,echo=F}
# From Kenneth Train’s exercises using the mlogit package for R

# data("Mode", package="mlogit")
# Mo <- mlogit.data(Mode, choice='choice', shape='wide', varying=c(2:9))
# p1 <- mlogit(choice~cost+time, Mo, seed = 20, R = 100, probit = TRUE)

finney.mlogit <- mlogit.data(finney, choice='Resp', shape='wide') #, varying=c(1:2)
fit.probit.mlogit <- mlogit(Resp~1|Vol+Rate, probit=F,finney.mlogit) #|0 ,reflevel=5,seed=20,R=100
```

```{r,eval=F,echo=F}
# From lecture5_sample;# response must have 3 or more levels
# fit.probit.plr <- polr(Resp~Vol+Rate, method = "probit",finney) 
```







```{r,echo=F,out.width='50%'}
#z.cub <- glm(Resp~lVol+lRate,family=binomial,method="cubinf", ufact=3.2)
# summary(z.cub)
# plot(z.cub, smooth=TRUE, ask=TRUE)
```



```{r,eval=F,echo=F,out.width='50%'}
## Adds a QQ-line for the values in x in the current plot.
x <- residuals(z.probit, type="deviance")
qqnorm(x, ylab = "Deviance Residuals")
QQline(x, lty = 2)
# Predictions provided by a model fit when method is "cubinf".
rVol <- runif(20,0.4,3.7); rRate <- runif(20,0.3,3.75)
newdat <- data.frame(lVol=log(rVol),lRate=log(rRate))
predict(z.probit, newdat, type="response")

```


### 5.2 Election Data


### 5.3 A Trivariate Probit Example



## (Polson etal, 2013)

Nicholas G. Polson, James G. Scott & Jesse Windle (2013) Bayesian Inference for Logistic Models Using Pólya–Gamma Latent Variables, Journal of the American Statistical Association, 108:504, 1339-1349, DOI: 10.1080/01621459.2013.829001