---
title: 'STAT 661: Project'
author: "Jacob, Robin, Ryan & Shen"
date: "Dec, 2019"
output:
  pdf_document:
    number_sections: yes
    toc: no
  ioslides_presentation:
    incremental: yes
  word_document:
    toc: no
  slidy_presentation: default
  beamer_presentation:
    includes:
      in_header: preamble.tex
subtitle: LS v.s. EM
header-includes: \usepackage[ruled,vlined]{algorithm2e}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, message=FALSE, warning=F,fig.align='center',out.width = '100%')

if (!require("pacman")) {install.packages("pacman"); library(pacman)}
# special installation process for printr as it is not available on CRAN
pacman::p_load(tidyverse, knitr, readr, pander, ggplot2,kableExtra)

```



# Appendix

```{r,echo=F}
rm(list=ls())
```

## Least Square Method with Full data

```{r,echo=F}
I_u <- c(11:17,21:25,31:35)
I_c <- c(1:10,18:20,26:30,36:40)
temp <- c(150,170,190,220) #temperature levels 
trec <- 1000/(temp+273.2) #reciprocal of the absolute temperature T 
x <- c(rep(trec[1],10),rep(trec[2],10),rep(trec[3],10),rep(trec[4],10)) 
y<- y_0<-log10(c(                          rep(8064,10),
         1764,2772,3444,3542,3780,4860,5196,rep(5448,3),
                     408,408,1344,1344,1440,rep(1680,5),
                        408,408,504,504,504,rep(528,5)))
w<- unique(y[I_c])
fit0 <- lm(y~x) #linear model between log10 of observed life time 
fit_c <- lm(y[I_c]~x[I_c])
fit_u <- lm(y[I_u]~x[I_u])
```

```{r,echo=F}
S <- 23;delta = 1e-006
PHI<-matrix(nrow=S,ncol=8,dimnames=list(NULL, 
     c('Iteration','Intercept','Slope','Sigma','mu150','mu170','mu190','mu220')))
# Iteration 0 
sigma <- sigma(fit0) #standard error of residuals 
beta0 <- coef(fit0)[1] #intercept 
beta1 <- coef(fit0)[2] #slope 
mu <- beta0 + beta1*unique(x[I_c]) #mean log time to failure 
z <- (w-mu)/sigma #z-vector 
ET <- mu + sigma*dnorm(z)/(1-pnorm(z)) #new expected mean log times to failure
PHI[1,]<-phi<-c(1, beta0, beta1,sigma,ET)
```


```{r,echo=F}
# Subsequent iteration
repeat { 
# Replace the new censored values
 y[I_c] <- c(rep(ET[1],10),rep(ET[2],3),rep(ET[3],5),rep(ET[4],5)) 
 fit <- lm(y~x)   # fit a new model
# fit <- lm(y[11:40]~x[11:40])   # fit a new model
 phi[2] <- coef(fit)[1] #intercept 
 phi[3] <- coef(fit)[2] #slope 
 phi[4] <- sigma(fit) #standard error of residuals
  
 mu <- phi[2] + phi[3]*unique(x[I_c]) 
 z <- (w-mu)/phi[4] #z-vector 
  #new expected mean log times to failure
 # phi[1:4] <- ET<- mu + phi[7]*dnorm(z)/(1-pnorm(z))
 phi[5:8] <- ET<- (mu + phi[4]*dnorm(z)/(1-pnorm(z)))
 conv <- dist(rbind(PHI[phi[1],5:8],phi[5:8]))
 if(conv < delta) break 
 phi[1] <- phi[1]+1   
  PHI[phi[1],]<-phi         
}
```

Using Schmee & Hahn's Least Square method, we reproduced the algoritm and got the exact same results after 22 iterations. The results of $\hat\beta_0$,$\hat\beta_1$, and $\hat\sigma$ are `r PHI[22,2:4]`.

In page 422, The authors say "If one ignores the 150c data, the iterative least squares estimates and the maximum likelihood estimates were even closer to each other than before". I will try to confirm this concultion.


```{r,echo=F}
plot(x,y_0,main="Least Squares Method with full data",xlab="reciprocal of the absolute temperature",ylab="log10 of failure time",
     xlim=c(2,2.5),ylim=c(2.5,5),
     panel.first=abline(h=c(3.577492,3.906551),v=c(2.027575,2.158895,2.256318,2.362949),lty=3,col="gray"))
for (i in (1:max(PHI[,1],na.rm =T)))
abline(PHI[i,2:3],lwd=0.1,lty=1,col=gray(1/max(PHI[,1],na.rm =T)))
abline(fit_c,lwd=0.5,lty=1,col="red")
abline(fit_u,lwd=0.5,lty=1,col="blue")
text(2.45,4.7, labels = "22nd");text(2.45,4.4, labels = "1st")
legend(2.2,3.2,legend=c("regression line after iterations","initial censored data","initial uncensored data"),bty="n",lwd=c(2,2),col=c("black","red","blue"))
```

```{r,echo=F}
kable(PHI)
LS_full<- PHI[max(PHI[,1],na.rm =T),]
# pander::pander(PHI)
```

## Removing 150c


```{r,echo=F}
I_c_no150 <-c(8:10,16:20,26:30)
I_u_no150 <-c(1:7,11:15,21:25)
x_no150 <- x[11:40]
y_no150<- y_0_no150<-y_0[11:40]
w_no150<- w[2:4]
fit0_no150 <- lm(y_no150~x_no150) 
fit_c_no150 <- lm(y_no150[I_c_no150]~x_no150[I_c_no150])
fit_u <- lm(y_no150[I_u_no150]~x_no150[I_u_no150])
```

```{r,echo=F}
S <- 17;delta = 1e-006
PHI<-matrix(nrow=S,ncol=7,dimnames=list(NULL, 
     c('Iteration','Intercept','Slope','Sigma','mu170','mu190','mu220')))
# Iteration 0 
sigma <- sigma(fit0_no150) #standard error of residuals 
beta0 <- coef(fit0_no150)[1] #intercept 
beta1 <- coef(fit0_no150)[2] #slope 
mu <- beta0 + beta1*unique(x_no150) #mean log time to failure 
z <- (w_no150-mu)/sigma #z-vector 
# z <- 0
ET <- mu + sigma*dnorm(z)/(1-pnorm(z)) #new expected mean log times to failure
PHI[1,]<-phi<-c(1, beta0, beta1,sigma,ET)
```


```{r,echo=F}
# Subsequent iteration
repeat { 
# Replace the new censored values
 y_no150[I_c_no150] <- c(rep(ET[1],3),rep(ET[2],5),rep(ET[3],5)) 
 fit <- lm(y_no150~x_no150)   # fit a new model
 phi[2] <- coef(fit)[1] #intercept 
 phi[3] <- coef(fit)[2] #slope 
 phi[4] <- sigma(fit) #standard error of residuals
  
 mu <- phi[2] + phi[3]*unique(x_no150)
 z <- (w_no150-mu)/phi[4] #z-vector 
#  z <- 0
  #new expected mean log times to failure
 phi[5:7] <- ET<- (mu + phi[4]*dnorm(z)/(1-pnorm(z)))
 conv <- dist(rbind(PHI[phi[1],5:7],phi[5:7]))
 if(conv < delta) break 
 phi[1] <- phi[1]+1   
  PHI[phi[1],]<-phi         
}
```


After removing the 150c censored data and 16 iterations, the results of $\hat\beta_0$,$\hat\beta_1$,$\hat\sigma$ are `r PHI[max(PHI[,1],na.rm =T),2:4]`. From the figure and table we can know the estimats of $\hat\beta_1$ and log time to failure are smaller.

However, the censored data are underestimated values. After iterations, we anticipate the estimate should be larger than the before.

The reason might be below:

For the full data, the initial total fitted $\hat\beta_1$ (3.747043) is lager than the $\hat\beta_1$ (3.532411) fitted merely by censored data. The positive difference will be accumulated during the iterations and make the $\hat\beta_1$ and $\hat\mu$ larger and larger until convergency.

Without the 150c data, the initial total fitted $\hat\beta_1$ (3.886073) is smaller than the $\hat\beta_1$ (4.324944) fitted merely by censored data. Thus, the negative difference will be accumulated during the iterations and make the $\hat\beta_1$ and $\hat\mu$ smaller and smaller.


```{r, echo=F}
kable(data.frame(fit0$coef,fit_c$coef,fit0_no150$coef,fit_c_no150$coef))%>%footnote(general = "The initial regression coefficients")
```


```{r,echo=F}
plot(x_no150,y_0_no150,main="LS Method without 150c",xlab="reciprocal of the absolute temperature",ylab="log10 of failure time",
     xlim=c(2,2.5),ylim=c(2.5,5),
     panel.first=abline(h=c(3.577492,3.906551),v=c(2.027575,2.158895,2.256318,2.362949),lty=3,col="gray"))
for (i in (1:max(PHI[,1],na.rm =T)))
abline(PHI[i,2:3],lwd=0.1,lty=1,col=gray(1/max(PHI[,1],na.rm =T)))
abline(fit_c_no150,lwd=0.5,lty=1,col="red")
abline(fit_u,lwd=0.5,lty=1,col="blue")
text(2.48,4.5, labels = "16th");text(2.45,4.4, labels = "1st")
legend(2.2,3.2,legend=c("regression line after iterations","initial censored data","initial uncensored data"),bty="n",lwd=c(2,2),col=c("black","red","blue"))
```

```{r,echo=F}
kable(PHI)
# pander::pander(PHI)
LS_no150<- PHI[max(PHI[,1],na.rm =T),]
```






<!--
### Replace 150c by regression value
A solution is updating the initial value for 150c


```{r,echo=T}
y_new150 <- coef(fit0_no150)[1]+coef(fit0_no150)[2]*trec[1]
y_new150<-c(rep(y_new150,10),y_0[11:40])

w<- unique(y_0[I_c])
fit0_new <- lm(y_new150~x) #linear model between log10 of observed life time 
fit_c_new <- lm(y_new150[I_c]~x[I_c])
fit_u_new <- lm(y_new150[I_u]~x[I_u])
```

After this manipulation, the initial total fitted $\beta_1$ (3.886) is lager than the $\beta_1$ (3.701) fitted merely by censored data.

```{r, echo=F}
data.frame(fit0$coef,fit_c$coef,fit0_no150$coef,fit_c_no150$coef,fit0_new$coef,fit_c_new$coef)
```

```{r,echo=F}
S <- 23;delta = 1e-006
PHI<-matrix(nrow=S,ncol=8,dimnames=list(NULL, 
     c('Iteration','Intercept','Slope','Sigma','mu150','mu170','mu190','mu220')))
# Iteration 0 
sigma <- sigma(fit0_new) #standard error of residuals 
beta0 <- coef(fit0_new)[1] #intercept 
beta1 <- coef(fit0_new)[2] #slope 
mu <- beta0 + beta1*unique(x[I_c]) #mean log time to failure 
z <- (w-mu)/sigma #z-vector 
ET <- mu + sigma*dnorm(z)/(1-pnorm(z)) #new expected mean log times to failure
PHI[1,]<-phi<-c(1, beta0, beta1,sigma,ET)
```


```{r,echo=F}
# Subsequent iteration
repeat { 
# Replace the new censored values
y_new150[I_c] <- c(rep(ET[1],10),rep(ET[2],3),rep(ET[3],5),rep(ET[4],5)) 
 fit <- lm(y_new150~x)   # fit a new model
# fit <- lm(y[11:40]~x[11:40])   # fit a new model
 phi[2] <- coef(fit)[1] #intercept 
 phi[3] <- coef(fit)[2] #slope 
 phi[4] <- sigma(fit) #standard error of residuals
  
 mu <- phi[2] + phi[3]*unique(x[I_c]) 
 z <- (w-mu)/phi[4] #z-vector 
  #new expected mean log times to failure
 # phi[1:4] <- ET<- mu + phi[7]*dnorm(z)/(1-pnorm(z))
 phi[5:8] <- ET<- (mu + phi[4]*dnorm(z)/(1-pnorm(z)))
 conv <- dist(rbind(PHI[phi[1],5:8],phi[5:8]))
 if(conv < delta) break 
 phi[1] <- phi[1]+1   
  PHI[phi[1],]<-phi         
}
```

```{r,echo=F}
plot(x,y_0,main="Least Squares Method",xlab="reciprocal of the absolute temperature",ylab="log10 of failure time",
     xlim=c(2,2.5),ylim=c(2.5,5),
     panel.first=abline(h=c(3.577492,3.906551),v=c(2.027575,2.158895,2.256318,2.362949),lty=3,col="gray"))
for (i in (1:max(PHI[,1],na.rm =T)))
abline(PHI[i,2:3],lwd=0.1,lty=1,col=gray(1/max(PHI[,1],na.rm =T)))
abline(fit_c,lwd=0.5,lty=1,col="red")
abline(fit_u,lwd=0.5,lty=1,col="blue")
text(2.45,4.7, labels = "22nd");text(2.45,4.4, labels = "1st")
legend(2.2,3.2,legend=c("regression line after iterations","initial censored data","initial uncensored data"),bty="n",lwd=c(2,2),col=c("black","red","blue"))
```

```{r,echo=F}
kableExtra::kable(PHI)
# pander::pander(PHI)
```

-->


\pagebreak


## EM Method with full data


$$\log(f(\beta_0,\beta_1,\sigma|y,z))=-\frac{n}2\ln(2\pi)-n\ln(\sigma)-\frac1{2\sigma^2}\sum_{i=1}^n(t_i-\beta_0-\beta_1\nu_i)^2$$

$\mu_i=\beta_0-\beta_1\nu_i$;
$\mu_j=\beta_0-\beta_1\nu_j$;
$z^{\star}_i=\frac{w_i-\mu_i^{\star}}{\sigma^{\star}}$

- E-step
Let $\vec\theta=(\beta_0,\beta_1,\sigma)$

$$E[T_i|T_i>w_i,\vec\theta^\star]=\mu_i^\star+\sigma^{\star}H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})$$

$$E[T_i^2|T_i>w_i,\vec\theta^\star]=\mu_i^{\star2}+\sigma^{\star2}+\sigma^{\star}(w_i+\mu_i^{\star})H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})$$

$$E[(T_i-\beta_0-\beta_1\nu_i)^2|T_i>w_i,\vec\theta^\star]=\mu_i^{\star2}+\sigma^{\star2}+\sigma^{\star}(w_i+\mu_i^{\star})H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})-2(\beta_0+\beta_1\nu_i)[\mu_i^\star+\sigma^{\star}H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})]+(\beta_0+\beta_1\nu_i)^2$$

$$E[(T_i-\mu_i)^2|T_i>w_i,\vec\theta^\star]=(\mu_i^{\star}-\mu_i)^2+\sigma^{\star2}+\sigma^{\star}H(z^{\star}_i)(w_i+\mu_i^{\star}-2\mu_i)$$


$$Q(\vec\theta,\vec\theta^\star)=-\frac{n}2\ln(2\pi)-n\ln(\sigma)-\frac1{2\sigma^2}\sum_{j=1}^m(t_j-\beta_0-\beta_1\nu_j)^2-\frac1{2\sigma^2}\sum_{i=m+1}^nE[(T_i-\beta_0-\beta_1\nu_i)^2|T_i>w_i,\vec\theta^\star]$$

$$=-\frac{n}2\ln(2\pi)-n\ln(\sigma)-\frac1{2\sigma^2}\sum_{j=1}^m(t_j-\mu_j)^2-\frac1{2\sigma^2}\sum_{i=m+1}^n[(\mu_i^{\star}-\mu_i)^2+\sigma^{\star2}+\sigma^{\star}H(z^{\star}_i)(w_i+\mu_i^{\star}-2\mu_i)]$$


- M-step

$$\frac{\partial Q}{\partial\beta_0}=-\frac1{\sigma^2}\left\{\sum_{j=1}^m[t_j-\beta_0-\beta_1\nu_j]+\sum_{i=m+1}^n[\mu_i^\star+\sigma^{\star}H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})-\beta_0-\beta_1\nu_i]\right\}=0$$

$$n\beta_0=\sum_{j=1}^mt_j+\sum_{i=m+1}^n[\mu_i^\star+\sigma^{\star}H(z^{\star}_i)]-\beta_1n\bar\nu$$


$$\frac{\partial Q}{\partial\beta_1}=-\frac1{\sigma^2}\left\{\sum_{j=1}^m[t_j-\beta_0-\beta_1\nu_j]\nu_j+\sum_{i=m+1}^n[\mu_i^\star+\sigma^{\star}H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})-\beta_0-\beta_1\nu_i]\nu_i\right\}=0$$

$$n\beta_0\bar\nu+\beta_1\sum_{i=1}^n\nu_i^2=\sum_{j=1}^mt_j\nu_j+\sum_{i=m+1}^n[\mu_i^\star+\sigma^{\star}H(z^{\star}_i)]\nu_i$$

$$\beta_1(\sum_{i=1}^n\nu_i^2-n\bar\nu^2)=\sum_{j=1}^mt_j(\nu_j-\bar\nu)+\sum_{i=m+1}^n[\mu_i^\star+\sigma^{\star}H(z^{\star}_i)](\nu_i-\bar\nu)$$


$$\frac{\partial Q}{\partial\sigma^2}=\frac1{2\sigma^2}\left\{-n+\frac1{\sigma^2}\sum_{j=1}^m[t_j-\beta_0-\beta_1\nu_j]^2+\frac1{\sigma^2}\sum_{i=m+1}^nE[(T_i-\beta_0-\beta_1\nu_i)^2|T_i>w_i,\vec\theta^\star]\right\}=0$$
$$\sum_{j=1}^m[t_j-\beta_0-\beta_1\nu_j]^2+\sum_{i=m+1}^n\left\{\mu_i^{\star2}+\sigma^{\star2}+\sigma^{\star}(w_i+\mu_i^{\star}-2\mu_i)H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})-2\mu_i\mu_i^\star+\mu_i^2\right\}=n\sigma^2$$

$$\sigma^2=\frac1n\sum_{j=1}^m[t_j-\mu_j]^2+\frac1n\sum_{i=m+1}^n\left\{(\mu_i^{\star}-\mu_i)^2+\sigma^{\star2}+\sigma^{\star}H(z^{\star}_i)(w_i+\mu_i^{\star}-2\mu_i)\right\}$$

## the EM algorithm's pseudo code

\begin{algorithm}[htbp]
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{observed data $\mathcal{D}=\{\vec{x}_1,\vec{x}_2,\cdots, \vec{x}_N$\},joint distribution $P(\vec{x},\vec{z}|\vec{\theta})$}
	\Output{model's parameters $\vec{\theta}$}

	// 1. identify hidden variables $\vec{z}$, write out the log likelihood function $\ell(\vec{x},\vec{z}|\vec{\theta})$ \\
	$\vec{\theta}^{(0)}$ = ... // initialize \\
	
	\While{(!convergency)} {
	    // 2. E-step: plug in $P(\vec{x},\vec{z}|\vec{\theta})$, derive the formula of $Q(\vec{\theta}, \vec{\theta}^{t-1})$ \\
	    $Q(\vec{\theta}, \vec{\theta}^{t-1})=\mathbb{E}\left[\ell_c(\vec{\theta})| \mathcal{D},\theta^{t-1}\right]$ \\
	    // 3. M-step: find $\vec{\theta}$ that maximizes the value of $Q(\vec{\theta}, \vec{\theta}^{t-1})$ \\
		$\vec{\theta}^t=\arg\max\limits_{\vec{\theta}} Q(\vec{\theta}, \vec{\theta}^{t-1})$ \\
	}
	
\caption{EM algorithm}
\end{algorithm}


```{r,echo=F}
I_u <- c(11:17,21:25,31:35)
I_c <- c(1:10,18:20,26:30,36:40)
temp <- c(150,170,190,220) #temperature levels 
trec <- 1000/(temp+273.2) #reciprocal of the absolute temperature T 
nu <- c(rep(trec[1],10),rep(trec[2],10),rep(trec[3],10),rep(trec[4],10))
t<- t_0<-log10(c(                          rep(8064,10),
         1764,2772,3444,3542,3780,4860,5196,rep(5448,3),
                     408,408,1344,1344,1440,rep(1680,5),
                        408,408,504,504,504,rep(528,5)))

# initial value
m <- length(I_u)
n<-length(I_c)+m
w<- unique(t_0[I_c])

nu_i <- nu[I_c]
nu_j <- nu[I_u]
nu_bar <- mean(nu)

t_i<- t[I_c]
t_j<- t[I_u]


fit0 <- lm(t~nu) 
fit_c <- lm(t_i~nu_i)
fit_u <- lm(t_j~nu_j)

sigma <- sigma(fit0) #standard error of residuals 
beta0 <- coef(fit0)[1] #intercept 
beta1 <- coef(fit0)[2] #slope 


mu_j <- beta0 + beta1 * nu_j
SS_nu <- sum(nu^2)-n*nu_bar^2
mu_i <- beta0 + beta1 * nu_i
z_i=(t_i-mu_i)/sigma
H_i <- dnorm(z_i)/(1-pnorm(z_i))
# H_i <-dnorm(0)/(1-pnorm(0))
ET_i  <- mu_i+ sigma*H_i

S <- 60
THETA<-matrix(nrow=S,ncol=8,dimnames=list(NULL, 
     c('Iteration','Intercept','Slope','Sigma','mu150','mu170','mu190','mu220')))
THETA[1,]<-theta<-c(1, beta0, beta1,sigma,unique(ET_i))
Q <- 0
k=1
delta = 1e-4
```

```{r,echo=F}
repeat {  
beta0<- THETA[k,2]
beta1<- THETA[k,3]
sigma<- THETA[k,4]

# M step    

mu_i <- beta0 + beta1 * nu_i
#z_i=(t_i-mu_i)/sigma
#H_i <- dnorm(z_i)/(1-pnorm(z_i))
ET_i <- mu_i+ sigma*H_i

beta0_star <- sum(t_j)/n+sum(ET_i)/n-
              nu_bar/SS_nu*(sum(t_j*(nu_j-nu_bar))+sum(ET_i*(nu_i-nu_bar)))
               
beta1_star <- (sum(t_j*(nu_j-nu_bar))+sum(ET_i*(nu_i-nu_bar)))/SS_nu

mu_i_star <- beta0_star+beta1_star*nu_i 
mu_j_star <- beta0_star+beta1_star*nu_j

sigma_star <- (sum((t_j-mu_j_star)^2)+
                 sum(sigma^2+sigma*H_i*(ET_i+mu_i-2*mu_i_star)+(mu_i-mu_i_star)^2)
               )/n

ET_i_star <- mu_i_star+ sigma_star*H_i

# E step
# Get Q
k <-  k+1  
Q[k] <-  
 -n*log(2*pi)/2-
  n*log(sigma_star)-
 (1/(2*sigma_star^2))*sum((t_j-mu_j_star)^2)-# 1/2sigma^2*sum(j uncensored)-
 (1/(2*sigma_star^2))*sum(                   # 1/2sigma^2*sum(i censored  
                      sigma^2+       # sigma^2+                         
                      sigma*H_i*(    # sigma*H*(
          ET_i+mu_i-2*mu_i_star)+     # w_i+mu_i_star-2mu_i)
             (mu_i-mu_i_star)^2)     # (mu_i-mu_i_star)^2)
        
# Update THETA
 THETA[k,2] <- beta0_star
 THETA[k,3] <- beta1_star
 THETA[k,4] <- sigma_star
THETA[k,5:8]<-unique(ET_i_star)
THETA[k,1] <- k

 if(abs(Q[k]-Q[k-1])<=delta) break
}
```

Using the EM method, the results of $\hat\beta_0$,$\hat\beta_1$,$\hat\sigma$ are `r THETA[max(THETA[,1],na.rm =T),2:4]` after 58th iterations. Although the estimates are smaller than SL method, Thay shows a same trend: $\hat\beta_1$ and $\hat\mu$  grow larger and converges.


```{r,echo=F}
plot(nu,t_0,main="EM Method with full data",xlab="reciprocal of the absolute temperature",ylab="log10 of failure time",
     xlim=c(2,2.5),ylim=c(2.5,5),
     panel.first=abline(h=c(3.577492,3.906551),v=c(2.027575,2.158895,2.256318,2.362949),lty=3,col="gray"))
for (i in (1:max(THETA[,1],na.rm =T)))
abline(THETA[i,2:3],lwd=0.1,lty=1,col=gray(i/max(THETA[,1],na.rm =T)))
abline(fit_c,lwd=0.5,lty=1,col="red")
abline(fit_u,lwd=0.5,lty=1,col="blue")
text(2.48,4.6, labels = "58th");text(2.45,4.35, labels = "1st")
legend(2.2,3.2,legend=c("regression line after iterations","initial censored data","initial uncensored data"),bty="n",lwd=c(2,2),col=c("black","red","blue"))
```

```{r,echo=F}
kable(THETA[c(1:10,48:58),])%>%footnote(general = "The first and last 10 rows")
EM_full<- THETA[max(THETA[,1],na.rm =T),]
```

## EM Method removing 150c data

```{r,echo=F}
rm(list=ls())
I_c<-c(8:10,16:20,26:30)
I_u<-c(1:7,11:15,21:25)
temp <- c(170,190,220) 
trec <- 1000/(temp+273.2) 
nu <- c(rep(trec[1],10),rep(trec[2],10),rep(trec[3],10))
t<- t_0<-log10(c(                         
  1764,2772,3444,3542,3780,4860,5196,rep(5448,3),
  408,408,1344,1344,1440,rep(1680,5),
  408,408,504,504,504,rep(528,5)))

# initial value
m <- length(I_u)
n<-length(I_c)+m
w<- unique(t_0[I_c])

nu_i <- nu[I_c]
nu_j <- nu[I_u]
nu_bar <- mean(nu)
nu_bar_i <- mean(nu_i)
nu_bar_j <- mean(nu_j)

t_i<- t[I_c]
t_j<- t[I_u]


fit0 <- lm(t~nu) 
fit_c <- lm(t_i~nu_i)
fit_u <- lm(t_j~nu_j)

sigma <- sigma(fit0) #standard error of residuals 
beta0 <- coef(fit0)[1] #intercept 
beta1 <- coef(fit0)[2] #slope 

mu_j <- beta0 + beta1 * nu_j
SS_nu <- sum((nu_j)^2)+sum((nu_i)^2)-n*nu_bar^2
mu_i <- beta0 + beta1 * nu_i
H_i <- dnorm((t_i-mu_i)/sigma)/(1-pnorm((t_i-mu_i)/sigma))
# H_i <-dnorm(0)/(1-pnorm(0))

ET  <- unique(mu_i+ sigma*H_i) 

S <- 25
THETA<-matrix(nrow=S,ncol=7,dimnames=list(NULL, 
                                          c('Iteration','Intercept','Slope','Sigma','mu170','mu190','mu220')))
THETA[1,]<-theta<-c(1, beta0, beta1,sigma,ET)
Q <- 0
k=1
delta = 1e-4
```

```{r,echo=F}
repeat {  
  beta0<- THETA[k,2]
  beta1<- THETA[k,3]
  sigma<- THETA[k,4]
  
  # M step    
mu_i <- beta0 + beta1 * nu_i
#z_i=(t_i-mu_i)/sigma
#H_i <- dnorm(z_i)/(1-pnorm(z_i))
ET_i <- mu_i+ sigma*H_i

beta0_star <- sum(t_j)/n+sum(ET_i)/n-
              nu_bar/SS_nu*(sum(t_j*(nu_j-nu_bar))+sum(ET_i*(nu_i-nu_bar)))
               
beta1_star <- (sum(t_j*(nu_j-nu_bar))+sum(ET_i*(nu_i-nu_bar)))/SS_nu

mu_i_star <- beta0_star+beta1_star*nu_i 
mu_j_star <- beta0_star+beta1_star*nu_j

sigma_star <- (sum((t_j-mu_j_star)^2)+
                 sum(sigma^2+sigma*H_i*(ET_i+mu_i-2*mu_i_star)+(mu_i-mu_i_star)^2)
               )/n

ET_i_star <- mu_i_star+ sigma_star*H_i
  
  # E step
  # Get Q
  k <-  k+1  
  Q[k] <-  
    -n*log(2*pi)/2-
    n*log(sigma_star)-
    (1/(2*sigma_star^2))*sum((t_j-mu_j_star)^2)-# 1/2sigma^2*sum(j uncensored)-
    (1/(2*sigma_star^2))*sum(                   # 1/2sigma^2*sum(i censored  
                            sigma^2+       # sigma^2+                         
                            sigma*H_i*(    # sigma*H*(
                ET_i+mu_i-2*mu_i_star)+     # w_i+mu_i_star-2mu_i)
                   (mu_i-mu_i_star)^2)     # (mu_i-mu_i_star)^2)
  
  # Update THETA
  THETA[k,2] <- beta0_star
  THETA[k,3] <- beta1_star
  THETA[k,4] <- sigma_star
  THETA[k,5:7]<-unique(ET_i_star)
  THETA[k,1] <- k
  
  if(abs(Q[k]-Q[k-1])<=delta) break
}
```




```{r,echo=F}
plot(nu,t_0,main="EM Method without 150c",xlab="reciprocal of the absolute temperature",ylab="log10 of failure time",
     xlim=c(2,2.5),ylim=c(2.5,5),
     panel.first=abline(h=c(3.577492,3.906551),v=c(2.027575,2.158895,2.256318,2.362949),lty=3,col="gray"))
for (i in (1:max(THETA[,1],na.rm =T)))
  abline(THETA[i,2:3],lwd=0.1,lty=1,col=gray(1/max(THETA[,1],na.rm =T)))
abline(fit_c,lwd=0.5,lty=1,col="red")
abline(fit_u,lwd=0.5,lty=1,col="blue")
text(2.48,4.5, labels = "24th");text(2.45,4.4, labels = "1st")
legend(2.2,3.2,legend=c("regression line after iterations","initial censored data","initial uncensored data"),bty="n",lwd=c(2,2),col=c("black","red","blue"))

```

```{r,echo=F}
kable(THETA)
# pander::pander(Q)
EM_no150<- THETA[max(THETA[,1],na.rm =T),]
```

## Summary

Using Maximum Likelihood Mehtod, both Schmee & Hahn (1979), and Aitkin (1981) get a smaller $\hat\beta_0=-6.019$, a larger $\hat\beta_1=4.311$, and larger estimate of the expected log time to failure times. I cannot reproduce these results. The estimates of removing 150c were not closer to each other too.  In my attempts, Least Square Method and EM Method give similar results that ignoring the 150c data make $\hat\beta_1$ and $\hat\mu$ smaller.

```{r,echo=F}

compare<- dplyr::bind_rows(LS_full,EM_full,LS_no150,EM_no150)
rownames(compare) <- c("LS_full","EM_full","LS_no150","EM_no150")
kable(compare)
```







# Reference

Schmee, J., & Hahn, G. (1979). A Simple Method for Regression Analysis with Censored Data. Technometrics, 21(4), 417-432. doi:10.2307/1268280

Aitkin, M. (1981). A Note on the Regression Analysis of Censored Data. Technometrics, 23(2), 161-163. doi:10.2307/1268032





<!--



## Introduction 


Abstract of Schmee, J., & Hahn, G. (1979) :"Problems requiring regression analysis of censored data arise frequently in practice. For example, in accelerated testing one wishes to relate stress and average time to failure from data including unfailed units, i. e., censored observations. Maximum likelihood is one method for obtaining the desired estimates; in this paper, we propose an alternative approach. An initial least squares fit is obtained treating the censored values as failures. Then, based upon this initial fit, the expected failure time for each censored observation is estimated. These estimates are then used, instead of the censoring times, to obtain a revised least squares fit and new expected failure times are estimated for the censored values. These are then used in a further least squares fit. The procedure is iterated until convergence is achieved. This method is simpler to implement and explain to non-statisticians than maximum likelihood and appears to have good statistical and convergence properties. The method is illustrated by an example, and some simulation results are described. Variations and areas for further study also are discussed."


## Least Square Method

Description of method for simple situation

$\mu_x=\beta_0+\beta_1x$

$\mu^\star_x=\mu_x+\frac{\sigma f(z)}{1-F(z)}$
where
$z=\frac{(c_x-\mu_x)}{\sigma}$

- Iteration 0

   + Step 1:$\hat\beta_0^{(0)}=-4.9307$,$\hat\beta_1^{(0)}=3.7471$,$\hat\sigma^{(0)}=0.1572$.
   + Step 2:$x=\frac{1000}{170+273.2}=2.256318$

$\hat\mu^{(0)}_{2.26}=-4.9307+3.7471\frac{1000}{170+273.2}=3.523948$

$C_{2.26}=\log_{10}(5448)=3.736237$

$z=\frac{C_{2.26}-\hat\mu^{(0)}_{2.26}}{\sigma}=\frac{3.736237-3.523948}{0.1572178}=1.350286$

$\hat\mu^{\star(0)}_{2.26}=\hat\mu^{(0)}_{2.26}+\hat\sigma^{(0)}\frac{ f(z)}{1-F(z)}=3.8089$
Or 6440 hours

- Iteration 1

   + Step 1:$\hat\beta_0^{(1)}=-5.2603$,$\hat\beta_1^{(1)}=3.9263$,$\hat\sigma^{(1)}=0.1799$.
   + Step 2:$\hat\mu^{\star(1)}_{2.26}=3.83972$

- Subsequent Iterations

$\hat\beta_0=-5.81829$,$\hat\beta_1= 4.20426$,$\hat\sigma= 0.204322$.

$\hat\mu^{\star(17)}_{2.26}=3.87676$








\section{The EM algorithm}

> A reference

\subsection{Introduction}
For many models in machine learning and statistics, computing the ML or MAP parameter estimate is easy provided we observe all the values of all the relevant random variables, i.e., if we have complete data. However, if we have missing data and/or latent variables, then computing the ML/MAP estimate becomes hard.

One approach is to use a generic gradient-based optimizer to find a local minimum of the NLL$(\vec{\theta})$. However, we often have to enforce constraints, such as the fact that covariance matrices must be positive definite, mixing weights must sum to one, etc., which can be tricky. In such cases, it is often much simpler (but not always faster) to use an algorithm called \textbf{expectation maximization},or \textbf{EM} for short (Dempster et al. 1977; Meng and van Dyk 1997; McLachlan and Krishnan 1997). This is is an efficient iterative algorithm to compute the ML or MAP estimate in the presence of missing or hidden data, often with closed-form updates at each step. Furthermore, the algorithm automatically enforce the required constraints.



\subsection{Basic idea}
EM exploits the fact that if the data were fully observed, then the ML/ MAP estimate would be easy to compute. In particular, each iteration of the EM algorithm consists of two processes: The E-step, and the M-step. 
\begin{itemize}
\item{In the \textbf{E-step}, the missing data are inferred given the observed data and current estimate of the model parameters. This is achieved using the conditional expectation, explaining the choice of terminology.}
\item{In the \textbf{M-step}, the likelihood function is maximized under the assumption that the missing data are known. The missing data inferred from the E-step are used in lieu of the actual missing data.}
\end{itemize}

Let $\vec{x}_i$ be the visible or observed variables in case $i$, and let $\vec{z}_i$ be the hidden or missing variables. The goal is to maximize the log likelihood of the observed data:
\begin{equation}
\ell(\vec{\theta})=\log p(\mathcal{D}|\vec{\theta})=\sum\limits_{i=1}^N \log p(\vec{x}_i|\vec{\theta})=\sum\limits_{i=1}^N \log{\sum\limits_{\vec{z}_i} p(\vec{x}_i,\vec{z}_i|\vec{\theta})}
\end{equation}

Unfortunately this is hard to optimize, since the log cannot be pushed inside the sum.

EM gets around this problem as follows. Define the \textbf{complete data log likelihood} to be
\begin{equation}
\ell_c(\vec{\theta})=\sum\limits_{i=1}^N \log p(\vec{x}_i,\vec{z}_i|\vec{\theta})
\end{equation}

This cannot be computed, since $\vec{z}_i$ is unknown. So let us define the \textbf{expected complete data log likelihood} as follows:
\begin{equation}\label{eqn:auxiliary-function}
Q(\vec{\theta},\vec{\theta}^{t-1}) \triangleq \mathbb{E}_{\vec{z}|\mathcal{D},\theta^{t-1}}\left[\ell_c(\vec{\theta})\right]=\mathbb{E}\left[\ell_c(\vec{\theta})| \mathcal{D},\theta^{t-1}\right]
\end{equation}
where $t$ is the current iteration number. $Q$ is called the \textbf{auxiliary function}. The expectation is taken wrt the old parameters, $\vec{\theta}^{t-1}$, and the observed data $\mathcal{D}$. The goal of the E-step is to compute $Q(\vec{\theta},\vec{\theta}^{t-1})$, or rather, the parameters inside of it which the MLE(or MAP) depends on; these are known as the \textbf{expected sufficient statistics} or \textbf{ESS}. In the M-step, we optimize the $Q$ function wrt $\vec{\theta}$:
\begin{equation}
\vec{\theta}^t=\arg\max_{\vec{\theta}} Q(\vec{\theta},\vec{\theta}^{t-1})
\end{equation}

To perform MAP estimation, we modify the M-step as follows:
\begin{equation}
\vec{\theta}^t=\arg\max_{\vec{\theta}} Q(\vec{\theta},\vec{\theta}^{t-1})+\log p(\vec{\theta})
\end{equation}
The E step remains unchanged.



# Another reference

Now, the density of this mixture distribution can be expressed as

$$f(x; \mu, \sigma, \pi) = \sum_{r=1}^{2}\pi_r f_r(x; \mu_r, \sigma_r, \pi_r)$$

The complete data log likelihood can be expressed as

$$\ell(x, u_j; \mu, \sigma, \pi) = \sum_{r=1}^{2} I(u_j=r)\left[\log\pi_r+\log f_r(x; \mu_r, \sigma_r, \pi_r)\right]$$

where $I(u_j=r)$ is an indicator of to which subpopulation each observation $x_j$ belongs.

## E step

In the E step, the conditional probability to which subpopulation each observation $x_j$ belongs must be computed first

$$Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi) = \frac{\pi_r f_r(x_j;  \mu_r, \sigma_r, \pi_r)}{\sum_{r=1}^{2} \pi_r f_r(x_j; \mu_r, \sigma_r, \pi_r)}$$

Then the expected value of the log likelihood based on a random sample can be expressed as

$$Q = \sum_{j=1}^{n} \sum_{r=1}^{2} Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi) \left[\log\pi_r+\log f_r(x; \mu_r, \sigma_r, \pi_r)\right]$$


## M step

In the M step, the miximizing values for $\mu_r$, $\sigma_r$, and $\pi_r$ are calculated as

$$\pi_r^{'} = \frac{\sum_{j=1}^{n}Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}{n}$$

$$\mu_r^{'} = \frac{\sum_{j=1}^{n}x_jPr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}{\sum_{j=1}^{n}Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}$$

$$\sigma_r^{'} = \sqrt{\frac{\sum_{j=1}^{n}(x_j- \mu_r^{'})^2Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}{\sum_{j=1}^{n}Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}}$$

In the next iteration, the expected value of the log likelihood is recalculated using the new parameters, and is compared to $Q$ from the previous step. This process is repeated until the converting criteria is met, which in this case is the difference less than $10^{-6}$. Putting these all together, I have the EM Algorithm implemented as below.

```{r, eval=F}
# modified sum only considers finite values
sum.finite <- function(x) {
  sum(x[is.finite(x)])
}

Q <- 0
# starting value of expected value of the log likelihood
Q[2] <- sum.finite(log(pi1)+log(dnorm(x, mu1, sigma1))) + sum.finite(log(pi2)+log(dnorm(x, mu2, sigma2)))

k <- 2

while (abs(Q[k]-Q[k-1])>=1e-6) {
  # E step
  comp1 <- pi1 * dnorm(x, mu1, sigma1)
  comp2 <- pi2 * dnorm(x, mu2, sigma2)
  comp.sum <- comp1 + comp2
  
  p1 <- comp1/comp.sum
  p2 <- comp2/comp.sum
  
  # M step
  pi1 <- sum.finite(p1) / length(x)
  pi2 <- sum.finite(p2) / length(x)
  
  mu1 <- sum.finite(p1 * x) / sum.finite(p1)
  mu2 <- sum.finite(p2 * x) / sum.finite(p2)
  
  sigma1 <- sqrt(sum.finite(p1 * (x-mu1)^2) / sum.finite(p1))
  sigma2 <- sqrt(sum.finite(p2 * (x-mu2)^2) / sum.finite(p2))
  
  p1 <- pi1 
  p2 <- pi2
  
  k <- k + 1
  Q[k] <- sum(log(comp.sum))
}
```

It takes 120 iteration for the EM Algorithm to converge. The estimated parameters are $\mu_1= 0.3312$, $\mu_2= 0.4464$, $\sigma_1= 0.0195$, $\sigma_2= 0.0548$, $\pi_1= 0.1611$, and $\pi_2= 0.8389$.

Next, the function normalmixEM in the R package mixtools is used on the same dataset and the results follows

```{r, eval=F}
library(mixtools)
gm<-normalmixEM(x,k=2,lambda=c(0.9,0.1),mu=c(0.4,0.3),sigma=c(0.05,0.02))
```

The results agree with my EM Algorithm implementation.

To visualize how good the fit is, let’s look at the density plot.

```{r, eval=F}
hist(x, prob=T, breaks=32, xlim=c(range(x)[1], range(x)[2]), main='')
lines(density(x), col="green", lwd=2)
x1 <- seq(from=range(x)[1], to=range(x)[2], length.out=1000)
y <- pi1 * dnorm(x1, mean=mu1, sd=sigma1) + pi2 * dnorm(x1, mean=mu2, sd=sigma2)
lines(x1, y, col="red", lwd=2)
legend('topright', col=c("green", 'red'), lwd=2, legend=c("kernal", "fitted"))
```

This two component mixture of normal densities fit the first dataset very well.

-->