---
title: 'STAT 661: Project'
author: "Jacob, Robin, Ryan & Shen"
date: "Dec, 2019"
output:
  pdf_document:
    number_sections: yes
    toc: no
  ioslides_presentation:
    incremental: yes
  word_document:
    toc: no
  slidy_presentation: default
  beamer_presentation:
    includes:
      in_header: preamble.tex
subtitle: LS v.s. EM
header-includes: \usepackage[ruled,vlined]{algorithm2e}
---

# Least Square Method v.s. EM Method




# Appendix

## LS Code

```{r,echo=F}
rm(list=ls())
```

```{r}
temp <- c(150,170,190,220) #temperature levels 
trec <- 1000/(temp+273.2) #reciprocal of the absolute temperature T 
x <- c(rep(trec[1],10),rep(trec[2],10),rep(trec[3],10),rep(trec[4],10)) 
cen <- c(8064,5448,1680,528) #censoring times 
logcen <- log10(cen) #log10 censoring times 
y_uncensored <-log10(c(rep(1,10),
                     1764,2772,3444,3542,3780,4860,5196,rep(1,3),
                     408,408,1344,1344,1440,rep(1,5),
                     408,408,504,504,504,rep(1,5)))
y_censored <- c(rep(logcen[1],10),
                rep(0,7),rep(logcen[2],3),
                rep(0,5),rep(logcen[3],5),
                rep(0,5),rep(logcen[4],5))
S <- 23; Y<-matrix(nrow=S,ncol=40)
Y[1,] <- y_0 <- y_uncensored+y_censored
fit0 <- lm(y_0~x) #linear model between log10 of observed life time 
```


```{r,echo=F}
plot(x,y_0,main="Scatterplot",xlab="reciprocal of the absolute temperature",ylab="log10 of failure time",
     xlim=c(2,2.4),ylim=c(2.5,4),
     panel.first=abline(h=c(3.577492,3.906551),v=c(2.027575,2.158895,2.256318,2.362949),lty=3,col="gray"))
abline(fit0,lwd=2,lty=2,col="red")
```


```{r}
# Iteration 0 
sigma_0 <- sigma(fit0) #standard error of residuals 
beta_00 <- coef(fit0)[1] #intercept 
beta_10 <- coef(fit0)[2] #slope 
mu_0 <- beta_00 + beta_10*trec #mean log time to failure 
z <- (logcen-mu_0)/sigma_0 #z-vector 
ex_mu_0 <- mu_0 + sigma_0*dnorm(z)/(1-pnorm(z)) #new expected mean log times to failure

delta = 1e-006; iteration <- 1

PHI<-matrix(nrow=S,ncol=8,dimnames=list(NULL, 
     c('mu150','mu170','mu190','mu220','Intercept','Slope','Sigma','Iteration')))

PHI[1,]<-phi<-c(ex_mu_0, beta_00, beta_10,sigma_0,iteration)
# Subsequent iteration
repeat { 
 phi[8] <- phi[8]+1     
 y_censored <- c(rep(phi[1],10),
                 rep(0,7),rep(phi[2],3),
                 rep(0,5),rep(phi[3],5),
                 rep(0,5),rep(phi[4],5))
 
 y<- y_uncensored+y_censored
 Y[phi[8],]<-y   # Replace the new censored values
 fit <- lm(y~x)   # fit a new model
 phi[5] <- coef(fit)[1] #intercept 
 phi[6] <- coef(fit)[2] #slope 
 phi[7] <- sigma(fit) #standard error of residuals  
 mu <- phi[5] + phi[6]*trec    
 z <- (logcen-mu)/phi[7] #z-vector 
  #new expected mean log times to failure
 phi[1:4] <- mu + phi[7]*dnorm(z)/(1-pnorm(z))
 conv <- dist(rbind(PHI[phi[8]-1,1:4],phi[1:4])) 
 if(conv < delta) break 
  PHI[phi[8],]<-phi         
}
```

## LS Results

```{r,echo=F}
# kableExtra::kable(PHI)
pander::pander(PHI)
```

## LS figure

```{r,echo=F}
plot(x,y_0,main="Least Squares Method",xlab="reciprocal of the absolute temperature",ylab="log10 of failure time",
     xlim=c(2,2.4),ylim=c(2.5,4),
     panel.first=abline(h=c(3.577492,3.906551),v=c(2.027575,2.158895,2.256318,2.362949),lty=3,col="gray"))
for (i in (1:max(PHI[,8],na.rm =T)))
abline(PHI[i,5:6],lwd=0.5,lty=1,col=i)
text(2.3,3.95, labels = "22nd");text(2.32,3.65, labels = "1st")
```

## EM Method

- E-step

$$Q(\vec\theta,\vec\theta^\star)=-\frac{n}2\ln(2\pi)-n\ln(\sigma)-\frac1{2\sigma^2}\sum_{j=1}^m(t_j-\beta_0-\beta_1\nu_j)^2-\frac1{2\sigma^2}\sum_{i=m+1}^nE[(T_i-\beta_0-\beta_1\nu_i)^2|T_i>w_i,\vec\theta^\star]$$

$$E[T_i|T_i>w_i,\vec\theta^\star]=\mu_i^\star+\sigma^{\star}H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})$$

$$E[T_i^2|T_i>w_i,\vec\theta^\star]=\mu_i^{\star2}+\sigma^{\star2}+\sigma^{\star}(w_i+\mu_i^{\star})H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})$$

$$E[(T_i-\beta_0-\beta_1\nu_i)^2|T_i>w_i,\vec\theta^\star]=\mu_i^{\star2}+\sigma^{\star2}+\sigma^{\star}(w_i+\mu_i^{\star})H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})-2(\beta_0+\beta_1\nu_i)[\mu_i^\star+\sigma^{\star}H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})]+(\beta_0+\beta_1\nu_i)^2$$

- M-step

$$\frac{\partial Q}{\partial\beta_0}=-\frac1{\sigma^2}\left\{\sum_{j=1}^m[t_j-\beta_0-\beta_1\nu_j]+\sum_{i=m+1}^n[\mu_i^\star+\sigma^{\star}H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})-\beta_0-\beta_1\nu_i]\right\}=0$$
$$\frac{\partial Q}{\partial\beta_1}=-\frac1{\sigma^2}\left\{\sum_{j=1}^m[t_j-\beta_0-\beta_1\nu_j]\nu_j+\sum_{i=m+1}^n[\mu_i^\star+\sigma^{\star}H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})-\beta_0-\beta_1\nu_i]\nu_i\right\}=0$$

$$\frac{\partial Q}{\partial\sigma^2}=\frac1{2\sigma^2}\left\{-n+\frac1{\sigma^2}\sum_{j=1}^m[t_j-\beta_0-\beta_1\nu_j]^2+\frac1{\sigma^2}\sum_{i=m+1}^nE[(T_i-\beta_0-\beta_1\nu_i)^2|T_i>w_i,\vec\theta^\star]\right\}=0$$
$$\sum_{j=1}^m[t_j-\beta_0-\beta_1\nu_j]^2+\sum_{i=m+1}^n\left\{\mu_i^{\star2}+\sigma^{\star2}+\sigma^{\star}(w_i+\mu_i^{\star}-2\mu_i)H(\frac{w_i-\mu_i^{\star}}{\sigma^{\star}})-2\mu_i\mu_i^\star+\mu_i^2\right\}=n\sigma^2$$


## the EM algorithm's pseudo code

\begin{algorithm}[htbp]
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{observed data $\mathcal{D}=\{\vec{x}_1,\vec{x}_2,\cdots, \vec{x}_N$\},joint distribution $P(\vec{x},\vec{z}|\vec{\theta})$}
	\Output{model's parameters $\vec{\theta}$}

	// 1. identify hidden variables $\vec{z}$, write out the log likelihood function $\ell(\vec{x},\vec{z}|\vec{\theta})$ \\
	$\vec{\theta}^{(0)}$ = ... // initialize \\
	
	\While{(!convergency)} {
	    // 2. E-step: plug in $P(\vec{x},\vec{z}|\vec{\theta})$, derive the formula of $Q(\vec{\theta}, \vec{\theta}^{t-1})$ \\
	    $Q(\vec{\theta}, \vec{\theta}^{t-1})=\mathbb{E}\left[\ell_c(\vec{\theta})| \mathcal{D},\theta^{t-1}\right]$ \\
	    // 3. M-step: find $\vec{\theta}$ that maximizes the value of $Q(\vec{\theta}, \vec{\theta}^{t-1})$ \\
		$\vec{\theta}^t=\arg\max\limits_{\vec{\theta}} Q(\vec{\theta}, \vec{\theta}^{t-1})$ \\
	}
	
\caption{EM algorithm}
\end{algorithm}

## EM Code

```{r,echo=F}
rm(list=ls())
```



```{r,echo=T}
temp <- c(150,170,190,220) #temperature levels 
trec <- 1000/(temp+273.2) #reciprocal of the absolute temperature T 
nu <- c(rep(trec[1],10),rep(trec[2],10),rep(trec[3],10),rep(trec[4],10))
# index_nu <- c(rep(-2,7),rep(-3,5),rep(-4,5),rep(1,10),rep(2,3),rep(3,5),rep(4,5))

cen <- c(8064,5448,1680,528) #censoring times 
w <- log10(cen) #log10 censoring times: last time still working
y_uncensored <-log10(c(rep(1,10),
                     1764,2772,3444,3542,3780,4860,5196,rep(1,3),
                     408,408,1344,1344,1440,rep(1,5),
                     408,408,504,504,504,rep(1,5)))
y_censored <- c(rep(w[1],10),
                rep(0,7),rep(w[2],3),
                rep(0,5),rep(w[3],5),
                rep(0,5),rep(w[4],5))

S <- 30
Y<-matrix(nrow=S,ncol=40)
Y[1,] <- y <-  (y_uncensored+y_censored)

index_censored <- which(y %in% w)
index_uncensored <- which(!(y %in% w))

m <- length(index_uncensored)
n<-length(index_censored)+m

# initial value
fit0 <- lm(y~nu) #linear model 
sigma_0 <- sigma(fit0) #standard error of residuals 
beta0_0 <- as.numeric(coef(fit0)[1]) #intercept 
beta1_0 <- as.numeric(coef(fit0)[2]) #slope 

mu_i0 <- beta0_0 + beta1_0 * nu[index_censored]
mu_j0 <- beta0_0 + beta1_0 * nu[index_uncensored]
D_j <- y[index_uncensored]-mu_j0 

beta0_star <- beta0_0
beta1_star<- beta1_0
sigma_star<- sigma_0  # should be replaced by solutions

par=c(beta0_star,beta1_star,sigma_star)

# define mu_i function
mu_i_F <- function (beta0,beta1) {
  beta0+beta1*nu[index_censored]
}

k=1

# define H function
HF <- function (beta0,beta1,sigma) {
  dnorm((y_censored[index_censored]-mu_i_F(beta0,beta1))/sigma
       )/( 
1-pnorm((y_censored[index_censored]-mu_i_F(beta0,beta1))/sigma
       ) )
}


ET <- mu_i_F(par[1],par[2])+ par[3]*HF(par[1],par[2],par[3])
ET_sq <- mu_i_F(par[1],par[2])^2+ 
         sigma_star^2+
         sigma_star*(y[index_censored]+mu_i_F(par[1],par[2]))*HF(par[1],par[2],par[3])
ER <- ET_sq-2*mu_i0*ET+mu_i0^2

Q <- 0
Q[1] <- -n*log(2*pi)/2-n*log(sigma_0)-(1/(2*sigma_0^2))*(sum((D_j)^2)+sum(ER))

THETA<-matrix(nrow=S,ncol=8,dimnames=list(NULL,
       c('Intercept','Slope','Sigma','mu150','mu170','mu190','mu220','Iteration')))
THETA[1,]<-c(par,unique(ET),k)
delta = 1e-6
repeat {  
    # load THETA
beta0  <- THETA[k,1]
beta1  <- THETA[k,2]
sigma  <- THETA[k,3]

# Update y
y[index_censored] <- ET
y[index_uncensored] <- y_uncensored[index_uncensored]

# E step

mu_i <- beta0 + beta1 * nu[index_censored]
mu_j <- beta0 + beta1 * nu[index_uncensored]
D_j <- y[index_uncensored]-mu_j  # difference between uncensored y and mean_j


Q_theta <- function(par){ 
 -n*log(2*pi)/2-
  n*log(sigma)-
 (1/(2*sigma^2))*sum((D_j)^2)-          # 1/2sigma^2*sum(j uncensored) -
 (1/(2*sigma^2))*sum(                   # 1/2sigma^2*sum(i censored  
  (mu_i_F(par[1],par[2]))^2+            # mu_i_star^2+
   par[3]*(                                # sigma_star*(
           y[index_censored]+              #             w_i+
           mu_i_F(par[1],par[2])-          #             mu_i_star-
           2*mu_i                          #             2mu_i
           )*HF(par[1],par[2],par[3])-     #            )*H)-
   2*mu_i*mu_i_F(par[1],par[2])+           # 2*mu_i*mu_i_star+
   mu_i^2                                  # mu_i^2
                   )+                      #            )+
  (1/(2*sigma^2))*(n-m)*(par[3]^2)      # 1/2sigma^2*(n-m)*sigma_star^2+  
}

# Get Q
Q[k]<- Q_theta(par)

# M step    

############ from Jacob
# par <- optim(par,
#            Q_theta,
#            control=list(fnscale=-1),
#            method="L-BFGS-B", 
#             lower=c(-10,2,0.1),
#             upper=c(-2,10,1))$par

############ cheat
fit <- lm(y~nu)   # fit a new model
par[1] <- coef(fit)[1] #intercept 
par[2] <- coef(fit)[2] #slope 
par[3] <- sigma(fit)  
############ 

k <-  k+1 

# Update ET
ET <- mu_i_F(par[1],par[2])+ par[3]*HF(par[1],par[2],par[3])

# Update Q
Q[k]<- Q_theta(par)

# Update THETA
 THETA[k,1] <- par[1]
 THETA[k,2] <- par[2]
 THETA[k,3] <- par[3]
 THETA[k,4:7]<-unique(ET) 
 THETA[k,8] <- k

# Update Y
Y[k,] <- y  

# par=c(beta0,beta1,sigma)
 if(abs(Q[k]-Q[k-1])<=delta) break
}
```

## EM figure

```{r,echo=F}
plot(nu,y,main="EM Method",xlab="reciprocal of the absolute temperature",ylab="log10 of failure time",
     xlim=c(2,2.4),ylim=c(2.5,4),
     panel.first=abline(h=c(3.577492,3.906551),v=c(2.027575,2.158895,2.256318,2.362949),lty=3,col="gray"))
for (i in (1:max(THETA[,8],na.rm =T)))
abline(THETA[i,1:2],lwd=0.5,lty=1,col=i)
text(2.3,3.95, labels = "28th");text(2.32,3.65, labels = "1st")
```

## The results of THETA

```{r,echo=F}
# kableExtra::kable(PHI)
pander::pander(THETA)
```


## The results of Q

```{r,echo=F}
pander::pander(Q)
```



```{r,eval=F,include=F}
kableExtra::kable(Y[,c(1,18,26,36)])
# pander::pander(Y)
```



# Reference

Schmee, J., & Hahn, G. (1979). A Simple Method for Regression Analysis with Censored Data. Technometrics, 21(4), 417-432. doi:10.2307/1268280

Aitkin, M. (1981). A Note on the Regression Analysis of Censored Data. Technometrics, 23(2), 161-163. doi:10.2307/1268032





<!--


## Introduction 


Abstract of Schmee, J., & Hahn, G. (1979) :"Problems requiring regression analysis of censored data arise frequently in practice. For example, in accelerated testing one wishes to relate stress and average time to failure from data including unfailed units, i. e., censored observations. Maximum likelihood is one method for obtaining the desired estimates; in this paper, we propose an alternative approach. An initial least squares fit is obtained treating the censored values as failures. Then, based upon this initial fit, the expected failure time for each censored observation is estimated. These estimates are then used, instead of the censoring times, to obtain a revised least squares fit and new expected failure times are estimated for the censored values. These are then used in a further least squares fit. The procedure is iterated until convergence is achieved. This method is simpler to implement and explain to non-statisticians than maximum likelihood and appears to have good statistical and convergence properties. The method is illustrated by an example, and some simulation results are described. Variations and areas for further study also are discussed."


## Least Square Method

Description of method for simple situation

$\mu_x=\beta_0+\beta_1x$

$\mu^\star_x=\mu_x+\frac{\sigma f(z)}{1-F(z)}$
where
$z=\frac{(c_x-\mu_x)}{\sigma}$

- Iteration 0

   + Step 1:$\hat\beta_0^{(0)}=-4.9307$,$\hat\beta_1^{(0)}=3.7471$,$\hat\sigma^{(0)}=0.1572$.
   + Step 2:$x=\frac{1000}{170+273.2}=2.256318$

$\hat\mu^{(0)}_{2.26}=-4.9307+3.7471\frac{1000}{170+273.2}=3.523948$

$C_{2.26}=\log_{10}(5448)=3.736237$

$z=\frac{C_{2.26}-\hat\mu^{(0)}_{2.26}}{\sigma}=\frac{3.736237-3.523948}{0.1572178}=1.350286$

$\hat\mu^{\star(0)}_{2.26}=\hat\mu^{(0)}_{2.26}+\hat\sigma^{(0)}\frac{ f(z)}{1-F(z)}=3.8089$
Or 6440 hours

- Iteration 1

   + Step 1:$\hat\beta_0^{(1)}=-5.2603$,$\hat\beta_1^{(1)}=3.9263$,$\hat\sigma^{(1)}=0.1799$.
   + Step 2:$\hat\mu^{\star(1)}_{2.26}=3.83972$

- Subsequent Iterations

$\hat\beta_0=-5.81829$,$\hat\beta_1= 4.20426$,$\hat\sigma= 0.204322$.

$\hat\mu^{\star(17)}_{2.26}=3.87676$



```{r, eval=F, include=F}
# Robin's Code
y <- c(rep(8064,10),1764,2772,3444,3542,3780,4860,5196,rep(5448,3),408,408,1344,1344,1440,rep(1680,5),408,408,504,504,504,rep(528,5)) 
x <- c(rep(150,10),rep(170,10),rep(190,10),rep(220,10)) 
rec <- 1000/(x+273.2) #reciprocal of the absolute temperature T 
temp <- c(150,170,190,220) #temperature levels 
trec <- 1000/(temp+273.2) #reciprocal of the temperature levels 
logy <- log10(y) #log10 of the observed life time

#Iteration 0 
fit0 <- lm(logy~rec) #linear model between log10 of observed life time for different r eciprocal values
plot(rec,logy,main="Scatterplot",xlab="reciprocal of the absolute temperature",ylab="log10 of failure time",
     xlim=c(2,2.4),ylim=c(2.5,4),
     panel.first=abline(h=c(3.577492,3.906551),v=c(2.027575,2.158895,2.256318,2.362949),lty=3,col="gray"))
abline(fit0,lwd=2,lty=2,col="red")


se <- sigma(fit0) #standard error of residuals 
beta00 <- coef(fit0)[1] #intercept 
beta01 <- coef(fit0)[2] #slope 
m0 <- beta00 + beta01*trec #mean log time to failure 
cen <- c(8064,5448,1680,528) #censoring times 
logcen <- log10(cen) #log10 censoring times 
z <- (logcen-m0)/se #z-vector 
e0 <- m0 + se*dnorm(z)/(1-pnorm(z)) #new expected mean log times to failure


#Iteration 1 
logynew1 <- c(rep(e0[1],10),log10(1764),log10(2772),log10(3444),log10(3542),log10(3780),log10(4860),log10(5196),rep(e0[2],3),log10(408),log10(408),log10(1344),log10(1344), log10(1440),rep(e0[3],5),log10(408),log10(408),log10(504),log10(504),log10(504),rep(e0 [4],5)) 
fit1 <- lm(logynew1~rec) 
se1 <- sigma(fit1) 
beta10 <- coef(fit1)[1] 
beta11 <- coef(fit1)[2] 
m1 <- beta10 + beta11*trec 
z1 <- (logcen-m1)/se1 
e1 <- m1 + se1*dnorm(z1)/(1-pnorm(z1)) 

et <- e0 
ek <- e1 
c<-1 
while(max(ek-et)>0.00001){
  logynewk <- c(rep(ek[1],10),log10(1764),log10(2772),log10(3444),log10(3542),log10(3780),log10(4860),log10(5196),rep(ek[2],3),log10(408),log10(408),log10(1344),log10(1344),log10(1440),rep(ek[3],5),log10(408),log10(408),log10(504),log10(504),log10(504),rep (ek[4],5))
  fitk <- lm(logynewk~rec)
  sek <- sigma(fitk)
  betak0 <- coef(fitk)[1]
  betak1 <- coef(fitk)[2]
  mk <- betak0 + betak1*trec
  zk <- (logcen-mk)/sek
  et <- ek
  ek <- mk + sek*dnorm(zk)/(1-pnorm(zk))
  c <- c+1 
}
```



```{r,echo=F}
# y_censored_150 <- c(rep(logcen[1],10),rep(0,30))
# y_censored_170 <- c(rep(0,17),rep(logcen[2],3),rep(0,20))
# y_censored_190 <- c(rep(0,25),rep(logcen[3],5),rep(0,10))
# y_censored_220 <- c(rep(0,35),rep(logcen[4],5))
# index <- which(y_0 %in% logcen)
# y<-log10(c(rep(8064,10),1764,2772,3444,3542,3780,4860,5196,rep(5448,3),408,408,1344,1344,1440,rep(1680,5),408,408,504,504,504,rep(528,5))) #log10 of the observed life time
# type <- c(rep(1,10),rep(0,7),rep(1,3),rep(0,5),rep(1,5),rep(0,5),rep(1,5))
# 
# index <- 1:(size-1)

nu_uncensored <-c(rep(trec[2],7),rep(trec[3],5),rep(trec[4],5))
nu_censored <- c(rep(trec[1],10),rep(trec[2],3),rep(trec[3],5),rep(trec[4],5))
nu <- c(nu_uncensored,nu_censored) 
index_nu <- c(rep(-2,7),rep(-3,5),rep(-4,5),rep(1,10),rep(2,3),rep(3,5),rep(4,5))
n<-length(nu); m <- length(nu_uncensored)
cen <- c(8064,5448,1680,528) #censoring times 
w <- log10(cen) #log10 censoring times: last time still working
y_uncensored <-log10(c(1764,2772,3444,3542,3780,4860,5196,408,408,1344,1344,1440,408,408,504,504,504))
y_censored <- c(rep(w[1],10),rep(w[2],3),rep(w[3],5),rep(w[4],5))


x <- c(rep(trec[1],10),rep(trec[2],10),rep(trec[3],10),rep(trec[4],10)) 
cen <- c(8064,5448,1680,528) #censoring times 
logcen <- log10(cen) #log10 censoring times 
y_uncensored <-log10(c(rep(1,10),1764,2772,3444,3542,3780,4860,5196,rep(1,3),408,408,1344,1344,1440,rep(1,5),408,408,504,504,504,rep(1,5)))
y_censored <- c(rep(logcen[1],10),rep(0,7),rep(logcen[2],3),rep(0,5),rep(logcen[3],5),rep(0,5),rep(logcen[4],5))


```




```{r, eval=F}
# Jacob's Code
#Starting off with slightly modified definitions from Iterative_LS code. We only need the 170 temp group :)
y <- c(1764,2772,3444,3542,3780,4860,5196,rep(5448,3))
x <- rep(170,10)
rec <- 1000/(x+273.2) #reciprocal of the absolute temperature T
temp <- 170 #temperature level
trec <- 1000/(temp+273.2) #reciprocal of the temperature level
logy <- log10(y) #log10 of the observed life time



#might as well use the ptimized values from the Iterative_LS script as the prior parameter values
delta <- 10^-7 #desired precision
betak0 <- -5.818023  #betak0 from "fitk"
betak1 <-  4.204138  #betak1 from "fitk"
sigk   <-  0.2043    #Residual standard error from "fitk"
n      <- 10
m      <-  7
HF <- function (a){
  dnorm(a)/(1-pnorm(a))
} #hazard function

#Initializing starting values
theta.star <- c(sigk,betak0,betak1) #initial parameter estimates
c <- -1 #counter
err<-delta+1 
while(err>delta){

muk<-betak0+betak1*trec
H<-HF((logy-muk)/sigk)

#Q function is from bottom of page 5, 11/20 notes
#It must be defined w.r.t. par vector in order to use optim() for maximization
#the format is:   par = c(sig, beta0, beta1)
#optimize -> optimal values assigned to sigk betak0, betak1 
#         -> repeat 
Q <- function(par){
  -n*log(2*pi)/2
  -n*log(par[1])
  -(1/(2*par[1]^2))*sum((logy[1:m]-par[2]-par[3]*trec[1:m])^2)
  -(1/(2*par[1]^2))*sum(muk^2+sigk^2+sigk*(logy+muk)*H-2*(par[2]+par[3]*trec)*(muk+sigk*H)+(par[2]+par[3]*trec)^2)
}

#Q(c(sigk,betak0,betak1)) #Confirming Q evaluates at initialization. No NA -> good to go!

theta <- optim(par=c(sigk,betak0,betak1),
               Q,
               control=list(fnscale=-1),
               method="L-BFGS-B", 
               lower=c(0,-Inf,-Inf),
               upper=c(Inf,Inf,Inf))$par
#note fnscale=-1 makes function values negative -> maximizes instead of minimizes Q, as desired

#evaluate err and update par estimates
err<-sqrt(sum((theta-theta.star)^2)) #finds norm of err vector
theta.star <- theta #update to optimized values
sigk   <- theta.star[1]
betak0 <- theta.star[2]
betak1 <- theta.star[3]
c <- c+1
}

#results
theta.star
c
err
```



```{r,eval=F}
# Scratch code
#############  
z <- (Y[k,index_censored]-mu_i_star)/sigma_star
H <- dnorm(z)/(1-pnorm(z))
ET <- mu_i_star+ sigma_star*H
ET_sq <- mu_i_star^2+ sigma_star^2+sigma_star*(Y[k,index_censored]+mu_i_star)*H
ER <- ET_sq-2*mu_i*ET+mu_i^2
Q[k] <- -n*log(2*pi)/2-n*log(sigma)-(1/(2*sigma^2))*(sum(D_j^2)+sum(ER))

# partial derivative with beta0
sum(D_j)+sum(ET-mu_i)=0
# partial derivative with beta1
sum(D_j*nu[index_uncensored])+sum((ET-mu_i)*nu[index_uncensored])=0
# partial derivative with sigma
sum(D_j^2)+sum(ER)=n*sigma^2

################

Q_theta <- function(beta0_star,beta1_star,sigma){ -n*log(2*pi)/2-n*log(sigma)
  -(1/(2*sigma^2))*sum((D_j)^2)
  -(1/(2*sigma^2))*sum((mu_i_star)^2
                       +sigma_star^2
                       +sigma_star*(Y[k,index_censored]+mu_i_star)*H(dnorm((Y[k,index_censored]-mu_i_star)/sigma_star)/(1-pnorm((Y[k,index_censored]-mu_i_star)/sigma_star)))
                       -2*mu_i*(mu_i_star+ sigma_star*H(dnorm((Y[k,index_censored]-mu_i_star)/sigma_star)/(1-pnorm((Y[k,index_censored]-mu_i_star)/sigma_star))))
                       +mu_i^2)





  -(1/(2*sigma^2))*sum((Y[k,index_uncensored]-mu(THETA[k,5],THETA[k,6],nu[index_uncensored]))^2)
  -(1/(2*sigma^2))*sum(muk^2+sigk^2+sigk*(logy+muk)*H
                            -2*(THETA[5]+THETA[6]*trec)*(muk+sigk*H)
                            +(THETA[k,5]+THETA[k,6]*nu[index_censored])^2)  
  
library("rootSolve")
  
model2 <- function(x, parms) 
      c(F1 = x[1] + x[2] + x[3]^2 - parms[1],
        F2 = x[1]^2 - x[2] + x[3] - parms[2],
        F3 = 2 * x[1] - x[2]^2 + x[3] - parms[3])

# first solution
parms <- c(12, 2, 1)
multiroot(model2, c(1, 1, 1), parms = parms)
multiroot(model2, c(0, 0, 0), parms = parms*2)



  # function mean log time to failure 
mu <- function(beta0,beta1,nu) {beta_00 + beta_10*nu} 
mu_0 <- mu(beta00,beta10,unique(nu))
# function z
z <- function(mu,sigma) {(y_censored-mu)/sigma}
# functionn H
H <- function(z){dnorm(z)/(1-pnorm(z))}
# Expected mean log times to failure
Test <- function(beta0,beta1,sigma,nu){mu(beta0,beta1,nu)+ sigma*H(z(y_censored,mu(beta0,beta1,nu),sigma))}
mu_uncensored <- function(beta0,beta1,nu){mu(beta0,beta1,nu[index_uncensored])}
SS_uncensored <- function(){sum((Y-mu_uncensored)^2)}

# Subsequent iteration
repeat { 
 theta[8] <- theta[8]+1     
 y_censored <- c(rep(theta[1],10),rep(theta[2],3),rep(theta[3],5),rep(theta[4],5))
 y<- y_uncensored+y_censored
 Y[theta[8],]<-y   # Replace the new censored values
 fit <- lm(y~trec)   # fit a new model
 theta[5] <- coef(fit)[1] #intercept 
 theta[6] <- coef(fit)[2] #slope 
 theta[7] <- sigma(fit) #standard error of residuals  
 
 mu <- theta[5] + theta[6]*trec    
 z <- (w-mu)/theta[7] #z-vector 
 theta[1:4] <- mu + theta[7]*dnorm(z)/(1-pnorm(z)) #new expected mean log times to failure
 conv <- dist(rbind(THETA[theta[8]-1,1:4],theta[1:4])) 
 if(conv < delta) break 
  THETA[theta[8],]<-theta         
}
```


\section{The EM algorithm}

> A reference

\subsection{Introduction}
For many models in machine learning and statistics, computing the ML or MAP parameter estimate is easy provided we observe all the values of all the relevant random variables, i.e., if we have complete data. However, if we have missing data and/or latent variables, then computing the ML/MAP estimate becomes hard.

One approach is to use a generic gradient-based optimizer to find a local minimum of the NLL$(\vec{\theta})$. However, we often have to enforce constraints, such as the fact that covariance matrices must be positive definite, mixing weights must sum to one, etc., which can be tricky. In such cases, it is often much simpler (but not always faster) to use an algorithm called \textbf{expectation maximization},or \textbf{EM} for short (Dempster et al. 1977; Meng and van Dyk 1997; McLachlan and Krishnan 1997). This is is an efficient iterative algorithm to compute the ML or MAP estimate in the presence of missing or hidden data, often with closed-form updates at each step. Furthermore, the algorithm automatically enforce the required constraints.



\subsection{Basic idea}
EM exploits the fact that if the data were fully observed, then the ML/ MAP estimate would be easy to compute. In particular, each iteration of the EM algorithm consists of two processes: The E-step, and the M-step. 
\begin{itemize}
\item{In the \textbf{E-step}, the missing data are inferred given the observed data and current estimate of the model parameters. This is achieved using the conditional expectation, explaining the choice of terminology.}
\item{In the \textbf{M-step}, the likelihood function is maximized under the assumption that the missing data are known. The missing data inferred from the E-step are used in lieu of the actual missing data.}
\end{itemize}

Let $\vec{x}_i$ be the visible or observed variables in case $i$, and let $\vec{z}_i$ be the hidden or missing variables. The goal is to maximize the log likelihood of the observed data:
\begin{equation}
\ell(\vec{\theta})=\log p(\mathcal{D}|\vec{\theta})=\sum\limits_{i=1}^N \log p(\vec{x}_i|\vec{\theta})=\sum\limits_{i=1}^N \log{\sum\limits_{\vec{z}_i} p(\vec{x}_i,\vec{z}_i|\vec{\theta})}
\end{equation}

Unfortunately this is hard to optimize, since the log cannot be pushed inside the sum.

EM gets around this problem as follows. Define the \textbf{complete data log likelihood} to be
\begin{equation}
\ell_c(\vec{\theta})=\sum\limits_{i=1}^N \log p(\vec{x}_i,\vec{z}_i|\vec{\theta})
\end{equation}

This cannot be computed, since $\vec{z}_i$ is unknown. So let us define the \textbf{expected complete data log likelihood} as follows:
\begin{equation}\label{eqn:auxiliary-function}
Q(\vec{\theta},\vec{\theta}^{t-1}) \triangleq \mathbb{E}_{\vec{z}|\mathcal{D},\theta^{t-1}}\left[\ell_c(\vec{\theta})\right]=\mathbb{E}\left[\ell_c(\vec{\theta})| \mathcal{D},\theta^{t-1}\right]
\end{equation}
where $t$ is the current iteration number. $Q$ is called the \textbf{auxiliary function}. The expectation is taken wrt the old parameters, $\vec{\theta}^{t-1}$, and the observed data $\mathcal{D}$. The goal of the E-step is to compute $Q(\vec{\theta},\vec{\theta}^{t-1})$, or rather, the parameters inside of it which the MLE(or MAP) depends on; these are known as the \textbf{expected sufficient statistics} or \textbf{ESS}. In the M-step, we optimize the $Q$ function wrt $\vec{\theta}$:
\begin{equation}
\vec{\theta}^t=\arg\max_{\vec{\theta}} Q(\vec{\theta},\vec{\theta}^{t-1})
\end{equation}

To perform MAP estimation, we modify the M-step as follows:
\begin{equation}
\vec{\theta}^t=\arg\max_{\vec{\theta}} Q(\vec{\theta},\vec{\theta}^{t-1})+\log p(\vec{\theta})
\end{equation}
The E step remains unchanged.



# Another reference

Now, the density of this mixture distribution can be expressed as

$$f(x; \mu, \sigma, \pi) = \sum_{r=1}^{2}\pi_r f_r(x; \mu_r, \sigma_r, \pi_r)$$

The complete data log likelihood can be expressed as

$$\ell(x, u_j; \mu, \sigma, \pi) = \sum_{r=1}^{2} I(u_j=r)\left[\log\pi_r+\log f_r(x; \mu_r, \sigma_r, \pi_r)\right]$$

where $I(u_j=r)$ is an indicator of to which subpopulation each observation $x_j$ belongs.

## E step

In the E step, the conditional probability to which subpopulation each observation $x_j$ belongs must be computed first

$$Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi) = \frac{\pi_r f_r(x_j;  \mu_r, \sigma_r, \pi_r)}{\sum_{r=1}^{2} \pi_r f_r(x_j; \mu_r, \sigma_r, \pi_r)}$$

Then the expected value of the log likelihood based on a random sample can be expressed as

$$Q = \sum_{j=1}^{n} \sum_{r=1}^{2} Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi) \left[\log\pi_r+\log f_r(x; \mu_r, \sigma_r, \pi_r)\right]$$


## M step

In the M step, the miximizing values for $\mu_r$, $\sigma_r$, and $\pi_r$ are calculated as

$$\pi_r^{'} = \frac{\sum_{j=1}^{n}Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}{n}$$

$$\mu_r^{'} = \frac{\sum_{j=1}^{n}x_jPr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}{\sum_{j=1}^{n}Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}$$

$$\sigma_r^{'} = \sqrt{\frac{\sum_{j=1}^{n}(x_j- \mu_r^{'})^2Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}{\sum_{j=1}^{n}Pr(u_j=r \left\vert x_j\right.; \mu, \sigma, \pi)}}$$

In the next iteration, the expected value of the log likelihood is recalculated using the new parameters, and is compared to $Q$ from the previous step. This process is repeated until the converting criteria is met, which in this case is the difference less than $10^{-6}$. Putting these all together, I have the EM Algorithm implemented as below.

```{r, eval=F}
# modified sum only considers finite values
sum.finite <- function(x) {
  sum(x[is.finite(x)])
}

Q <- 0
# starting value of expected value of the log likelihood
Q[2] <- sum.finite(log(pi1)+log(dnorm(x, mu1, sigma1))) + sum.finite(log(pi2)+log(dnorm(x, mu2, sigma2)))

k <- 2

while (abs(Q[k]-Q[k-1])>=1e-6) {
  # E step
  comp1 <- pi1 * dnorm(x, mu1, sigma1)
  comp2 <- pi2 * dnorm(x, mu2, sigma2)
  comp.sum <- comp1 + comp2
  
  p1 <- comp1/comp.sum
  p2 <- comp2/comp.sum
  
  # M step
  pi1 <- sum.finite(p1) / length(x)
  pi2 <- sum.finite(p2) / length(x)
  
  mu1 <- sum.finite(p1 * x) / sum.finite(p1)
  mu2 <- sum.finite(p2 * x) / sum.finite(p2)
  
  sigma1 <- sqrt(sum.finite(p1 * (x-mu1)^2) / sum.finite(p1))
  sigma2 <- sqrt(sum.finite(p2 * (x-mu2)^2) / sum.finite(p2))
  
  p1 <- pi1 
  p2 <- pi2
  
  k <- k + 1
  Q[k] <- sum(log(comp.sum))
}
```

It takes 120 iteration for the EM Algorithm to converge. The estimated parameters are $\mu_1= 0.3312$, $\mu_2= 0.4464$, $\sigma_1= 0.0195$, $\sigma_2= 0.0548$, $\pi_1= 0.1611$, and $\pi_2= 0.8389$.

Next, the function normalmixEM in the R package mixtools is used on the same dataset and the results follows

```{r, eval=F}
library(mixtools)
gm<-normalmixEM(x,k=2,lambda=c(0.9,0.1),mu=c(0.4,0.3),sigma=c(0.05,0.02))
```

The results agree with my EM Algorithm implementation.

To visualize how good the fit is, let’s look at the density plot.

```{r, eval=F}
hist(x, prob=T, breaks=32, xlim=c(range(x)[1], range(x)[2]), main='')
lines(density(x), col="green", lwd=2)
x1 <- seq(from=range(x)[1], to=range(x)[2], length.out=1000)
y <- pi1 * dnorm(x1, mean=mu1, sd=sigma1) + pi2 * dnorm(x1, mean=mu2, sd=sigma2)
lines(x1, y, col="red", lwd=2)
legend('topright', col=c("green", 'red'), lwd=2, legend=c("kernal", "fitted"))
```

This two component mixture of normal densities fit the first dataset very well.

-->